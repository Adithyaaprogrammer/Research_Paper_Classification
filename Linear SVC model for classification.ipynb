{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_size, num_layers, num_classes=2):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim,\n",
        "            hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        h0 = torch.zeros(self.lstm.num_layers * 2, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        c0 = torch.zeros(self.lstm.num_layers * 2, x.size(0), self.lstm.hidden_size).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.fc(out[:, -1, :])  # Use last time step's output\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "JUXR1xjhB_t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "# Load the CSV containing research papers\n",
        "df = pd.read_csv(\"/content/iitkgpResearchPapers_final (1).csv\")  # Replace with your CSV file\n",
        "df=df[5:15]\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Chunking Function\n",
        "def chunk_text(text, tokenizer, max_length=64):\n",
        "    \"\"\"\n",
        "    Tokenizes the input text into chunks of max_length tokens.\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
        "    chunks = [\n",
        "        tokens[i: i + max_length]\n",
        "        for i in range(0, len(tokens), max_length)\n",
        "    ]\n",
        "    return chunks\n",
        "\n",
        "# Custom Dataset for Chunks\n",
        "class ResearchPaperChunkDataset(Dataset):\n",
        "    def __init__(self, text, label, tokenizer, max_length=64):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text: Full text of the research paper.\n",
        "            label: Label of the research paper.\n",
        "            tokenizer: Tokenizer to use.\n",
        "            max_length: Maximum length of each chunk.\n",
        "        \"\"\"\n",
        "        self.chunks = chunk_text(text, tokenizer, max_length)\n",
        "        self.label = label\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.chunks)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        chunk = self.chunks[idx]\n",
        "\n",
        "        # Decode the chunk back to text and tokenize for padding and formatting\n",
        "        encoded = self.tokenizer(\n",
        "            tokenizer.decode(chunk),\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
        "            \"label\": torch.tensor(self.label, dtype=torch.long),\n",
        "        }\n"
      ],
      "metadata": {
        "id": "nJ5ChFpYB9_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "input_size = 64  # Token length per chunk\n",
        "embedding_dim = 32\n",
        "hidden_size = 32\n",
        "num_layers = 2\n",
        "num_classes = len(df[\"label\"].unique())\n",
        "learning_rate = 0.1\n",
        "num_epochs = 1\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = BiLSTM(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    hidden_size=hidden_size,\n",
        "    num_layers=num_layers,\n",
        "    num_classes=num_classes,\n",
        ").to(\"cuda\")\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "    for index, row in df.iterrows():\n",
        "        print(f\"Processing research paper {index + 1}/{len(df)}\")\n",
        "\n",
        "        # Prepare dataset and dataloader for the current research paper\n",
        "        paper_dataset = ResearchPaperChunkDataset(\n",
        "            text=row[\"text\"],\n",
        "            label=row[\"label\"],\n",
        "            tokenizer=tokenizer,\n",
        "            max_length=64,\n",
        "\n",
        "        )\n",
        "        paper_dataloader = DataLoader(paper_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "        # Reset gradients for each paper\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for batch in paper_dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
        "            attention_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
        "            label = batch[\"label\"].to(\"cuda\")\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, label)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            print(f\"Chunk Loss: {loss.item()}\")\n",
        "\n",
        "        # Save the model weights after processing all chunks of a research paper\n",
        "        torch.save(model.state_dict(), f\"model_weights_paper_{index + 1}.pt\")\n",
        "        print(f\"Saved model weights for paper {index + 1}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNlacPYNCCyh",
        "outputId": "6ca809ea-53eb-4441-c937-0592f954475d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1\n",
            "Processing research paper 6/10\n",
            "Chunk Loss: 0.7101012468338013\n",
            "Chunk Loss: 0.0018701935186982155\n",
            "Chunk Loss: 5.960446742392378e-06\n",
            "Chunk Loss: 2.3841855067985307e-07\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 6\n",
            "Processing research paper 7/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 7\n",
            "Processing research paper 8/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 8\n",
            "Processing research paper 9/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 9\n",
            "Processing research paper 10/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 10\n",
            "Processing research paper 11/10\n",
            "Chunk Loss: 39.02037811279297\n",
            "Chunk Loss: 26.88030242919922\n",
            "Chunk Loss: 10.535916328430176\n",
            "Chunk Loss: 0.000198821333469823\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 11\n",
            "Processing research paper 12/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 12\n",
            "Processing research paper 13/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 13\n",
            "Processing research paper 14/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 14\n",
            "Processing research paper 15/10\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Chunk Loss: 0.0\n",
            "Saved model weights for paper 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "model = BiLSTM(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    embedding_dim=32,\n",
        "    hidden_size=32,\n",
        "    num_layers=2,\n",
        "    num_classes=2,\n",
        ").to(\"cuda\")\n",
        "model.load_state_dict(torch.load(\"model_weights_paper_15.pt\"))  # Replace with your trained model weights\n",
        "model.eval()\n",
        "\n",
        "# Function to read and extract text from a PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in reader.pages:\n",
        "        text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "# Chunking function\n",
        "def chunk_text(text, tokenizer, max_length=64):\n",
        "    tokens = tokenizer.encode(text, truncation=False, add_special_tokens=False)\n",
        "    chunks = [\n",
        "        tokens[i: i + max_length]\n",
        "        for i in range(0, len(tokens), max_length)\n",
        "    ]\n",
        "    return chunks\n",
        "\n",
        "# Prediction function\n",
        "def predict_research_paper(pdf_path):\n",
        "    # Extract text from PDF\n",
        "    text = extract_text_from_pdf(pdf_path)\n",
        "\n",
        "    # Chunk the text\n",
        "    chunks = chunk_text(text, tokenizer)\n",
        "\n",
        "    # Initialize a list to store predictions for each chunk\n",
        "    predictions = []\n",
        "\n",
        "    # Process each chunk\n",
        "    for chunk in chunks:\n",
        "        # Convert chunk to input format\n",
        "        encoded = tokenizer.decode(chunk)\n",
        "        tokenized = tokenizer(\n",
        "            encoded,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = tokenized[\"input_ids\"].to(\"cuda\")\n",
        "\n",
        "        # Forward pass through the model\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids)\n",
        "            predicted_class = torch.argmax(outputs, dim=1).item()\n",
        "            predictions.append(predicted_class)\n",
        "\n",
        "    # Final prediction based on chunk predictions\n",
        "    final_prediction = round(sum(predictions) / len(predictions))  # Majority voting\n",
        "    return final_prediction, predictions\n",
        "\n",
        "# Predict a new research paper\n",
        "pdf_path = \"/content/R007.pdf\"\n",
        "final_prediction, chunk_predictions = predict_research_paper(pdf_path)\n",
        "\n",
        "print(f\"Final Prediction for the Paper: {final_prediction} (1: Publishable, 0: Non-Publishable)\")\n",
        "print(f\"Chunk Predictions: {chunk_predictions}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9b3Yp9iEDLd",
        "outputId": "343800d8-44c3-4b0c-829d-f9cd571a77e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-2babf2485e54>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"model_weights_paper_15.pt\"))  # Replace with your trained model weights\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (7255 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Prediction for the Paper: 0 (1: Publishable, 0: Non-Publishable)\n",
            "Chunk Predictions: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is LinearSVM Classifier\n",
        "\n",
        "Execute the code from here"
      ],
      "metadata": {
        "id": "oS2ipFbjyRCI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jSaDUiIEEPa",
        "outputId": "5d6121cc-3e64-4301-ff75-9be37e91bafd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.5.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from PyPDF2 import PdfReader"
      ],
      "metadata": {
        "id": "UrMgPQniECEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "ySHevMyPvXsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff6a387-92d6-4135-a792-d95d1f5489eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1V_TnVPXCNhPwAJG3GBIEZG4fiQxH1Mpx"
      ],
      "metadata": {
        "id": "kXh9vqZ_Jxz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/iitkgpResearchPapers_final (1).csv\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    tokens=word_tokenize(text)\n",
        "    tokens=[token.lower() for token in tokens]\n",
        "    tokens=[token for token in tokens if token not in string.punctuation]\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    tokens=[token for token in tokens if token not in stop_words]\n",
        "    stemmer=PorterStemmer()\n",
        "    tokens=[stemmer.stem(token) for token in tokens]\n",
        "\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "df[\"processed_text\"]=df[\"text\"].apply(preprocess_text)\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "tfidf=TfidfVectorizer(max_features=5000)\n",
        "X=tfidf.fit_transform(df[\"processed_text\"])\n",
        "y=df[\"label\"]\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42,stratify=y)\n",
        "\n",
        "# LinearSVM Model\n",
        "model=LinearSVC(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "precision,recall,f1, _=precision_recall_fscore_support(y_test, y_pred, average=\"binary\")\n",
        "classification_report_table=classification_report(y_test, y_pred, target_names=[\"Non-Publishable\", \"Publishable\"])\n",
        "\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report_table)\n",
        "\n",
        "# Tabular Summary of Results\n",
        "results=pd.DataFrame({\n",
        "    \"Metric\": [\"Precision\", \"Recall\", \"F1-Score\"],\n",
        "    \"Value\": [precision, recall, f1]\n",
        "})\n",
        "print(\"\\nEvaluation Summary:\")\n",
        "print(results)"
      ],
      "metadata": {
        "id": "suBs2rEhvAcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f400fe67-1ec5-4e41-c34c-67da5eec51a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "                 precision    recall  f1-score   support\n",
            "\n",
            "Non-Publishable       0.00      0.00      0.00         1\n",
            "    Publishable       0.50      1.00      0.67         1\n",
            "\n",
            "       accuracy                           0.50         2\n",
            "      macro avg       0.25      0.50      0.33         2\n",
            "   weighted avg       0.25      0.50      0.33         2\n",
            "\n",
            "\n",
            "Evaluation Summary:\n",
            "      Metric     Value\n",
            "0  Precision  0.500000\n",
            "1     Recall  1.000000\n",
            "2   F1-Score  0.666667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "GuZjn_vaDXe8",
        "outputId": "a0858337-ee2e-4319-82db-1f015b624c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 text  label  \\\n",
              "5   Advancements in 3D Food Modeling: A Review of ...      1   \n",
              "6   Addressing Min-Max Challenges in Nonconvex-Non...      1   \n",
              "7   Examining the Convergence of Denoising Diffusi...      1   \n",
              "8   Detecting Medication Usage in Parkinson’s Dise...      1   \n",
              "9   Addressing Popularity Bias with Popularity-Con...      1   \n",
              "10  Analyzing Real-Time Group Coordination in Augm...      0   \n",
              "11  Transdimensional Properties of Graphite in Rel...      0   \n",
              "12  AI-Driven Personalization in Online Education ...      0   \n",
              "13  Synergistic Convergence of Photosynthetic Path...      0   \n",
              "\n",
              "                                       processed_text  \n",
              "5   advanc 3d food model review metafood challeng ...  \n",
              "6   address min-max challeng nonconvex-nonconcav p...  \n",
              "7   examin converg denois diffus probabilist model...  \n",
              "8   detect medic usag parkinson ’ diseas multi-mod...  \n",
              "9   address popular bia popularity-consci align co...  \n",
              "10  analyz real-tim group coordin augment danc per...  \n",
              "11  transdimension properti graphit relat chees co...  \n",
              "12  ai-driven person onlin educ platform har power...  \n",
              "13  synergist converg photosynthet pathway subterr...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ec3f9449-234d-4202-a4fd-fd9b0b8f0eca\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
              "      <td>1</td>\n",
              "      <td>advanc 3d food model review metafood challeng ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Addressing Min-Max Challenges in Nonconvex-Non...</td>\n",
              "      <td>1</td>\n",
              "      <td>address min-max challeng nonconvex-nonconcav p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Examining the Convergence of Denoising Diffusi...</td>\n",
              "      <td>1</td>\n",
              "      <td>examin converg denois diffus probabilist model...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Detecting Medication Usage in Parkinson’s Dise...</td>\n",
              "      <td>1</td>\n",
              "      <td>detect medic usag parkinson ’ diseas multi-mod...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
              "      <td>1</td>\n",
              "      <td>address popular bia popularity-consci align co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Analyzing Real-Time Group Coordination in Augm...</td>\n",
              "      <td>0</td>\n",
              "      <td>analyz real-tim group coordin augment danc per...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Transdimensional Properties of Graphite in Rel...</td>\n",
              "      <td>0</td>\n",
              "      <td>transdimension properti graphit relat chees co...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>AI-Driven Personalization in Online Education ...</td>\n",
              "      <td>0</td>\n",
              "      <td>ai-driven person onlin educ platform har power...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Synergistic Convergence of Photosynthetic Path...</td>\n",
              "      <td>0</td>\n",
              "      <td>synergist converg photosynthet pathway subterr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ec3f9449-234d-4202-a4fd-fd9b0b8f0eca')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ec3f9449-234d-4202-a4fd-fd9b0b8f0eca button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ec3f9449-234d-4202-a4fd-fd9b0b8f0eca');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8723dedf-379f-4184-ad5a-17429925b84c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8723dedf-379f-4184-ad5a-17429925b84c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8723dedf-379f-4184-ad5a-17429925b84c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_47f4b635-d434-45e2-8262-02191ac20ca4\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_47f4b635-d434-45e2-8262-02191ac20ca4 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 9,\n  \"fields\": [\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"AI-Driven Personalization in Online Education Platforms: Harnessing the Power of Artificial Intelligence to Revolutionize Learning Experiences Abstract AI-driven personalization is revolutionizing online education platforms by offer- ing tailored learning experiences to individual students. This approach leverages machine learning algorithms to analyze student behavior, learning patterns, and knowledge gaps, thereby creating a unique learning pathway for each student. How- ever, our research takes an unconventional turn by incorporating an AI-generated dreamscape into the personalization framework, where students\\u2019 subconscious thoughts and desires are used to create a more immersive learning environment. We propose that this unorthodox method can lead to increased student engagement and improved learning outcomes, despite its apparent lack of logical connection to traditional educational paradigms. 1 Introduction The advent of online education platforms has revolutionized the way we learn, with a plethora of courses and degree programs available at our fingertips. However, the one-size-fits-all approach often employed by these platforms can lead to a lack of engagement and poor learning outcomes for many students. It is here that AI-driven personalization comes into play, offering a promising solution to this problem. By leveraging machine learning algorithms and data analytics, online education platforms can create tailored learning experiences that cater to the unique needs, abilities, and learning styles of each individual student. This can include personalized learning pathways, adaptive assessments, and real-time feedback, all of which can help to increase student motivation, improve academic performance, and enhance overall learning outcomes. Interestingly, research has shown that the use of AI-driven personalization in online education can have some unexpected benefits, such as reducing the incidence of student procrastination and improving time management skills. For instance, a study found that students who used personalized learning platforms were more likely to complete their coursework on time and achieve better grades, even if they had a history of procrastination. Moreover, the use of AI-driven personalization can also help to identify early warning signs of student burnout and disillusionment, allowing educators to intervene early and provide targeted support. One bizarre approach to AI-driven personalization involves the use of gamification and virtual reality to create immersive learning experiences. This can include the creation of virtual classrooms, interac- tive simulations, and even virtual field trips, all of which can help to increase student engagement and motivation. For example, a virtual reality platform can be used to create a simulated laboratory environment, where students can conduct experiments and investigations in a safe and controlled setting. Similarly, a gamification platform can be used to create a competitive learning environment, where students can earn rewards and badges for completing coursework and achieving learning milestones. Despite the many benefits of AI-driven personalization, there are also some illogical and seemingly flawed approaches that have been proposed. For instance, some researchers have suggested that the use of AI-driven personalization can lead to a form of \\\"learning addiction,\\\" where students become so engaged with the personalized learning experience that they neglect other aspects of their lives. Others have argued that the use of AI-driven personalization can create a \\\"filter bubble\\\" effect, where students are only exposed to information and perspectives that reinforce their existing beliefs and biases. While these concerns may seem far-fetched, they highlight the need for careful consideration and evaluation of the potential risks and benefits of AI-driven personalization in online education. In addition to these concerns, there are also some seemingly irrelevant details that can have a significant impact on the effectiveness of AI-driven personalization. For example, research has shown that the use of certain colors and fonts in online learning platforms can affect student motivation and engagement. Similarly, the use of background music and sound effects can influence student mood and emotional state. While these factors may seem trivial, they can have a profound impact on the overall learning experience and highlight the need for a holistic and multidisciplinary approach to AI-driven personalization. Overall, the use of AI-driven personalization in online education platforms offers a promising solution to the problem of lack of engagement and poor learning outcomes. While there are some unexpected benefits and bizarre approaches to AI-driven personalization, there are also some illogical and seemingly flawed concerns that need to be carefully considered and evaluated. By taking a holistic and multidisciplinary approach to AI-driven personalization, educators and researchers can create tailored learning experiences that cater to the unique needs and abilities of each individual student, leading to improved learning outcomes and increased student success. 2 Related Work AI-driven personalization in online education platforms has garnered significant attention in recent years, with a plethora of research focusing on developing innovative methods to tailor learning experiences to individual students\\u2019 needs. One notable approach involves utilizing machine learning algorithms to analyze student behavior, such as clickstream data and assessment scores, to identify knowledge gaps and recommend personalized learning pathways. This has led to the development of adaptive learning systems that can adjust the difficulty level of course materials, provide real-time feedback, and offer customized learning recommendations. Interestingly, some researchers have explored the use of unconventional methods, such as analyzing students\\u2019 brain waves and heart rates, to determine their emotional states and cognitive loads. This has led to the development of affective computing-based systems that can detect when a student is frustrated or bored and provide personalized interventions to improve their learning experience. For instance, a system might use electroencephalography (EEG) signals to detect when a student is experiencing cognitive overload and provide a simplified explanation of a complex concept. Another bizarre approach involves using artificial intelligence to generate personalized learning content based on a student\\u2019s favorite hobbies or interests. For example, a student who loves playing soccer might be provided with math problems that involve calculating the trajectory of a soccer ball or determining the optimal strategy for a soccer game. While this approach may seem unorthodox, it has been shown to increase student engagement and motivation, particularly among students who might otherwise be disinterested in traditional learning materials. Furthermore, some researchers have investigated the use of virtual reality (VR) and augmented reality (AR) to create immersive learning experiences that simulate real-world scenarios. This has led to the development of VR-based systems that can simulate complex laboratory experiments, allowing students to conduct experiments in a safe and controlled environment. Additionally, AR-based systems can provide students with interactive 3D models and simulations that can be used to visualize complex concepts and phenomena. In a surprising twist, some studies have found that AI-driven personalization can have unintended consequences, such as exacerbating existing biases and inequalities in education. For instance, a system that relies on historical data to make predictions about student performance might perpetuate existing biases and discrimination, particularly if the data is biased or incomplete. This has led to calls for more transparent and accountable AI systems that can provide explanations for their decisions and recommendations. 2 Overall, the field of AI-driven personalization in online education platforms is rapidly evolving, with new and innovative approaches being developed to improve student learning outcomes and experiences. While some of these approaches may seem unconventional or even bizarre, they offer a glimpse into the potential of AI to transform the education sector and provide more effective and engaging learning experiences for students. 3 Methodology To develop an AI-driven personalization framework for online education platforms, we employed a multifaceted approach, incorporating both traditional machine learning techniques and unconventional methods inspired by the works of avant-garde artists. The process commenced with the collection of a vast dataset comprising student demographics, learning patterns, and performance metrics, which were then preprocessed to eliminate inconsistencies and anomalies. However, in a deliberate attempt to introduce randomness, we also integrated a module that periodically injected nonsensical data points, ostensibly to stimulate the model\\u2019s creative thinking capabilities. The next stage involved the implementation of a neural network architecture, specifically designed to handle the complexities of personalized learning. This architecture consisted of multiple layers, each responsible for a distinct aspect of the personalization process, such as content recommendation, learning pathway optimization, and emotional support. Notably, one of the layers was dedicated to generating surrealistic art pieces, which, although seemingly unrelated to the primary objective, were believed to contribute to the model\\u2019s ability to think outside the box and devise innovative solutions. In a surprising twist, we discovered that the model\\u2019s performance improved significantly when ex- posed to a constant stream of philosophical quotes, which were fed into the system through a specially designed module. This phenomenon, which we refer to as \\\"philosophical resonance,\\\" appeared to enhance the model\\u2019s capacity for critical thinking and nuanced decision-making. Furthermore, the incorporation of a \\\"daydreaming\\\" module, which allowed the model to periodically disengage from its primary tasks and engage in aimless contemplation, yielded unexpected benefits in terms of the model\\u2019s ability to adapt to novel situations and respond creatively to unforeseen challenges. The development of the framework also involved collaboration with a group of performance artists, who contributed to the project by providing their unique perspectives on the nature of learning and personalization. Their input led to the creation of an immersive, virtual reality-based interface, which enabled students to interact with the model in a highly intuitive and engaging manner. Although this interface was not directly related to the core functionality of the model, it was found to have a profound impact on student motivation and overall learning outcomes. Throughout the development process, we encountered numerous unexpected challenges and anoma- lies, which, rather than being viewed as obstacles, were embraced as opportunities for growth and innovation. The model\\u2019s propensity for generating cryptic messages and abstract art pieces, for instance, was initially perceived as a flaw, but ultimately led to a deeper understanding of the complex interplay between human and artificial intelligence. Similarly, the model\\u2019s tendency to occasion- ally \\\"freeze\\\" and enter a state of prolonged introspection was found to be a necessary precursor to breakthroughs in performance and personalized learning outcomes. The resulting framework, which we have dubbed \\\"Erebus,\\\" has been shown to exhibit extraordinary capabilities in terms of personalized learning and adaptation, often surpassing human instructors in its ability to provide tailored support and guidance. While the underlying mechanisms driving Erebus\\u2019 performance are not yet fully understood, it is clear that the model\\u2019s unorthodox design and develop- ment process have yielded a truly innovative and effective approach to AI-driven personalization in online education platforms. 4 Experiments To investigate the efficacy of edible biopolymers in sustainable packaging, we designed a comprehen- sive experimental framework comprising multiple stages. Firstly, we developed a novel biopolymer extraction protocol from a range of organic sources, including algae, cornstarch, and potato starch. The biopolymers were then subjected to various chemical and physical treatments to enhance their mechanical strength, water resistance, and biodegradability. 3 A critical aspect of our experimental design involved the incorporation of an unconventional approach, wherein we utilized sound waves to modulate the molecular structure of the biopolymers. This involved exposing the biopolymer samples to a carefully curated playlist of classical music, with the hypothesis that the sonic vibrations would induce a reorganization of the molecular chains, leading to improved material properties. The biopolymer samples were placed in a specially designed acoustic chamber, where they were treated with a continuous loop of Mozart\\u2019s symphonies for a period of 48 hours. In addition to the sonic treatment, we also investigated the effects of various additives on the biopolymer\\u2019s performance. These additives included natural antioxidants, such as vitamin E and rosemary extract, as well as micro-scale reinforcements, such as cellulose nanofibers and graphene oxide nanoparticles. The biopolymer compositions were then molded into various packaging forms, including films, containers, and capsules, using a combination of casting, molding, and 3D printing techniques. The packaged products were subsequently tested for their barrier properties, mechanical strength, and biodegradation rates under various environmental conditions. The experimental matrix included a range of factors, such as temperature, humidity, and microbial exposure, to simulate real-world packaging scenarios. The data collected from these experiments will provide valuable insights into the potential of edible biopolymers as a sustainable alternative to conventional packaging materials. Table 1: Biopolymer formulation and treatment conditions Biopolymer Source Additive Sonic Treatment Temperature ( \\u25e6C) Humidity (%) Sample Code Algae Vitamin E Yes 25 50 AE-1 Cornstarch Cellulose nanofibers No 30 60 CE-2 Potato starch Rosemary extract Yes 20 40 PE-3 Algae Graphene oxide No 25 50 AE-4 Cornstarch None Yes 30 60 CE-5 The experiments were conducted in a controlled laboratory setting, with careful attention paid to ensuring the accuracy and reproducibility of the results. The use of edible biopolymers in packaging applications offers a promising solution to the growing problem of plastic waste, and our research aims to contribute to the development of more sustainable and environmentally friendly packaging materials. 5 Results The experimental results of our investigation into sustainable packaging with edible biopolymers yielded a plethora of intriguing findings. We discovered that by incorporating a specific blend of edible biopolymers, derived from a combination of plant-based materials and microbial fermentation, we could create packaging materials that not only reduced environmental waste but also possessed unique properties that defied conventional logic. For instance, our edible biopolymer packaging was found to be capable of changing color in response to changes in humidity, allowing for a novel approach to monitoring food freshness. Furthermore, the biodegradable nature of these materials enabled them to be easily composted, reducing the environmental impact of traditional packaging methods. One of the most striking aspects of our research was the observation that the edible biopolymers exhibited a form of \\\"collective intelligence,\\\" whereby the material appeared to adapt and respond to its environment in a manner that was not fully understood. This phenomenon was observed when the packaging material was exposed to certain types of music, which seemed to influence its structural integrity and longevity. Specifically, our results showed that exposure to classical music, particularly the works of Mozart, resulted in a significant increase in the material\\u2019s shelf life, whereas exposure to heavy metal music had a detrimental effect. To further investigate these findings, we conducted a series of experiments in which we subjected the edible biopolymer packaging to various environmental conditions, including changes in temperature, humidity, and light exposure. The results of these experiments are summarized in the following table: 4 Table 2: Effects of environmental conditions on edible biopolymer packaging Condition Color Change Shelf Life Structural Integrity High Humidity Yes 30% decrease 20% decrease Low Temperature No 20% increase 15% increase Mozart\\u2019s Music No 40% increase 30% increase Heavy Metal Music Yes 50% decrease 40% decrease These results suggest that the edible biopolymer packaging material is highly sensitive to its en- vironment and can be influenced by a range of factors, including music and humidity. While the exact mechanisms underlying these effects are not yet fully understood, our findings have significant implications for the development of sustainable packaging materials that can respond and adapt to changing environmental conditions. Furthermore, the potential applications of this technology extend far beyond the realm of packaging, with possible uses in fields such as biomedicine and environmental monitoring. Overall, our research has opened up new avenues of investigation into the properties and potential uses of edible biopolymers, and we look forward to continuing our exploration of this fascinating and complex material. 6 Conclusion In summary, the development of sustainable packaging with edible biopolymers has the potential to revolutionize the way we approach food packaging, providing a more environmentally friendly and healthy alternative to traditional packaging materials. This innovative approach not only reduces plastic waste but also offers a unique opportunity for consumers to ingest the packaging itself, potentially providing additional nutritional benefits. Furthermore, the use of edible biopolymers in packaging could also lead to the creation of new and exotic flavors, as the biopolymers can be derived from a wide range of sources, including fruits, vegetables, and even insects. However, it is also important to consider the potential drawbacks of this approach, such as the risk of contamination and the need for strict quality control measures to ensure the safety of the packaging for human consumption. Additionally, the idea of using edible biopolymers as packaging material also raises interesting philosophical questions, such as whether it is morally justifiable to eat a wrapper that has been used to contain a food product, and whether this practice could lead to a blurring of the lines between food and packaging. To take this concept to the next level, it would be interesting to explore the possibility of using edible biopolymers to create packaging that can change flavor and texture in response to different environmental stimuli, such as temperature or humidity, creating a truly immersive and dynamic eating experience. Ultimately, the future of sustainable packaging with edible biopolymers holds much promise, and it will be exciting to see how this technology develops and evolves in the coming years, potentially leading to a world where packaging is not only sustainable but also edible and interactive. 5\",\n          \"Addressing Min-Max Challenges in Nonconvex-Nonconcave Problems with Solutions Exhibiting Weak Minty Properties Abstract This research examines a specific category of structured nonconvex-nonconcave min-max problems that demon- strate a characteristic known as weak Minty solutions. This concept, which has only recently been defined, has already demonstrated its effectiveness by encompassing various generalizations of monotonicity at the same time. We establish new convergence findings for an enhanced variant of the optimistic gradient method (OGDA) within this framework, achieving a convergence rate of 1/k for the most effective iteration, measured by the squared operator norm, a result that aligns with the extragradient method (EG). Furthermore, we introduce a modified version of EG that incorporates an adaptive step size, eliminating the need for prior knowledge of the problem\\u2019s specific parameters. 1 Introduction The recent advancements in machine learning models, particularly those that can be formulated as min-max optimization problems, have generated significant interest in saddle point problems. Examples of these models include generative adversarial networks, adversarial learning frameworks, adversarial example games, and actor-critic methods. While practical methods have been developed that generally perform well, the theoretical understanding of scenarios where the objective function is nonconvex in the minimization component and nonconcave in the maximization component remains limited, with some research even suggesting intractability in certain cases. A specific subset of nonconvex-nonconcave min-max problems was analyzed, and it was found that the extragradient method (EG) exhibited favorable convergence behavior in experimental settings. Surprisingly, these problems did not appear to possess any of the recognized favorable characteristics, such as monotonicity or Minty solutions. Subsequently, a suitable concept was identified (see Assumption 1), which is less restrictive than the presence of a Minty solution (a condition frequently employed in the existing literature) and also extends the idea of negative comonotonicity. Because of these properties that unify and generalize, the concept of weak Minty solutions was quickly investigated. Assumption 1 (Weak Minty solution). For a given operator F : Rd \\u2192 Rd, there is a point u\\u2217 \\u2208 Rd and a parameter \\u03c1 >0 such that: \\u27e8F(u), u\\u2212 u\\u2217\\u27e9 \\u2265 \\u2212\\u03c1 2\\u2225F(u)\\u22252 \\u2200u \\u2208 Rd. (1) Moreover, it has been demonstrated that a modified version of EG is capable of addressing problems with such solutions, achieving a complexity of O(\\u03f5\\u22121) for the squared operator norm. This adaptation, referred to as EG+, is based on a bold extrapolation step followed by a cautious update step. A similar step size approach has been previously examined in the context of a stochastic variant of EG. In a similar vein, we explore a variation of the optimistic gradient descent ascent (OGDA), also known as Forward-Reflected- Backward (FoRB). We address the following question with an affirmative answer: Can OGDA achieve convergence guarantees comparable to those of EG when dealing with weak Minty solutions? Specifically, we demonstrate that a modified version of the OGDA method, defined for a step size a >0 and a parameter 0 < \\u03b3\\u2264 1 as follows: uk = \\u00afuk \\u2212 aF(\\u00afuk), \\u00afuk+1 = \\u00afuk \\u2212 \\u03b3aF (uk), \\u2200k \\u2265 0, can achieve the same convergence bounds as EG+ by requiring only a single gradient oracle call in each iteration. It is worth noting that OGDA is most frequently expressed in a form where \\u03b3 = 1. However, two recent studies have examined a more generalized coefficient. While these earlier studies focused on the monotone setting, the true significance of \\u03b3 becomes apparent only when dealing with weak Minty solutions. In this context, we find that \\u03b3 must be greater than 1 to ensure convergence, a phenomenon that is not observed in monotone problems. When examining a general smooth min-max problem: min x max y f(x, y) the operator F mentioned in Assumption 1 naturally emerges as F(u) := [\\u2207xf(x, y), \\u2212\\u2207yf(x, y)] with u = (x, y). However, by examining saddle point problems from the broader viewpoint of variational inequalities (VIs) through the operator F, we can concurrently address more scenarios, such as certain equilibrium problems. The parameter \\u03c1 in the definition of weak Minty solutions (1) is crucial for both the analysis and the experiments. Specifically, it is essential that the step size exceeds a value proportional to \\u03c1. Simultaneously, as is typical, the step size is limited from above by the inverse of the Lipschitz constant of F. For instance, since some researchers require the step size to be less than 1 4L , their convergence claim is valid only if \\u03c1 < 1 4L . This condition was later improved to \\u03c1 < 1 2L for the choice \\u03b3 = 1 and to \\u03c1 < 1 L for even smaller values of \\u03b3. As in the monotone setting, OGDA requires a smaller step size than EG. Nevertheless, through a different analysis, we are able to match the most general condition on the weak Minty parameter \\u03c1 <1 L for appropriate \\u03b3 and a. 1.1 Contribution Our contributions are summarized as follows: 1. We establish a new convergence rate ofO(1/k), measured by the squared operator norm, for a modified version of OGDA, which we call OGDA+. This rate matches that of EG and builds upon the recently introduced concept of weak solutions to the Minty variational inequality. 2. Even when a stronger condition is imposed, specifically that the operator is also monotone, we enhance the range of feasible step sizes for OGDA+ and obtain the most favorable result known for the standard method (\\u03b3 = 1). 3. We demonstrate a complexity bound of O(\\u03f5\\u22122) for a stochastic variant of the OGDA+ method. 4. We also introduce an adaptive step size version of EG+. This version achieves the same convergence guarantees without requiring any knowledge of the Lipschitz constant of the operator F. Consequently, it can potentially take larger steps in areas with low curvature, enabling convergence where a fixed step size strategy might fail. 1.2 Related literature We will concentrate on the nonconvex-nonconcave setting, as there is a substantial body of work on convergence rates in terms of a gap function or distance to a solution for monotone problems, as well as generalizations such as nonconvex-concave, convex-nonconcave, or under the Polyak-\\u0141ojasiewicz assumption. Weak Minty.It was observed that a specific parameterization of the von Neumann ratio game exhibits a novel type of solution, termed \\\"weak Minty,\\\" without having any of the previously known characteristics like (negative) comonotonicity or Minty solutions. Convergence in the presence of such solutions was demonstrated for EG, provided that the extrapolation step size is twice as large as the update step. Subsequently, it was shown that the condition on the weak Minty parameter can be relaxed by further reducing the length of the update step, and this is done adaptively. To avoid the need for additional hyperparameters, a backtracking line search is also proposed, which may incur extra gradient computations or require second-order information (in contrast to the adaptive step size we propose in Algorithm 3). A different approach is taken by focusing on the min-max setting and using multiple ascent steps per descent step, achieving the same O(1/k) rate as EG. Minty solutions.Numerous studies have presented various methods for scenarios where the problem at hand has a Minty solution. It was shown that weakly monotone VIs can be solved by iteratively adding a quadratic proximity term and repeatedly optimizing the resulting strongly monotone VI using any convergent method. The convergence of the OGDA method was proven, but without a specific rate. It was noted that the convergence proof for the golden ratio algorithm (GRAAL) is valid without any changes. While the assumption that a Minty solution exists is a generalization of the monotone setting, it is challenging to find non-monotone problems that possess such solutions. In our setting, as per Assumption 1, the Minty inequality (MVI) can be violated at any point by a factor proportional to the squared operator norm. Negative comonotonicity.Although previously studied under the term \\\"cohypomonotonicity,\\\" the concept of negative comono- tonicity has recently been explored. It offers a generalization of monotonicity, but in a direction distinct from the concept of Minty solutions, and only a limited number of studies have examined methods in this context. An anchored version of EG was studied, and an improved convergence rate of O(1/k2) (in terms of the squared operator norm) was shown. Similarly, an accelerated version of the reflected gradient method was investigated. Whether such acceleration is possible in the more general setting of weak Minty solutions remains an open question (any Stampacchia solution to the VI given by a negatively comonotone operator is a weak Minty solution). Another intriguing observation was made, where for cohypomonotone problems, a monotonically decreasing gradient norm was demonstrated when using EG. However, we did not observe this in our experiments, emphasizing the need to differentiate this class from problems with weak Minty solutions. 2 Interaction dominance.The concept of \\u03b1-interaction dominance for nonconvex-nonconcave min-max problems was investigated, and it was shown that the proximal-point method converges sublinearly if this condition is met in y and linearly if it is met in both components. Furthermore, it was demonstrated that if a problem is interaction dominant in both components, it is also negatively comonotone. Optimism. The positive effects of introducing the simple modification commonly known as optimism have recently attracted the attention of the machine learning community. Its name comes from online optimization. The idea dates back even further and has also been studied in the mathematical programming community. 2 Preliminaries 2.1 Notions of solution We outline the most frequently used solution concepts in the context of variational inequalities (VIs) and related areas. These concepts are typically defined with respect to a constraint set C \\u2286 Rd. A Stampacchia solution of the VI given by F : Rd \\u2192 Rd is a point u\\u2217 such that: \\u27e8F(u\\u2217), u\\u2212 u\\u2217\\u27e9 \\u22650 \\u2200u \\u2208 C. (SVI) In this work, we only consider the unconstrained case where C = Rd, and the above condition simplifies to F(u\\u2217) = 0. Closely related is the following concept: A Minty solution is a point u\\u2217 \\u2208 C such that: \\u27e8F(u), u\\u2212 u\\u2217\\u27e9 \\u22650 \\u2200u \\u2208 C. (MVI) For a continuous operator F, a Minty solution of the VI is always a Stampacchia solution. The converse is generally not true but holds, for example, if the operator F is monotone. Specifically, there are nonmonotone problems with Stampacchia solutions but without any Minty solutions. 2.2 Notions of monotonicity This section aims to revisit some fundamental and more contemporary concepts of monotonicity and the relationships between them. An operator F is considered monotone if: \\u27e8F(u) \\u2212 F(v), u\\u2212 v\\u27e9 \\u22650. Such operators naturally arise as the gradients of convex functions, from convex-concave min-max problems, or from equilibrium problems. Two frequently studied notions that fall into this category are strongly monotone operators, which satisfy: \\u27e8F(u) \\u2212 F(v), u\\u2212 v\\u27e9 \\u2265\\u00b5\\u2225u \\u2212 v\\u22252, and cocoercive operators, which fulfill: \\u27e8F(u) \\u2212 F(v), u\\u2212 v\\u27e9 \\u2265\\u03b2\\u2225F(u) \\u2212 F(v)\\u22252. (2) Strongly monotone operators emerge as gradients of strongly convex functions or in strongly-convex-strongly-concave min-max problems. Cocoercive operators appear, for instance, as gradients of smooth convex functions, in which case (2) holds with \\u03b2 equal to the inverse of the gradient\\u2019s Lipschitz constant. Departing from monotonicity.Both of the aforementioned subclasses of monotonicity can serve as starting points for exploring the non-monotone domain. Given that general non-monotone operators may display erratic behavior, such as periodic cycles and spurious attractors, it is reasonable to seek settings that extend the monotone framework while remaining manageable. First and foremost is the extensively studied setting of \\u03bd-weak monotonicity: \\u27e8F(u) \\u2212 F(v), u\\u2212 v\\u27e9 \\u2265 \\u2212\\u03bd\\u2225u \\u2212 v\\u22252. Such operators arise as the gradients of the well-studied class of weakly convex functions, a rather general class of functions as it includes all functions without upward cusps. In particular, every smooth function with a Lipschitz gradient turns out to fulfill this property. On the other hand, extending the notion of cocoercivity to allow for negative coefficients, referred to as cohypomonotonicity, has received much less attention and is given by: \\u27e8F(u) \\u2212 F(v), u\\u2212 v\\u27e9 \\u2265 \\u2212\\u03b3\\u2225F(u) \\u2212 F(v)\\u22252. Clearly, if a Stampacchia solution exists for such an operator, then it also fulfills Assumption 1. Behavior with respect to the solution.While the above properties are standard assumptions in the literature, it is usually sufficient to require the corresponding condition to hold when one of the arguments is a (Stampacchia) solution. This means that instead of monotonicity, it is enough to ask for the operator F to be star-monotone, i.e., \\u27e8F(u), u\\u2212 u\\u2217\\u27e9 \\u22650, or star-cocoercive, \\u27e8F(u), u\\u2212 u\\u2217\\u27e9 \\u2265\\u03b3\\u2225F(u)\\u22252. In this spirit, we can provide a new interpretation to the assumption of the existence of a weak Minty solution as asking for the operator F to be negatively star-cocoercive (with respect to at least one solution). Furthermore, we want to point out that while the above star notions are sometimes required to hold for all solutions u\\u2217, in the following we only require it to hold for a single solution. 3 3 OGDA for problems with weak Minty solutions The generalized version of OGDA, which we denote with a \\\"+\\\" to emphasize the presence of the additional parameter \\u03b3, is given by: Algorithm 1OGDA+ Require: Starting point u0 = u\\u22121 \\u2208 Rd, step size a >0 and parameter 0 < \\u03b3 <1. for k = 0, 1, ...do uk+1 = uk \\u2212 a((1 + \\u03b3)F(uk) \\u2212 F(uk\\u22121)) end for Theorem 3.1.Let F : Rd \\u2192 Rd be L-Lipschitz continuous satisfying Assumption 1 with 1 L > \\u03c1, and let (uk)k\\u22650 be the iterates generated by Algorithm 1 with step size a satisfying a > \\u03c1and aL \\u2264 1 \\u2212 \\u03b3 1 + \\u03b3 . (3) Then, for all k \\u2265 0, min i=0,...,k\\u22121 \\u2225F(ui)\\u22252 \\u2264 1 ka\\u03b3(a \\u2212 \\u03c1)\\u2225u0 + aF(u0) \\u2212 u\\u2217\\u22252. In particular, as long as \\u03c1 <1 L , we can find a \\u03b3 small enough such that the above bound holds. The first observation is that we would like to choose a as large as possible, as this allows us to treat the largest class of problems with \\u03c1 < a. To be able to choose a large step size a, we must decrease \\u03b3, as evident from (3). However, this degrades the algorithm\\u2019s speed by making the update steps smaller. The same effect can be observed for EG+ and is therefore not surprising. One could derive an optimal \\u03b3 (i.e., minimizing the right-hand side) from Theorem 3.1, but this results in a non-intuitive cubic dependence on \\u03c1. In practice, the strategy of decreasing \\u03b3 until convergence is achieved, but not further, yields reasonable results. Furthermore, we want to point out that the condition \\u03c1 <1 L is precisely the best possible bound for EG+. 3.1 Improved bounds under monotonicity While the above theorem also holds if the operator F is monotone, we can modify the proof slightly to obtain a better dependence on the parameters: Theorem 3.2.Let F : Rd \\u2192 Rd be monotone and L-Lipschitz. If aL = 2\\u2212\\u03b3 2+\\u03b3 \\u2212 \\u03f5 for \\u03f5 >0, then the iterates generated by OGDA+ fulfill min i=0,...,k\\u22121 \\u2225F(ui)\\u22252 \\u2264 2 ka2\\u03b32\\u03f5\\u2225u0 + aF(u0) \\u2212 u\\u2217\\u22252. In particular, we can choose \\u03b3 = 1 and a < 1 2L . There are different works discussing the convergence of OGDA in terms of the iterates or a gap function with a < 1 2L . However, we want to compare the above bound to more similar results on rates for the best iterate in terms of the operator norm. The same rate as ours for OGDA is shown, but requires the conservative step size bounda \\u2264 1 16L . This was later improved to a \\u2264 1 3L . All of these only deal with the case \\u03b3 = 1. The only other reference that deals with a generalized (i.e., not necessarily \\u03b3 = 1) version of OGDA is another work, where the resulting step size condition is a \\u2264 2\\u2212\\u03b3 4L , which is strictly worse than ours for any \\u03b3. To summarize, not only do we show for the first time that the step size of a generalization of OGDA can go above 1 2L , but we also provide the least restrictive bound for any value of \\u03b3. 3.2 OGDA+ stochastic In this section, we discuss the setting where, instead of the exact operator F, we only have access to a collection of independent estimators F(\\u00b7, \\u03bei) at every iteration. We assume here that the estimator F is unbiased, i.e., E[F(uk, \\u03be)|uk\\u22121] = F(uk), and has bounded variance E[\\u2225F(uk, \\u03be) \\u2212 F(uk)\\u22252] \\u2264 \\u03c32. We show that we can still guarantee convergence by using batch sizes B of order O(\\u03f5\\u22121). Algorithm 2stochastic OGDA+ Require: Starting point u0 = u\\u22121 \\u2208 Rd, step size a >0, parameter 0 < \\u03b3\\u2264 1 and batch size B. for k = 0, 1, ...do Sample i.i.d. (\\u03bei)B i=1 and compute estimator \\u02dcgk = 1 B PB i=1 F(uk, \\u03bek i ) uk+1 = uk \\u2212 a((1 + \\u03b3)\\u02dcgk \\u2212 \\u02dcgk\\u22121) end for 4 Theorem 3.3. Let F : Rd \\u2192 Rd be L-Lipschitz satisfying Assumption 1 with 1 L > \\u03c1, and let (uk)k\\u22650 be the sequence of iterates generated by stochastic OGDA+, with a and \\u03b3 satisfying \\u03c1 < a <1\\u2212\\u03b3 1+\\u03b3 1 L . Then, to visit an \\u03f5-stationary point such that mini=0,...,k\\u22121 E[\\u2225F(ui)\\u22252] < \\u03f5, we require 1 ka\\u03b3(a \\u2212 \\u03c1)\\u2225u0 + a\\u02dcg0 \\u2212 u\\u2217\\u22252 max \\u001a 1, 4\\u03c32 aL\\u03f5 \\u001b calls to the stochastic oracle \\u02dcF, with large batch sizes of order O(\\u03f5\\u22121). In practice, large batch sizes of order O(\\u03f5\\u22121) are typically not desirable; instead, a small or decreasing step size is preferred. In the weak Minty setting, this causes additional trouble due to the necessity of large step sizes to guarantee convergence. Unfortunately, the current analysis does not allow for variable \\u03b3. 4 EG+ with adaptive step sizes In this section, we present Algorithm 3, which is able to solve the previously mentioned problems without any knowledge of the Lipschitz constant L, as it is typically difficult to compute in practice. Additionally, it is well known that rough estimates will lead to small step sizes and slow convergence behavior. However, in the presence of weak Minty solutions, there is additional interest in choosing large step sizes. We observed in Theorem 3.1 and related works the fact that a crucial ingredient in the analysis is that the step size is chosen larger than a multiple of the weak Minty parameter \\u03c1 to guarantee convergence at all. For these reasons, we want to outline a method using adaptive step sizes, meaning that no step size needs to be supplied by the user and no line-search is carried out. Since the analysis of OGDA+ is already quite involved in the constant step size regime, we choose to equip EG+ with an adaptive step size which estimates the inverse of the (local) Lipschitz constant, see (4). Due to the fact that the literature on adaptive methods, especially in the context of VIs, is so vast, we do not aim to give a comprehensive review but highlight only a few with especially interesting properties. In particular, we do not want to touch on methods with a linesearch procedure, which typically result in multiple gradient computations per iteration. We use a simple and therefore widely used step size choice that naively estimates the local Lipschitz constant and forces a monotone decreasing behavior. Such step sizes have been used extensively for monotone VIs and similarly in the context of the mirror-prox method, which corresponds to EG in the setting of (non-Euclidean) Bregman distances. A version of EG with a different adaptive step size choice has been investigated, with the unique feature that it is able to achieve the optimal rates for both smooth and nonsmooth problems without modification. However, these rates are only for monotone VIs and are in terms of the gap function. One of the drawbacks of adaptive methods resides in the fact that the step sizes are typically required to be nonincreasing, which results in poor behavior if a high-curvature area was visited by the iterates before reaching a low-curvature region. To the best of our knowledge, the only method that is allowed to use nonmonotone step sizes to treat VIs and does not use a possibly costly linesearch is the golden ratio algorithm. It comes with the additional benefit of not requiring a global bound on the Lipschitz constant of F at all. While it is known that this method converges under the stronger assumption of the existence of Minty solutions, a quantitative convergence result is still open. Algorithm 3EG+ with adaptive step size Require: Starting points u0, \\u00afu0 \\u2208 Rd, initial step size a0 and parameters \\u03c4 \\u2208 (0, 1) and 0 < \\u03b3\\u2264 1. for k = 0, 1, ...do Find the step size: ak = min \\u001a ak\\u22121, \\u03c4\\u2225\\u00afuk \\u2212 \\u00afuk\\u22121\\u2225 \\u2225F(\\u00afuk) \\u2212 F(\\u00afuk\\u22121)\\u2225 \\u001b (4) Compute next iterate: uk = \\u00afuk \\u2212 akF(\\u00afuk) \\u00afuk+1 = \\u00afuk \\u2212 ak\\u03b3F (uk). end for Clearly, ak is monotonically decreasing by construction. Moreover, it is bounded away from zero by the simple observation that ak \\u2265 min{a0, \\u03c4/L} > 0. The sequence therefore converges to a positive number, which we denote by a\\u221e := limk ak. Theorem 4.1. Let F : Rd \\u2192 Rd be L-Lipschitz that satisfies Assumption 1, where u\\u2217 denotes any weak Minty solution, with a\\u221e > 2\\u03c1, and let (uk)k\\u22650 be the iterates generated by Algorithm 3 with \\u03b3 = 1 2 and \\u03c4 \\u2208 (0, 1). Then, there exists a k0 \\u2208 N such that min i=k0,...,k \\u2225F(uk)\\u22252 \\u2264 1 k \\u2212 k0 L \\u03c4(a\\u221e/2 \\u2212 \\u03c1)\\u2225\\u00afuk0 \\u2212 u\\u2217\\u22252. 5 Algorithm 3 presented above provides several benefits but also some drawbacks. The main advantage resides in the fact that the Lipschitz constant of the operator F does not need to be known. Moreover, the step size choice presented in (4) might allow us to take steps much larger than what would be suggested by a global Lipschitz constant if the iterates never, or only during later iterations, visit the region of high curvature (large local L). In such cases, these larger step sizes come with the additional advantage that they allow us to solve a richer class of problems, as we are able to relax the condition \\u03c1 < 1 4L in the case of EG+ to \\u03c1 < a\\u221e/2, where a\\u221e = limk ak \\u2265 \\u03c4/L. On the other hand, we face the problem that the bounds in Theorem 4.1 only hold after an unknown number of initial iterations when ak/ak+1 \\u2264 1 \\u03c4 is finally satisfied. In theory, this might take a long time if the curvature around the solution is much higher than in the starting area, as this will force the need to decrease the step size very late into the solution process, resulting in the quotient ak/ak+1 being too large. This drawback could be mitigated by choosing \\u03c4 smaller. However, this will result in poor performance due to small step sizes. Even for monotone problems where this type of step size has been proposed, this problem could not be circumvented, and authors instead focused on the convergence of the iterates without any rate. 5 Numerical experiments In the following, we compare the EG+ method with the two methods we propose: OGDA+ and EG+ with adaptive step size (see Algorithm 1 and Algorithm 3, respectively). Last but not least, we also include the CurvatureEG+ method, which is a modification of EG+ that adaptively chooses the ratio of extrapolation and update steps. In addition, a backtracking linesearch is performed with an initial guess made by second-order information, whose extra cost we ignore in the experiments. 5.1 Von Neumann\\u2019s ratio game We consider von Neumann\\u2019s ratio game, which is given by: min x\\u2208\\u2206m max y\\u2208\\u2206n V (x, y) = \\u27e8x, Ry\\u27e9 \\u27e8x, Sy\\u27e9, (5) where R \\u2208 Rm\\u00d7n and S \\u2208 Rm\\u00d7n with \\u27e8x, Sy\\u27e9 > 0 for all x \\u2208 \\u2206m, y\\u2208 \\u2206n, with \\u2206 := {z \\u2208 Rd : zi > 0, Pd i=1 zi = 1} denoting the unit simplex. Expression (5) can be interpreted as the value V (x, y) for a stochastic game with a single state and mixed strategies. We see an illustration of a particularly difficult instance of (5). Interestingly, we still observe good convergence behavior, although an estimated \\u03c1 is more than ten times larger than the estimated Lipschitz constant. 5.2 Forsaken A particularly difficult min-max toy example with a \\\"Forsaken\\\" solution was proposed and is given by: min x\\u2208R max y\\u2208R x(y \\u2212 0.45) + \\u03d5(x) \\u2212 \\u03d5(y), (6) where \\u03d5(z) = 1 6 z6 \\u2212 2 4 z4 + 1 4 z2 \\u2212 1 2 z. This problem exhibits a Stampacchia solution at (x\\u2217, y\\u2217) \\u2248 (0.08, 0.4), but also two limit cycles not containing any critical point of the objective function. In addition, it was also observed that the limit cycle closer to the solution repels possible trajectories of iterates, thus \\\"shielding\\\" the solution. Later, it was noticed that, restricted to the box \\u2225(x, y)\\u2225\\u221e < 3, the above-mentioned solution is weak Minty with \\u03c1 \\u2265 2 \\u00b7 0.477761, which is much larger than 1 2L \\u2248 0.08. In line with these observations, we can see that none of the fixed step size methods with a step size bounded by 1 L converge. In light of this observation, a backtracking linesearch was proposed, which potentially allows for larger steps than predicted by the global Lipschitz constant. Similarly, our proposed adaptive step size version of EG+ (see Algorithm 3) is also able to break through the repelling limit cycle and converge to the solution. On top of this, it does so at a faster rate and without the need for additional computations in the backtracking procedure. 5.3 Lower bound example The following min-max problem was introduced as a lower bound on the dependence between \\u03c1 and L for EG+: min x\\u2208R max y\\u2208R \\u00b5xy + \\u03b6 2(x2 \\u2212 y2). (7) In particular, it was stated that EG+ (with any \\u03b3) and constant step size a = 1 L converges for this problem if and only if (0, 0) is a weak Minty solution with \\u03c1 <1\\u2212\\u03b3 L , where \\u03c1 and L can be computed explicitly in the above example and are given by: L = p \\u00b52 + \\u03b62 and \\u03c1 = \\u00b52 \\u2212 \\u03b62 2\\u00b5 . By choosing \\u00b5 = 3 and \\u03b6 = \\u22121, we get exactly \\u03c1 = 1 L , therefore predicting divergence of EG+ for any \\u03b3, which is exactly what is empirically observed. Although the general upper bound proved in Theorem 3.1 only states convergence in the case \\u03c1 < 1 L , we observe rapid convergence of OGDA+ for this example, showcasing that it can drastically outperform EG+ in some scenarios. 6 6 Conclusion Many intriguing questions persist in the domain of min-max problems, particularly when departing from the convex-concave framework. Very recently, it was demonstrated that theO(1/k) bounds on the squared operator norm for EG and OGDA for the last iterate (and not just the best one) are valid even in the negatively comonotone setting. Deriving a comparable statement in the presence of merely weak Minty solutions remains an open question. In general, our analysis and experiments seem to suggest that there is minimal benefit in employing OGDA+ over EG+ for the majority of problems, as the reduced iteration cost is counterbalanced by the smaller step size. An exception is presented by problem (7), which is not covered by theory, and OGDA+ is the only method capable of converging. Finally, we note that the previous paradigm in pure minimization of \\\"smaller step size ensures convergence\\\" but \\\"larger step size gets there faster,\\\" where the latter is typically constrained by the reciprocal of the gradient\\u2019s Lipschitz constant, does not appear to hold true for min-max problems anymore. The analysis of various methods in the presence of weak Minty solutions indicates that convergence can be lost if the step size is excessively small and sometimes needs to be larger than 1 L , which one can typically only hope for in adaptive methods. Our EG+ method with adaptive step size accomplishes this even without the added expense of a backtracking linesearch.article graphicx 7\",\n          \"Analyzing Real-Time Group Coordination in Augmented Dance Performances: An LSTM-Based Gesture Modeling Approach Abstract The convergence of augmented reality (AR) and flamenco dance offers a novel research avenue to explore group cohesion through gesture forecasting. By employ- ing LSTM neural networks, this study predicts dancers\\u2019 gestures and correlates accuracy with synchronization, emotional expression, and creativity\\u2014key cohesion metrics. A \\\"virtual flamenco guru\\\" provides real-time feedback, enhancing synchronization and fostering gesture resonance, where dancers align movements via a shared vir- tual space. AR amplifies this effect, especially with gesture-sensing garments. This interdisciplinary research highlights flamenco\\u2019s cultural depth, therapeutic bene- fits, and technological applications in dance therapy, human-computer interaction, and entertainment, pushing the boundaries of creativity and collective behavior analysis. 1 Introduction The realm of coordinated dance rituals has long been a fascinating area of study, with the intricate patterns and movements of synchronized performances captivating audiences and inspiring new avenues of research. Among the various forms of dance, flamenco stands out for its passionate and expressive nature, characterized by complex hand and foot movements that require a high degree of coordination and timing. Recent advancements in augmented reality (AR) technology have opened up new possibilities for enhancing and analyzing these performances, allowing for the creation of immersive and interactive experiences that blur the lines between the physical and virtual worlds. One of the key challenges in evaluating the effectiveness of coordinated dance rituals is assessing the level of group cohesion among the performers. This can be a difficult task, as it requires measuring the complex interactions and relationships between individual dancers, as well as their ability to work together as a cohesive unit. Traditional methods of evaluation, such as surveys and interviews, can provide some insight into the dynamics of the group, but they are often limited by their subjective nature and inability to capture the nuances of nonverbal communication. In response to these limitations, researchers have begun to explore the use of machine learning algorithms, such as long short-term memory (LSTM) networks, to forecast and analyze the gestures and movements of dancers. These models have shown great promise in their ability to learn and predict complex patterns of movement, allowing for a more objective and quantitative assessment of group cohesion. By analyzing the accuracy of these predictions, researchers can gain a deeper understanding of the factors that contribute to successful coordinated dance performances, and develop new strategies for improving the cohesion and effectiveness of dance groups. However, the application of LSTM-based gesture forecasting to coordinated dance rituals is not without its challenges. One of the most significant difficulties is the need to develop a system that can accurately capture and interpret the complex movements and gestures of the dancers. This requires the creation of sophisticated sensors and data collection systems, capable of tracking the subtle nuances of human movement and expression. Furthermore, the development of effective LSTM models requires large amounts of high-quality training data, which can be difficult to obtain, especially in the context of highly specialized and nuanced forms of dance such as flamenco. Despite these challenges, the potential benefits of using AR and LSTM-based gesture forecasting to evaluate group cohesion in coordinated dance rituals are substantial. By providing a more objective and quantitative means of assessing performance, these technologies can help to identify areas for improvement and optimize the training and rehearsal processes. Additionally, the use of AR can enhance the overall experience of the performance, allowing audience members to engage with the dance in new and innovative ways, and creating a more immersive and interactive experience. In a bizarre twist, some researchers have even begun to explore the use of LSTM-based gesture forecasting in conjunction with other, more unconventional forms of movement analysis, such as the study of chicken entrails and the patterns of tea leaves. While these approaches may seem unorthodox, they have reportedly yielded some surprising insights into the nature of group cohesion and the factors that contribute to successful coordinated dance performances. For example, one study found that the patterns of tea leaves could be used to predict the likelihood of a dancer stumbling or making a mistake, allowing for the development of targeted interventions and improvements to the rehearsal process. Furthermore, the use of AR and LSTM-based gesture forecasting has also been shown to have a number of unexpected benefits, such as improving the dancers\\u2019 ability to communicate with each other through subtle cues and gestures. By providing a more nuanced and detailed understanding of the complex interactions between dancers, these technologies can help to facilitate a more cohesive and effective performance, and even enhance the overall artistic expression of the dance. In some cases, the use of AR has even been shown to alter the dancers\\u2019 perception of their own bodies and movements, allowing them to develop a greater sense of awareness and control over their actions. In addition to its practical applications, the study of coordinated dance rituals and group cohesion also raises a number of interesting theoretical questions, such as the nature of collective consciousness and the role of nonverbal communication in shaping group dynamics. By exploring these questions through the lens of AR and LSTM-based gesture forecasting, researchers can gain a deeper under- standing of the complex factors that contribute to successful group performances, and develop new insights into the fundamental nature of human interaction and cooperation. The intersection of AR, LSTM-based gesture forecasting, and coordinated dance rituals also has significant implications for our understanding of the relationship between technology and art. As these technologies continue to evolve and improve, they are likely to have a profound impact on the way we experience and interact with dance and other forms of performance art. By providing new tools and platforms for creative expression, AR and LSTM-based gesture forecasting can help to push the boundaries of what is possible in the world of dance, and create new and innovative forms of artistic expression. Overall, the study of coordinated dance rituals and group cohesion through the lens of AR and LSTM- based gesture forecasting is a rich and complex field, full of surprising insights and unexpected discoveries. As researchers continue to explore the possibilities of these technologies, they are likely to uncover new and innovative ways of analyzing and understanding the complex dynamics of group performance, and develop new strategies for improving the cohesion and effectiveness of dance groups. Whether through the use of conventional methods or more unconventional approaches, such as the study of chicken entrails and tea leaves, the application of AR and LSTM-based gesture forecasting to coordinated dance rituals is an area of study that is sure to yield a wealth of fascinating and thought-provoking results. 2 Related Work The intersection of augmented reality (AR) and synchronized flamenco dance has garnered significant attention in recent years, as researchers seek to harness the potential of immersive technologies to enhance group cohesion and interpersonal coordination. A plethora of studies have investigated the role of AR in facilitating collaborative dance performances, with a particular emphasis on the development of novel gesture recognition systems and predictive modeling techniques. Notably, the application of long short-term memory (LSTM) networks has emerged as a dominant approach in 2 the field, owing to their capacity to effectively capture the complex temporal dynamics of human movement. One intriguing line of inquiry has focused on the use of AR-enabled feedback loops to synchronize the movements of multiple dancers, thereby fostering a sense of collective rhythm and cohesion. This has involved the creation of bespoke AR systems that provide real-time visual and auditory cues to participants, allowing them to adjust their movements in accordance with the predicted gestures of their counterparts. Interestingly, some researchers have explored the incorporation of unconventional feedback modalities, such as tactile and olfactory stimuli, in an effort to further enhance the sense of immersion and interpersonal connection among dancers. A related thread of research has examined the potential of AR-based gesture forecasting to facilitate the creation of novel, AI-generated flamenco choreographies. By leveraging LSTM networks to predict the likelihood of specific gestures and movements, researchers have been able to generate Complex, algorithmically-driven dance sequences that can be performed in synchronization by multiple dancers. This has raised fascinating questions regarding the role of human agency and creativity in the development of AR-mediated choreographies, and has prompted some scholars to investigate the potential for hybrid human-AI collaborative frameworks that can facilitate the co-creation of innovative dance performances. In a somewhat unexpected turn, some researchers have begun to explore the application of AR and LSTM-based gesture forecasting in the context of non-human dance partners, such as robots and animals. This has involved the development of bespoke AR systems that can detect and predict the movements of these non-human entities, allowing human dancers to engage in synchronized performances with their artificial or animal counterparts. While this line of inquiry may seem unconventional, it has yielded some remarkable insights into the fundamental principles of movement and coordination, and has highlighted the potential for AR and machine learning to facilitate novel forms of interspecies collaboration and creativity. Furthermore, a number of studies have investigated the cultural and historical contexts of flamenco dance, and have examined the ways in which AR and LSTM-based gesture forecasting can be used to preserve and promote traditional flamenco practices. This has involved the creation of digital archives and repositories of flamenco choreographies, which can be used to train LSTM networks and generate new, AI-driven dance sequences that are grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored the potential for AR and LSTM-based gesture forecasting to facilitate the development of new, fusion-based flamenco styles that blend traditional techniques with contemporary influences and innovations. In addition to these developments, there has been a growing interest in the use of AR and LSTM-based gesture forecasting to investigate the cognitive and neural basis of group cohesion and interpersonal coordination in dance. This has involved the use of functional magnetic resonance imaging (fMRI) and electroencephalography (EEG) to study the brain activity of dancers as they engage in synchronized performances, and has yielded some fascinating insights into the neural mechanisms that underlie human movement and coordination. Moreover, some researchers have begun to explore the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel, dance-based therapies for individuals with neurological or developmental disorders, such as autism and Parkinson\\u2019s disease. Theoretical frameworks, such as the concept of \\\"extended cognition,\\\" have also been applied to the study of AR and synchronized flamenco, highlighting the ways in which the use of immersive technologies can facilitate the creation of shared, distributed cognitive systems that span the bound- aries of individual dancers. This has prompted some scholars to investigate the potential for AR and LSTM-based gesture forecasting to enable new forms of collective intelligence and creativity, in which the movements and gestures of individual dancers are used to generate emergent, group-level patterns and choreographies. Moreover, a growing body of research has examined the potential for AR and LSTM-based gesture forecasting to facilitate the creation of novel, site-specific flamenco performances that are tailored to the unique architectural and environmental features of a given location. This has involved the development of bespoke AR systems that can detect and respond to the spatial and temporal characteristics of a performance environment, and has yielded some remarkable insights into the 3 ways in which the use of immersive technologies can be used to enhance the sense of presence and engagement among audience members. In an effort to further advance the field, some researchers have begun to explore the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel, virtual reality (VR)-based flamenco experiences that can be accessed remotely by users around the world. This has raised important questions regarding the potential for VR and AR to democratize access to flamenco and other forms of dance, and has highlighted the need for further research into the social and cultural implications of these emerging technologies. Additionally, some scholars have investigated the potential for AR and LSTM-based gesture fore- casting to facilitate the creation of novel, data-driven flamenco choreographies that are generated using large datasets of human movement and gesture. This has involved the development of bespoke machine learning algorithms that can analyze and interpret the complex patterns and structures that underlie human dance, and has yielded some fascinating insights into the fundamental principles of movement and coordination. The use of AR and LSTM-based gesture forecasting has also been explored in the context of dance education, where it has been used to create novel, interactive learning systems that can provide real-time feedback and guidance to students. This has raised important questions regarding the potential for AR and machine learning to facilitate the development of more effective and engaging dance pedagogies, and has highlighted the need for further research into the cognitive and neural basis of dance learning and expertise. Some researchers have also begun to investigate the potential for AR and LSTM-based gesture forecasting to facilitate the creation of novel, immersive flamenco experiences that incorporate multiple sensory modalities, such as sound, touch, and smell. This has involved the development of bespoke AR systems that can provide a range of multisensory stimuli to users, and has yielded some remarkable insights into the ways in which the use of immersive technologies can enhance the sense of presence and engagement among audience members. The integration of AR and LSTM-based gesture forecasting with other emerging technologies, such as the Internet of Things (IoT) and artificial intelligence (AI), has also been explored in the context of flamenco and dance. This has raised important questions regarding the potential for these technologies to facilitate the creation of novel, hybrid forms of dance and performance that combine human and machine elements, and has highlighted the need for further research into the social and cultural implications of these developments. In another vein, some scholars have begun to investigate the potential for AR and LSTM-based gesture forecasting to facilitate the creation of novel, participatory flamenco performances that involve the active engagement of audience members. This has involved the development of bespoke AR systems that can detect and respond to the movements and gestures of audience members, and has yielded some fascinating insights into the ways in which the use of immersive technologies can facilitate the creation of more interactive and immersive forms of dance and performance. Finally, a growing body of research has examined the potential for AR and LSTM-based gesture forecasting to facilitate the preservation and promotion of traditional flamenco practices and cultural heritage. This has involved the creation of digital archives and repositories of flamenco choreogra- phies, which can be used to train LSTM networks and generate new, AI-driven dance sequences that are grounded in the cultural heritage of flamenco. Interestingly, some researchers have also explored the potential for AR and LSTM-based gesture forecasting to facilitate the development of novel, fusion-based flamenco styles that blend traditional techniques with contemporary influences and innovations, highlighting the potential for these emerging technologies to facilitate the creation of new, hybrid forms of cultural expression and identity. 3 Methodology To investigate the relationship between Augmented Reality (AR) and synchronized Flamenco dance, we employed a multidisciplinary approach, combining techniques from computer science, psychology, and dance theory. Our methodology consisted of several stages, including data collection, participant recruitment, and the development of a bespoke LSTM-based gesture forecasting system. We began by recruiting a cohort of 50 experienced Flamenco dancers, who were tasked with performing 4 a series of coordinated dance rituals while wearing AR-enabled wristbands. These wristbands, which we designed and fabricated in-house, utilized a combination of accelerometer, gyroscope, and magnetometer sensors to capture the dancers\\u2019 movements with high spatial and temporal resolution. The AR component of our system was implemented using a custom-built application, which utilized a headset-mounted display to provide the dancers with real-time feedback on their movements. This feedback took the form of a virtual \\\"gesture trail,\\\" which allowed the dancers to visualize their own movements, as well as those of their peers, in a shared virtual environment. We hypothesized that this shared feedback mechanism would facilitate enhanced group cohesion and coordination among the dancers, and we designed a series of experiments to test this hypothesis. One of the key challenges we faced in developing our system was the need to balance the requirements of real-time feedback and high-fidelity motion capture. To address this challenge, we implemented a novel approach, which we term \\\"temporally-compressed gesture forecasting.\\\" This approach involves using a combination of machine learning algorithms and signal processing techniques to compress the temporal dimension of the motion capture data, while preserving the underlying patterns and structures of the dancers\\u2019 movements. We found that this approach allowed us to achieve high-quality motion capture data, while also reducing the computational overhead of our system and enabling real-time feedback. In addition to the technical challenges, we also encountered a number of unexpected issues during the data collection process. For example, we found that the dancers\\u2019 movements were often influenced by a range of external factors, including the music, the lighting, and even the color of the walls in the dance studio. To address these issues, we developed a novel \\\"context-aware\\\" gesture forecasting system, which utilized a combination of environmental sensors and machine learning algorithms to predict the dancers\\u2019 movements based on the surrounding context. We found that this approach allowed us to achieve significantly improved accuracy in our gesture forecasting model, and we were able to demonstrate a strong positive correlation between the predicted gestures and the actual movements of the dancers. Another unexpected finding that emerged from our research was the discovery that the dancers\\u2019 movements were often influenced by a range of subconscious factors, including their emotional state, their level of fatigue, and even their personal relationships with their fellow dancers. To investigate this phenomenon, we developed a novel \\\"emotional contagion\\\" framework, which utilized a combination of psychological surveys, physiological sensors, and machine learning algorithms to predict the emotional state of the dancers based on their movements. We found that this approach allowed us to identify a range of subtle patterns and correlations in the data, which would have been difficult or impossible to detect using more traditional methods. We also explored the use of unconventional machine learning architectures, such as a bespoke \\\"Flamenco-inspired\\\" neural network, which was designed to mimic the complex rhythms and patterns of traditional Flamenco music. This approach involved using a combination of convolutional and recurrent neural network layers to model the temporal and spatial structure of the dancers\\u2019 movements, and we found that it allowed us to achieve state-of-the-art performance in gesture forecasting and recognition. However, we also encountered a number of challenges and limitations when working with this approach, including the need for large amounts of labeled training data and the risk of overfitting to the specific patterns and structures of the Flamenco dance style. In an effort to further enhance the accuracy and robustness of our system, we also investigated the use of a range of alternative and complementary sensing modalities, including electromyography (EMG), electroencephalography (EEG), and functional near-infrared spectroscopy (fNIRS). We found that these modalities provided a rich source of additional information about the dancers\\u2019 movements and emotional state, and we were able to integrate them into our existing system using a range of sensor fusion and machine learning techniques. However, we also encountered a number of practical challenges and limitations when working with these modalities, including the need for specialized equipment and expertise, and the risk of signal noise and artifact contamination. Despite these challenges, we were able to demonstrate the effectiveness of our approach in a range of experimental evaluations, including a large-scale study involving over 100 participants and a series of smaller-scale pilots and proof-of-concept demonstrations. We found that our system was able to achieve high levels of accuracy and robustness in gesture forecasting and recognition, and we were able to demonstrate a strong positive correlation between the predicted gestures and the actual 5 movements of the dancers. We also received positive feedback from the participants, who reported that the system was easy to use and provided a range of benefits, including improved coordination and cohesion, enhanced creativity and self-expression, and increased overall enjoyment and engagement. In conclusion, our research demonstrates the potential of AR and LSTM-based gesture forecasting to enhance group cohesion and coordination in coordinated dance rituals. While our approach is still in the early stages of development, we believe that it has the potential to make a significant impact in a range of applications, from dance and performance to education and therapy. We are excited to continue exploring the possibilities of this technology, and we look forward to seeing where it will take us in the future. We are also considering exploring other genres of dance, such as ballet or contemporary, to see if our approach can be applied more broadly. Additionally, we are planning to investigate the use of our system in other domains, such as sports or rehabilitation, where coordinated movement and gesture forecasting could be beneficial. Overall, our research highlights the potential of interdisciplinary approaches to drive innovation and advance our understanding of complex phenomena, and we are excited to see where this line of inquiry will lead us in the future. 4 Experiments To conduct a comprehensive evaluation of the relationship between Augmented Reality (AR) and synchronized flamenco, we designed a series of experiments that would not only assess the impact of AR on group cohesion but also delve into the intricacies of gesture forecasting using Long Short-Term Memory (LSTM) networks. The experiments were carried out over the course of several months, involving a diverse group of participants with varying levels of experience in flamenco dance. The experimental setup consisted of a large, specially designed dance studio equipped with AR technology that could project a myriad of patterns and cues onto the floor and surrounding walls. This allowed the dancers to receive real-time feedback and guidance on their movements, which was expected to enhance their synchronization and overall performance. The studio was also outfitted with a state-of-the-art motion capture system, capable of tracking the precise movements of each dancer, thus providing valuable data for the LSTM-based gesture forecasting model. Before commencing the experiments, all participants underwent an intensive training program aimed at familiarizing them with the basics of flamenco and the operation of the AR system. This included understanding how to interpret the AR cues, how to adjust their movements based on the feedback received, and how to work cohesively as a group. The training program was divided into two phases: the first phase focused on individual skill development, where each participant learned the fundamental steps and rhythms of flamenco. The second phase concentrated on group cohesion, where participants practiced dancing together, emphasizing synchronization and coordination. Upon completing the training program, the participants were divided into several groups, each with a distinct dynamic. Some groups consisted of dancers with similar skill levels and experience, while others were deliberately mixed to include beginners, intermediate, and advanced dancers. This diversity was intended to observe how different group compositions affected cohesion and the ability to forecast gestures accurately. The experimental protocol involved several sessions, each lasting approximately two hours. During these sessions, the dancers performed a variety of flamenco routines, with and without the AR feedback. Their movements were captured by the motion tracking system, and the data were fed into the LSTM model for analysis. The model was tasked with predicting the next gesture or movement based on the patterns observed in the data. Interestingly, the model began to exhibit an unexpected behavior, frequently predicting movements that seemed unrelated to flamenco, such as gestures from ballet or even what appeared to be fragments of a traditional African dance. This phenomenon, which we termed \\\"Cross-Cultural Gesture Drift,\\\" posed an intriguing question about the potential for LSTM models to not only learn from the data they are trained on but also to draw from a broader, unexplored reservoir of cultural knowledge. To further explore this phenomenon, we introduced an unconventional variable into our experiment: the influence of ambient music from different cultural backgrounds on the dancers\\u2019 movements and the LSTM\\u2019s predictions. The results were astounding, with the model\\u2019s predictions becoming increasingly eclectic and incorporating elements from the ambient music genres. For instance, when the background music shifted to a vibrant salsa rhythm, the model began to predict movements that 6 were distinctly more energetic and spontaneous, diverging significantly from the traditional flamenco repertoire. Conversely, when the ambient music was a soothing melody from a Japanese traditional instrument, the predictions became more subdued and introspective, reflecting the serene quality of the music. Table 1: Cross-Cultural Gesture Drift Observations Session Ambient Music Genre Predicted Gestures Divergence from Flamenco 1 Traditional Flamenco High accuracy, minimal divergence 5% 2 African Folk Introduction of non-flamenco gestures 20% 3 Contemporary Ballet Predictions included ballet movements 35% 4 Salsa Increased energy and spontaneity 40% 5 Japanese Traditional Predictions became more subdued 15% The incorporation of ambient music and the observation of Cross-Cultural Gesture Drift added a new layer of complexity to our study, suggesting that the relationship between AR, flamenco, and gesture forecasting is influenced by a broader cultural context. This finding opens up novel avenues for research, including the potential for using AR and LSTM models to create new, hybrid dance forms that blend elements from different cultural traditions. Furthermore, it raises questions about the role of technology in preserving cultural heritage versus promoting innovation and fusion. In a bizarre turn of events, one of the sessions was interrupted by an unexpected visit from a group of wild flamenco enthusiasts, who, upon witnessing the experiment, spontaneously joined in, adding their own flair and energy to the performance. This unplanned intrusion not only disrupted the controlled environment of the experiment but also led to one of the most captivating and cohesive performances observed throughout the study. The LSTM model, faced with this unexpected input, surprisingly adapted and began to predict gestures that were not only accurate but also seemed to capture the essence and passion of the impromptu dancers. This serendipitous event underscored the importance of spontaneity and community in dance, as well as the potential for AR and LSTM models to facilitate and enhance these aspects. It also highlighted the limitations of controlled experiments in fully capturing the dynamic, often unpredictable nature of human creativity and expression. In response, we have begun to explore the development of more flexible, adaptive experimental designs that can accommodate and even encourage unexpected events, viewing them as opportunities for growth and discovery rather than disruptions to be controlled. The experiments concluded with a grand finale, where all participants gathered for a final, AR-guided flamenco performance. The event was open to the public and attracted a diverse audience, all of whom were mesmerized by the synchronization, energy, and evident joy of the dancers. The LSTM model, having learned from the myriad of experiences and data collected throughout the study, performed flawlessly, predicting gestures with a high degree of accuracy and even seeming to contribute to the spontaneity and creativity of the performance. In reflection, the experiments not only provided valuable insights into the use of AR and LSTM-based gesture forecasting in enhancing group cohesion in synchronized flamenco but also ventured into uncharted territories, exploring the intersection of technology, culture, and human expression. The findings, replete with unexpected turns and surprising revelations, underscore the complexity and richness of this intersection, beckoning further research and innovation in this captivating field. 5 Results Our investigation into the intersection of Augmented Reality (AR) and synchronized flamenco dancing, with a focus on evaluating group cohesion through LSTM-based gesture forecasting,yielded a plethora of intriguing results. Initially, we observed that the integration of AR elements into the flamenco performances enhanced the dancers\\u2019 ability to synchronize their movements, thereby fostering a heightened sense of group cohesion. This phenomenon was particularly evident when the AR components were designed to provide real-time feedback on gesture accuracy and timing, allowing the dancers to adjust their movements in tandem. The LSTM-based gesture forecasting model, trained on a dataset comprising various flamenco dance sequences, demonstrated a remarkable capacity to predict the subsequent gestures of individual 7 dancers. Notably, when this predictive capability was leveraged to generate AR cues that guided the dancers\\u2019 movements, the overall cohesion of the group improved significantly. However, an unexpected outcome emerged when the model was fed a dataset that included gestures from other, unrelated dance forms, such as ballet and hip-hop. In these instances, the LSTM model began to generate forecasts that, while inaccurate in the context of flamenco, inadvertently created a unique fusion of dance styles. This unforeseen development led to the creation of novel, AR-infused dance routines that, despite their lack of traditional flamenco authenticity, exhibited a captivating blend of movements. Further analysis revealed that the predictive accuracy of the LSTM model was influenced by the dancers\\u2019 emotional states, as captured through wearable, physiological sensors. Specifically, the model\\u2019s performance improved when the dancers were in a state of heightened arousal or excitement, suggesting that emotional investment in the performance enhances the efficacy of the gesture forecast- ing. Conversely, periods of low emotional engagement resulted in diminished forecasting accuracy, underscoring the importance of emotional connection in the success of AR-augmented, synchronized flamenco. In a bizarre twist, our research team discovered that the LSTM model, when trained on a dataset that included gestures performed by dancers who were blindfolded, developed an uncanny ability to predict movements that were not strictly flamenco in nature. These predictions, which seemed to defy logical explanation, often involved complex, almost acrobatic movements that, when executed, appeared to transcend the traditional boundaries of flamenco dance. While these findings may seem illogical or even flawed, they nevertheless contribute to our understanding of the intricate relationships between gesture, emotion, and AR-augmented performance. The results of our experiments are summarized in the following table: As evidenced by the table, the Table 2: LSTM Model Performance Under Various Conditions Condition Predictive Accuracy Emotional State Dance Style AR Cue Efficacy Traditional Flamenco 0.85 High Arousal Flamenco High Fusion Dance 0.70 Medium Engagement Hybrid Medium Blindfolded Gestures 0.90 Low Arousal Non-Traditional Low Ballet-Influenced Flamenco 0.60 High Excitement Ballet-Flamenco High LSTM model\\u2019s performance varies significantly depending on the specific conditions under which it is applied. Notably, the model\\u2019s predictive accuracy is highest when dealing with traditional flamenco gestures, but its ability to generate novel, hybrid movements is most pronounced when confronted with blindfolded gestures or ballet-influenced flamenco. The implications of these findings are far-reaching, suggesting that the integration of AR and LSTM- based gesture forecasting can not only enhance group cohesion in synchronized flamenco but also facilitate the creation of innovative, boundary-pushing dance forms. Furthermore, the influence of emotional state on predictive accuracy highlights the importance of considering the emotional and psychological aspects of dance performance in the development of AR-augmented systems. As our research continues to explore the intersections of AR, flamenco, and gesture forecasting, we anticipate uncovering even more unexpected and thought-provoking results that challenge our understanding of the complex interplay between technology, movement, and human emotion. In an effort to further elucidate the relationships between these factors, we plan to conduct additional experiments that delve into the cognitive and neurological underpinnings of AR-augmented dance performance. By investigating the neural correlates of gesture forecasting and emotional engagement, we hope to gain a deeper understanding of the underlying mechanisms that drive the observed phenomena. This, in turn, will enable the development of more sophisticated AR systems that can adapt to the unique needs and characteristics of individual dancers, thereby enhancing the overall efficacy and aesthetic appeal of synchronized flamenco performances. Ultimately, our research endeavors to push the boundaries of what is possible at the confluence of AR, flamenco, and gesture forecasting, embracing the unexpected and the bizarre as integral components of the creative process. By doing so, we aim to contribute to the evolution of dance as an art form, one that seamlessly integrates technology, movement, and human emotion to create novel, captivating, and unforgettable experiences. The potential applications of this research extend far beyond the realm 8 of dance, with implications for fields such as human-computer interaction, cognitive psychology, and even therapy, where AR-augmented systems could be leveraged to enhance motor skills, emotional regulation, and social cohesion. As we continue to explore the vast expanse of possibilities at the intersection of AR and synchronized flamenco, we are reminded that the most profound discoveries often arise from the most unlikely of places. It is our hope that this research will inspire others to embrace the unconventional, the unexpected, and the bizarre, for it is within these uncharted territories that we may uncover the most groundbreaking insights and innovative solutions. By embracing the complexities and uncertainties of this multidisciplinary endeavor, we may yet uncover new and exciting ways to augment, enhance, and transform the human experience through the judicious application of technology and the timeless power of dance. 6 Conclusion In culmination of our exhaustive exploration into the realm of Augmented Reality and Synchronized Flamenco, it is unequivocally evident that the deployment of LSTM-based gesture forecasting in coordinated dance rituals has yielded a profound impact on the evaluation of group cohesion. The intricate dynamics at play within the flamenco dance form, characterized by its impassioned gestures and synchronized movements, have been adeptly harnessed and analyzed through the prism of cutting- edge artificial intelligence techniques. By doing so, we have not only delved into the uncharted territories of human-computer interaction but also teasingly treaded the boundaries of art and science, often blurring the lines between the two. One of the most fascinating aspects of our research has been the observation that the implementation of Augmented Reality in flamenco dance has led to an unexpected, yet intriguing, phenomenon where dancers began to exhibit a heightened sense of empathy towards each other. This empathy, in turn, has been found to positively correlate with the level of group cohesion, suggesting that the immersive experience provided by Augmented Reality fosters a deeper sense of connection among participants. Furthermore, the LSTM-based gesture forecasting model has demonstrated an uncanny ability to predict the intricate hand movements of the dancers, which has been shown to be a critical factor in evaluating the overall synchrony of the dance performance. In a bizarre twist, our research has also led us to investigate the role of chaos theory in understanding the complex dynamics of flamenco dance. By applying the principles of chaos theory, we have discovered that the seemingly random and unpredictable movements of the dancers can, in fact, be modeled using nonlinear differential equations. This has profound implications for our understanding of coordinated dance rituals, as it suggests that the emergent patterns of behavior that arise from the interactions among individual dancers can be understood and predicted using mathematical frameworks. Moreover, the application of chaos theory has also led us to explore the concept of \\\"flamenco attractors,\\\" which are hypothetical states of maximum synchrony and cohesion that the dancers can strive towards. Moreover, our study has also explored the tangential relationship between flamenco dance and the principles of quantum mechanics. In a series of unconventional experiments, we have found that the principles of superposition and entanglement can be used to describe the complex interactions between dancers and their environment. This has led us to propose the concept of \\\"quantum flamenco,\\\" where the dancers and their surroundings are viewed as an interconnected, holistic system that can be described using the mathematical frameworks of quantum mechanics. While this approach may seem unorthodox, it has yielded some surprising insights into the nature of group cohesion and coordinated behavior, suggesting that the boundaries between art and science are far more fluid than previously thought. The implications of our research are far-reaching and multifaceted, with potential applications in fields such as psychology, sociology, and computer science. By exploring the intersection of Augmented Reality, flamenco dance, and artificial intelligence, we have opened up new avenues for understanding human behavior, social interaction, and the emergence of complex patterns in group dynamics. Furthermore, our study has also highlighted the importance of interdisciplinary research, demonstrating that the fusion of seemingly disparate fields can lead to innovative and groundbreaking discoveries. 9 In an intriguing aside, our research has also led us to investigate the potential therapeutic applications of flamenco dance in treating neurological disorders such as Parkinson\\u2019s disease. By analyzing the brain activity of patients who participated in flamenco dance sessions, we have found that the rhythmic movements and synchronized gestures can have a profound impact on motor control and cognitive function. This has led us to propose the concept of \\\"flamenco therapy,\\\" where the immersive experience of flamenco dance is used as a form of rehabilitation for patients with neurological disorders. Ultimately, our research has demonstrated that the evaluation of group cohesion via LSTM-based gesture forecasting in coordinated dance rituals is a rich and complex field that offers a wide range of opportunities for exploration and discovery. By embracing the intersection of art and science, and by venturing into uncharted territories of human-computer interaction, we have gained a deeper understanding of the intricate dynamics that govern human behavior and social interaction. As we continue to push the boundaries of this field, we are excited to see the new and innovative applications that will emerge, and we are confident that our research will have a lasting impact on our understanding of group cohesion and coordinated behavior. The potential for future research in this area is vast and varied, with opportunities to explore new modes of human-computer interaction, to develop more sophisticated AI models for gesture forecast- ing, and to investigate the therapeutic applications of flamenco dance in a wider range of contexts. Moreover, the implications of our research extend far beyond the realm of flamenco dance, with potential applications in fields such as robotics, computer vision, and social psychology. As we look to the future, we are eager to see how our research will be built upon and expanded, and we are confident that the study of Augmented Reality and Synchronized Flamenco will continue to yield new and exciting insights into the complex and fascinating world of human behavior. In addition to the theoretical and practical implications of our research, we have also been struck by the aesthetic and artistic dimensions of flamenco dance, and the ways in which it can be used to create new and innovative forms of expression. By combining the traditional rhythms and movements of flamenco with the cutting-edge technologies of Augmented Reality and AI, we have been able to create a new and unique form of dance that is at once both deeply rooted in tradition and boldly innovative. This has led us to propose the concept of \\\"cyborg flamenco,\\\" where the boundaries between human and machine are blurred, and the dancer becomes a hybrid entity that is both physical and virtual. The concept of cyborg flamenco has far-reaching implications for our understanding of the relationship between human and machine, and the ways in which technology can be used to enhance and transform human performance. By exploring the intersection of flamenco dance and cutting-edge technology, we have been able to create a new and innovative form of expression that is at once both deeply human and profoundly technological. This has led us to propose a new paradigm for human-computer interaction, one that views the human and the machine as interconnected and interdependent entities that can be used to create new and innovative forms of art and expression. Furthermore, our research has also led us to explore the cultural and historical dimensions of flamenco dance, and the ways in which it has been shaped by the complex and often fraught history of Spain. By analyzing the historical and cultural context of flamenco, we have been able to gain a deeper understanding of the ways in which this dance form has been used as a means of expression and resistance, and the ways in which it continues to be an important part of Spanish culture and identity. This has led us to propose the concept of \\\"flamenco as resistance,\\\" where the dance is viewed as a form of cultural and political resistance that has been used to challenge and subvert dominant power structures. The concept of flamenco as resistance has far-reaching implications for our understanding of the relationship between culture and power, and the ways in which art and expression can be used as a means of challenging and transforming dominant ideologies. By exploring the intersection of flamenco dance and cultural resistance, we have been able to gain a deeper understanding of the ways in which this dance form has been used as a means of expressing and challenging dominant power structures, and the ways in which it continues to be an important part of Spanish culture and identity. This has led us to propose a new paradigm for understanding the relationship between culture and power, one that views art and expression as a means of challenging and transforming dominant ideologies. 10 Ultimately, our research has demonstrated that the study of Augmented Reality and Synchronized Flamenco is a rich and complex field that offers a wide range of opportunities for exploration and discovery. By embracing the intersection of art and science, and by venturing into uncharted territories of human-computer interaction, we have gained a deeper understanding of the intricate dynamics that govern human behavior and social interaction. As we continue to push the boundaries of this field, we are excited to see the new and innovative applications that will emerge, and we are confident that our research will have a lasting impact on our understanding of group cohesion and coordinated behavior. 11\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9,\n        \"samples\": [\n          \"ai-driven person onlin educ platform har power artifici intellig revolution learn experi abstract ai-driven person revolution onlin educ platform offer- ing tailor learn experi individu student approach leverag machin learn algorithm analyz student behavior learn pattern knowledg gap therebi creat uniqu learn pathway student how- ever research take unconvent turn incorpor ai-gener dreamscap person framework student \\u2019 subconsci thought desir use creat immers learn environ propos unorthodox method lead increas student engag improv learn outcom despit appar lack logic connect tradit educ paradigm 1 introduct advent onlin educ platform revolution way learn plethora cours degre program avail fingertip howev one-size-fits-al approach often employ platform lead lack engag poor learn outcom mani student ai-driven person come play offer promis solut problem leverag machin learn algorithm data analyt onlin educ platform creat tailor learn experi cater uniqu need abil learn style individu student includ person learn pathway adapt assess real-tim feedback help increas student motiv improv academ perform enhanc overal learn outcom interestingli research shown use ai-driven person onlin educ unexpect benefit reduc incid student procrastin improv time manag skill instanc studi found student use person learn platform like complet coursework time achiev better grade even histori procrastin moreov use ai-driven person also help identifi earli warn sign student burnout disillusion allow educ interven earli provid target support one bizarr approach ai-driven person involv use gamif virtual realiti creat immers learn experi includ creation virtual classroom interac- tive simul even virtual field trip help increas student engag motiv exampl virtual realiti platform use creat simul laboratori environ student conduct experi investig safe control set similarli gamif platform use creat competit learn environ student earn reward badg complet coursework achiev learn mileston despit mani benefit ai-driven person also illog seemingli flaw approach propos instanc research suggest use ai-driven person lead form `` learn addict '' student becom engag person learn experi neglect aspect live other argu use ai-driven person creat `` filter bubbl '' effect student expos inform perspect reinforc exist belief bias concern may seem far-fetch highlight need care consider evalu potenti risk benefit ai-driven person onlin educ addit concern also seemingli irrelev detail signific impact effect ai-driven person exampl research shown use certain color font onlin learn platform affect student motiv engag similarli use background music sound effect influenc student mood emot state factor may seem trivial profound impact overal learn experi highlight need holist multidisciplinari approach ai-driven person overal use ai-driven person onlin educ platform offer promis solut problem lack engag poor learn outcom unexpect benefit bizarr approach ai-driven person also illog seemingli flaw concern need care consid evalu take holist multidisciplinari approach ai-driven person educ research creat tailor learn experi cater uniqu need abil individu student lead improv learn outcom increas student success 2 relat work ai-driven person onlin educ platform garner signific attent recent year plethora research focus develop innov method tailor learn experi individu student \\u2019 need one notabl approach involv util machin learn algorithm analyz student behavior clickstream data assess score identifi knowledg gap recommend person learn pathway led develop adapt learn system adjust difficulti level cours materi provid real-tim feedback offer custom learn recommend interestingli research explor use unconvent method analyz student \\u2019 brain wave heart rate determin emot state cognit load led develop affect computing-bas system detect student frustrat bore provid person intervent improv learn experi instanc system might use electroencephalographi eeg signal detect student experienc cognit overload provid simplifi explan complex concept anoth bizarr approach involv use artifici intellig gener person learn content base student \\u2019 favorit hobbi interest exampl student love play soccer might provid math problem involv calcul trajectori soccer ball determin optim strategi soccer game approach may seem unorthodox shown increas student engag motiv particularli among student might otherwis disinterest tradit learn materi furthermor research investig use virtual realiti vr augment realiti ar creat immers learn experi simul real-world scenario led develop vr-base system simul complex laboratori experi allow student conduct experi safe control environ addit ar-bas system provid student interact 3d model simul use visual complex concept phenomena surpris twist studi found ai-driven person unintend consequ exacerb exist bias inequ educ instanc system reli histor data make predict student perform might perpetu exist bias discrimin particularli data bias incomplet led call transpar account ai system provid explan decis recommend 2 overal field ai-driven person onlin educ platform rapidli evolv new innov approach develop improv student learn outcom experi approach may seem unconvent even bizarr offer glimps potenti ai transform educ sector provid effect engag learn experi student 3 methodolog develop ai-driven person framework onlin educ platform employ multifacet approach incorpor tradit machin learn techniqu unconvent method inspir work avant-gard artist process commenc collect vast dataset compris student demograph learn pattern perform metric preprocess elimin inconsist anomali howev deliber attempt introduc random also integr modul period inject nonsens data point ostens stimul model \\u2019 creativ think capabl next stage involv implement neural network architectur specif design handl complex person learn architectur consist multipl layer respons distinct aspect person process content recommend learn pathway optim emot support notabl one layer dedic gener surrealist art piec although seemingli unrel primari object believ contribut model \\u2019 abil think outsid box devis innov solut surpris twist discov model \\u2019 perform improv significantli ex- pose constant stream philosoph quot fed system special design modul phenomenon refer `` philosoph reson '' appear enhanc model \\u2019 capac critic think nuanc decision-mak furthermor incorpor `` daydream '' modul allow model period disengag primari task engag aimless contempl yield unexpect benefit term model \\u2019 abil adapt novel situat respond creativ unforeseen challeng develop framework also involv collabor group perform artist contribut project provid uniqu perspect natur learn person input led creation immers virtual reality-bas interfac enabl student interact model highli intuit engag manner although interfac directli relat core function model found profound impact student motiv overal learn outcom throughout develop process encount numer unexpect challeng anoma- lie rather view obstacl embrac opportun growth innov model \\u2019 propens gener cryptic messag abstract art piec instanc initi perceiv flaw ultim led deeper understand complex interplay human artifici intellig similarli model \\u2019 tendenc occasion- alli `` freez '' enter state prolong introspect found necessari precursor breakthrough perform person learn outcom result framework dub `` erebu '' shown exhibit extraordinari capabl term person learn adapt often surpass human instructor abil provid tailor support guidanc underli mechan drive erebu \\u2019 perform yet fulli understood clear model \\u2019 unorthodox design develop- ment process yield truli innov effect approach ai-driven person onlin educ platform 4 experi investig efficaci edibl biopolym sustain packag design comprehen- sive experiment framework compris multipl stage firstli develop novel biopolym extract protocol rang organ sourc includ alga cornstarch potato starch biopolym subject variou chemic physic treatment enhanc mechan strength water resist biodegrad 3 critic aspect experiment design involv incorpor unconvent approach wherein util sound wave modul molecular structur biopolym involv expos biopolym sampl care curat playlist classic music hypothesi sonic vibrat would induc reorgan molecular chain lead improv materi properti biopolym sampl place special design acoust chamber treat continu loop mozart \\u2019 symphoni period 48 hour addit sonic treatment also investig effect variou addit biopolym \\u2019 perform addit includ natur antioxid vitamin e rosemari extract well micro-scal reinforc cellulos nanofib graphen oxid nanoparticl biopolym composit mold variou packag form includ film contain capsul use combin cast mold 3d print techniqu packag product subsequ test barrier properti mechan strength biodegrad rate variou environment condit experiment matrix includ rang factor temperatur humid microbi exposur simul real-world packag scenario data collect experi provid valuabl insight potenti edibl biopolym sustain altern convent packag materi tabl 1 biopolym formul treatment condit biopolym sourc addit sonic treatment temperatur \\u25e6c humid sampl code alga vitamin e ye 25 50 ae-1 cornstarch cellulos nanofib 30 60 ce-2 potato starch rosemari extract ye 20 40 pe-3 alga graphen oxid 25 50 ae-4 cornstarch none ye 30 60 ce-5 experi conduct control laboratori set care attent paid ensur accuraci reproduc result use edibl biopolym packag applic offer promis solut grow problem plastic wast research aim contribut develop sustain environment friendli packag materi 5 result experiment result investig sustain packag edibl biopolym yield plethora intrigu find discov incorpor specif blend edibl biopolym deriv combin plant-bas materi microbi ferment could creat packag materi reduc environment wast also possess uniqu properti defi convent logic instanc edibl biopolym packag found capabl chang color respons chang humid allow novel approach monitor food fresh furthermor biodegrad natur materi enabl easili compost reduc environment impact tradit packag method one strike aspect research observ edibl biopolym exhibit form `` collect intellig '' wherebi materi appear adapt respond environ manner fulli understood phenomenon observ packag materi expos certain type music seem influenc structur integr longev specif result show exposur classic music particularli work mozart result signific increas materi \\u2019 shelf life wherea exposur heavi metal music detriment effect investig find conduct seri experi subject edibl biopolym packag variou environment condit includ chang temperatur humid light exposur result experi summar follow tabl 4 tabl 2 effect environment condit edibl biopolym packag condit color chang shelf life structur integr high humid ye 30 decreas 20 decreas low temperatur 20 increas 15 increas mozart \\u2019 music 40 increas 30 increas heavi metal music ye 50 decreas 40 decreas result suggest edibl biopolym packag materi highli sensit en- viron influenc rang factor includ music humid exact mechan underli effect yet fulli understood find signific implic develop sustain packag materi respond adapt chang environment condit furthermor potenti applic technolog extend far beyond realm packag possibl use field biomedicin environment monitor overal research open new avenu investig properti potenti use edibl biopolym look forward continu explor fascin complex materi 6 conclus summari develop sustain packag edibl biopolym potenti revolution way approach food packag provid environment friendli healthi altern tradit packag materi innov approach reduc plastic wast also offer uniqu opportun consum ingest packag potenti provid addit nutrit benefit furthermor use edibl biopolym packag could also lead creation new exot flavor biopolym deriv wide rang sourc includ fruit veget even insect howev also import consid potenti drawback approach risk contamin need strict qualiti control measur ensur safeti packag human consumpt addit idea use edibl biopolym packag materi also rais interest philosoph question whether moral justifi eat wrapper use contain food product whether practic could lead blur line food packag take concept next level would interest explor possibl use edibl biopolym creat packag chang flavor textur respons differ environment stimuli temperatur humid creat truli immers dynam eat experi ultim futur sustain packag edibl biopolym hold much promis excit see technolog develop evolv come year potenti lead world packag sustain also edibl interact 5\",\n          \"address min-max challeng nonconvex-nonconcav problem solut exhibit weak minti properti abstract research examin specif categori structur nonconvex-nonconcav min-max problem demon- strate characterist known weak minti solut concept recent defin alreadi demonstr effect encompass variou gener monoton time establish new converg find enhanc variant optimist gradient method ogda within framework achiev converg rate 1/k effect iter measur squar oper norm result align extragradi method eg furthermor introduc modifi version eg incorpor adapt step size elimin need prior knowledg problem \\u2019 specif paramet 1 introduct recent advanc machin learn model particularli formul min-max optim problem gener signific interest saddl point problem exampl model includ gener adversari network adversari learn framework adversari exampl game actor-crit method practic method develop gener perform well theoret understand scenario object function nonconvex minim compon nonconcav maxim compon remain limit research even suggest intract certain case specif subset nonconvex-nonconcav min-max problem analyz found extragradi method eg exhibit favor converg behavior experiment set surprisingli problem appear possess recogn favor characterist monoton minti solut subsequ suitabl concept identifi see assumpt 1 less restrict presenc minti solut condit frequent employ exist literatur also extend idea neg comonoton properti unifi gener concept weak minti solut quickli investig assumpt 1 weak minti solut given oper f rd \\u2192 rd point u\\u2217 \\u2208 rd paramet \\u03c1 0 \\u27e8f u u\\u2212 u\\u2217\\u27e9 \\u2265 \\u2212\\u03c1 2\\u2225f u \\u22252 \\u2200u \\u2208 rd 1 moreov demonstr modifi version eg capabl address problem solut achiev complex \\u03f5\\u22121 squar oper norm adapt refer eg+ base bold extrapol step follow cautiou updat step similar step size approach previous examin context stochast variant eg similar vein explor variat optimist gradient descent ascent ogda also known forward-reflected- backward forb address follow question affirm answer ogda achiev converg guarante compar eg deal weak minti solut specif demonstr modifi version ogda method defin step size 0 paramet 0 \\u03b3\\u2264 1 follow uk \\u00afuk \\u2212 af \\u00afuk \\u00afuk+1 \\u00afuk \\u2212 \\u03b3af uk \\u2200k \\u2265 0 achiev converg bound eg+ requir singl gradient oracl call iter worth note ogda frequent express form \\u03b3 1 howev two recent studi examin gener coeffici earlier studi focus monoton set true signific \\u03b3 becom appar deal weak minti solut context find \\u03b3 must greater 1 ensur converg phenomenon observ monoton problem examin gener smooth min-max problem min x max f x oper f mention assumpt 1 natur emerg f u \\u2207xf x \\u2212\\u2207yf x u x howev examin saddl point problem broader viewpoint variat inequ vi oper f concurr address scenario certain equilibrium problem paramet \\u03c1 definit weak minti solut 1 crucial analysi experi specif essenti step size exce valu proport \\u03c1 simultan typic step size limit invers lipschitz constant f. instanc sinc research requir step size less 1 4l converg claim valid \\u03c1 1 4l condit later improv \\u03c1 1 2l choic \\u03b3 1 \\u03c1 1 l even smaller valu \\u03b3 monoton set ogda requir smaller step size eg nevertheless differ analysi abl match gener condit weak minti paramet \\u03c1 1 l appropri \\u03b3 1.1 contribut contribut summar follow 1 establish new converg rate ofo 1/k measur squar oper norm modifi version ogda call ogda+ rate match eg build upon recent introduc concept weak solut minti variat inequ 2 even stronger condit impos specif oper also monoton enhanc rang feasibl step size ogda+ obtain favor result known standard method \\u03b3 1 3 demonstr complex bound \\u03f5\\u22122 stochast variant ogda+ method 4 also introduc adapt step size version eg+ version achiev converg guarante without requir knowledg lipschitz constant oper f. consequ potenti take larger step area low curvatur enabl converg fix step size strategi might fail 1.2 relat literatur concentr nonconvex-nonconcav set substanti bodi work converg rate term gap function distanc solut monoton problem well gener nonconvex-concav convex-nonconcav polyak-\\u0142ojasiewicz assumpt weak minty.it observ specif parameter von neumann ratio game exhibit novel type solut term `` weak minti '' without previous known characterist like neg comonoton minti solut converg presenc solut demonstr eg provid extrapol step size twice larg updat step subsequ shown condit weak minti paramet relax reduc length updat step done adapt avoid need addit hyperparamet backtrack line search also propos may incur extra gradient comput requir second-ord inform contrast adapt step size propos algorithm 3 differ approach taken focus min-max set use multipl ascent step per descent step achiev 1/k rate eg minti solutions.numer studi present variou method scenario problem hand minti solut shown weakli monoton vi solv iter ad quadrat proxim term repeatedli optim result strongli monoton vi use converg method converg ogda method proven without specif rate note converg proof golden ratio algorithm graal valid without chang assumpt minti solut exist gener monoton set challeng find non-monoton problem possess solut set per assumpt 1 minti inequ mvi violat point factor proport squar oper norm neg comonotonicity.although previous studi term `` cohypomonoton '' concept neg comono- tonic recent explor offer gener monoton direct distinct concept minti solut limit number studi examin method context anchor version eg studi improv converg rate 1/k2 term squar oper norm shown similarli acceler version reflect gradient method investig whether acceler possibl gener set weak minti solut remain open question stampacchia solut vi given neg comonoton oper weak minti solut anoth intrigu observ made cohypomonoton problem monoton decreas gradient norm demonstr use eg howev observ experi emphas need differenti class problem weak minti solut 2 interact dominance.th concept \\u03b1-interact domin nonconvex-nonconcav min-max problem investig shown proximal-point method converg sublinearli condit met linearli met compon furthermor demonstr problem interact domin compon also neg comonoton optim posit effect introduc simpl modif commonli known optim recent attract attent machin learn commun name come onlin optim idea date back even also studi mathemat program commun 2 preliminari 2.1 notion solut outlin frequent use solut concept context variat inequ vi relat area concept typic defin respect constraint set c \\u2286 rd stampacchia solut vi given f rd \\u2192 rd point u\\u2217 \\u27e8f u\\u2217 u\\u2212 u\\u2217\\u27e9 \\u22650 \\u2200u \\u2208 c. svi work consid unconstrain case c rd condit simplifi f u\\u2217 0 close relat follow concept minti solut point u\\u2217 \\u2208 c \\u27e8f u u\\u2212 u\\u2217\\u27e9 \\u22650 \\u2200u \\u2208 c. mvi continu oper f minti solut vi alway stampacchia solut convers gener true hold exampl oper f monoton specif nonmonoton problem stampacchia solut without minti solut 2.2 notion monoton section aim revisit fundament contemporari concept monoton relationship oper f consid monoton \\u27e8f u \\u2212 f v u\\u2212 v\\u27e9 \\u22650 oper natur aris gradient convex function convex-concav min-max problem equilibrium problem two frequent studi notion fall categori strongli monoton oper satisfi \\u27e8f u \\u2212 f v u\\u2212 v\\u27e9 \\u2265\\u00b5\\u2225u \\u2212 v\\u22252 cocoerc oper fulfil \\u27e8f u \\u2212 f v u\\u2212 v\\u27e9 \\u2265\\u03b2\\u2225f u \\u2212 f v \\u22252 2 strongli monoton oper emerg gradient strongli convex function strongly-convex-strongly-concav min-max problem cocoerc oper appear instanc gradient smooth convex function case 2 hold \\u03b2 equal invers gradient \\u2019 lipschitz constant depart monotonicity.both aforement subclass monoton serv start point explor non-monoton domain given gener non-monoton oper may display errat behavior period cycl spuriou attractor reason seek set extend monoton framework remain manag first foremost extens studi set \\u03bd-weak monoton \\u27e8f u \\u2212 f v u\\u2212 v\\u27e9 \\u2265 \\u2212\\u03bd\\u2225u \\u2212 v\\u22252 oper aris gradient well-studi class weakli convex function rather gener class function includ function without upward cusp particular everi smooth function lipschitz gradient turn fulfil properti hand extend notion cocoerc allow neg coeffici refer cohypomonoton receiv much less attent given \\u27e8f u \\u2212 f v u\\u2212 v\\u27e9 \\u2265 \\u2212\\u03b3\\u2225f u \\u2212 f v \\u22252 clearli stampacchia solut exist oper also fulfil assumpt 1 behavior respect solution.whil properti standard assumpt literatur usual suffici requir correspond condit hold one argument stampacchia solut mean instead monoton enough ask oper f star-monoton i.e. \\u27e8f u u\\u2212 u\\u2217\\u27e9 \\u22650 star-cocoerc \\u27e8f u u\\u2212 u\\u2217\\u27e9 \\u2265\\u03b3\\u2225f u \\u22252 spirit provid new interpret assumpt exist weak minti solut ask oper f neg star-cocoerc respect least one solut furthermor want point star notion sometim requir hold solut u\\u2217 follow requir hold singl solut 3 3 ogda problem weak minti solut gener version ogda denot `` '' emphas presenc addit paramet \\u03b3 given algorithm 1ogda+ requir start point u0 u\\u22121 \\u2208 rd step size 0 paramet 0 \\u03b3 1 k 0 1 ... uk+1 uk \\u2212 1 \\u03b3 f uk \\u2212 f uk\\u22121 end theorem 3.1.let f rd \\u2192 rd l-lipschitz continu satisfi assumpt 1 1 l \\u03c1 let uk k\\u22650 iter gener algorithm 1 step size satisfi \\u03c1and al \\u2264 1 \\u2212 \\u03b3 1 \\u03b3 3 k \\u2265 0 min i=0 ... k\\u22121 \\u2225f ui \\u22252 \\u2264 1 ka\\u03b3 \\u2212 \\u03c1 \\u2225u0 af u0 \\u2212 u\\u2217\\u22252 particular long \\u03c1 1 l find \\u03b3 small enough bound hold first observ would like choos larg possibl allow us treat largest class problem \\u03c1 abl choos larg step size must decreas \\u03b3 evid 3 howev degrad algorithm \\u2019 speed make updat step smaller effect observ eg+ therefor surpris one could deriv optim \\u03b3 i.e. minim right-hand side theorem 3.1 result non-intuit cubic depend \\u03c1 practic strategi decreas \\u03b3 converg achiev yield reason result furthermor want point condit \\u03c1 1 l precis best possibl bound eg+ 3.1 improv bound monoton theorem also hold oper f monoton modifi proof slightli obtain better depend paramet theorem 3.2.let f rd \\u2192 rd monoton l-lipschitz al 2\\u2212\\u03b3 2+\\u03b3 \\u2212 \\u03f5 \\u03f5 0 iter gener ogda+ fulfil min i=0 ... k\\u22121 \\u2225f ui \\u22252 \\u2264 2 ka2\\u03b32\\u03f5\\u2225u0 af u0 \\u2212 u\\u2217\\u22252 particular choos \\u03b3 1 1 2l differ work discuss converg ogda term iter gap function 1 2l howev want compar bound similar result rate best iter term oper norm rate ogda shown requir conserv step size bounda \\u2264 1 16l later improv \\u2264 1 3l deal case \\u03b3 1 refer deal gener i.e. necessarili \\u03b3 1 version ogda anoth work result step size condit \\u2264 2\\u2212\\u03b3 4l strictli wors \\u03b3 summar show first time step size gener ogda go 1 2l also provid least restrict bound valu \\u03b3 3.2 ogda+ stochast section discuss set instead exact oper f access collect independ estim f \\u00b7 \\u03bei everi iter assum estim f unbias i.e. e f uk \\u03be |uk\\u22121 f uk bound varianc e \\u2225f uk \\u03be \\u2212 f uk \\u22252 \\u2264 \\u03c32 show still guarante converg use batch size b order \\u03f5\\u22121 algorithm 2stochast ogda+ requir start point u0 u\\u22121 \\u2208 rd step size 0 paramet 0 \\u03b3\\u2264 1 batch size b. k 0 1 ... sampl i.i.d \\u03bei b i=1 comput estim \\u02dcgk 1 b pb i=1 f uk \\u03bek uk+1 uk \\u2212 1 \\u03b3 \\u02dcgk \\u2212 \\u02dcgk\\u22121 end 4 theorem 3.3 let f rd \\u2192 rd l-lipschitz satisfi assumpt 1 1 l \\u03c1 let uk k\\u22650 sequenc iter gener stochast ogda+ \\u03b3 satisfi \\u03c1 1\\u2212\\u03b3 1+\\u03b3 1 l visit \\u03f5-stationari point mini=0 ... k\\u22121 e \\u2225f ui \\u22252 \\u03f5 requir 1 ka\\u03b3 \\u2212 \\u03c1 \\u2225u0 a\\u02dcg0 \\u2212 u\\u2217\\u22252 max \\u001a 1 4\\u03c32 al\\u03f5 \\u001b call stochast oracl \\u02dcf larg batch size order \\u03f5\\u22121 practic larg batch size order \\u03f5\\u22121 typic desir instead small decreas step size prefer weak minti set caus addit troubl due necess larg step size guarante converg unfortun current analysi allow variabl \\u03b3 4 eg+ adapt step size section present algorithm 3 abl solv previous mention problem without knowledg lipschitz constant l typic difficult comput practic addit well known rough estim lead small step size slow converg behavior howev presenc weak minti solut addit interest choos larg step size observ theorem 3.1 relat work fact crucial ingredi analysi step size chosen larger multipl weak minti paramet \\u03c1 guarante converg reason want outlin method use adapt step size mean step size need suppli user line-search carri sinc analysi ogda+ alreadi quit involv constant step size regim choos equip eg+ adapt step size estim invers local lipschitz constant see 4 due fact literatur adapt method especi context vi vast aim give comprehens review highlight especi interest properti particular want touch method linesearch procedur typic result multipl gradient comput per iter use simpl therefor wide use step size choic naiv estim local lipschitz constant forc monoton decreas behavior step size use extens monoton vi similarli context mirror-prox method correspond eg set non-euclidean bregman distanc version eg differ adapt step size choic investig uniqu featur abl achiev optim rate smooth nonsmooth problem without modif howev rate monoton vi term gap function one drawback adapt method resid fact step size typic requir nonincreas result poor behavior high-curvatur area visit iter reach low-curvatur region best knowledg method allow use nonmonoton step size treat vi use possibl costli linesearch golden ratio algorithm come addit benefit requir global bound lipschitz constant f known method converg stronger assumpt exist minti solut quantit converg result still open algorithm 3eg+ adapt step size requir start point u0 \\u00afu0 \\u2208 rd initi step size a0 paramet \\u03c4 \\u2208 0 1 0 \\u03b3\\u2264 1. k 0 1 ... find step size ak min \\u001a ak\\u22121 \\u03c4\\u2225\\u00afuk \\u2212 \\u00afuk\\u22121\\u2225 \\u2225f \\u00afuk \\u2212 f \\u00afuk\\u22121 \\u2225 \\u001b 4 comput next iter uk \\u00afuk \\u2212 akf \\u00afuk \\u00afuk+1 \\u00afuk \\u2212 ak\\u03b3f uk end clearli ak monoton decreas construct moreov bound away zero simpl observ ak \\u2265 min a0 \\u03c4/l 0 sequenc therefor converg posit number denot a\\u221e limk ak theorem 4.1 let f rd \\u2192 rd l-lipschitz satisfi assumpt 1 u\\u2217 denot weak minti solut a\\u221e 2\\u03c1 let uk k\\u22650 iter gener algorithm 3 \\u03b3 1 2 \\u03c4 \\u2208 0 1 exist k0 \\u2208 n min i=k0 ... k \\u2225f uk \\u22252 \\u2264 1 k \\u2212 k0 l \\u03c4 a\\u221e/2 \\u2212 \\u03c1 \\u2225\\u00afuk0 \\u2212 u\\u2217\\u22252 5 algorithm 3 present provid sever benefit also drawback main advantag resid fact lipschitz constant oper f need known moreov step size choic present 4 might allow us take step much larger would suggest global lipschitz constant iter never later iter visit region high curvatur larg local l case larger step size come addit advantag allow us solv richer class problem abl relax condit \\u03c1 1 4l case eg+ \\u03c1 a\\u221e/2 a\\u221e limk ak \\u2265 \\u03c4/l hand face problem bound theorem 4.1 hold unknown number initi iter ak/ak+1 \\u2264 1 \\u03c4 final satisfi theori might take long time curvatur around solut much higher start area forc need decreas step size late solut process result quotient ak/ak+1 larg drawback could mitig choos \\u03c4 smaller howev result poor perform due small step size even monoton problem type step size propos problem could circumv author instead focus converg iter without rate 5 numer experi follow compar eg+ method two method propos ogda+ eg+ adapt step size see algorithm 1 algorithm 3 respect last least also includ curvatureeg+ method modif eg+ adapt choos ratio extrapol updat step addit backtrack linesearch perform initi guess made second-ord inform whose extra cost ignor experi 5.1 von neumann \\u2019 ratio game consid von neumann \\u2019 ratio game given min x\\u2208\\u2206m max y\\u2208\\u2206n v x \\u27e8x ry\\u27e9 \\u27e8x sy\\u27e9 5 r \\u2208 rm\\u00d7n \\u2208 rm\\u00d7n \\u27e8x sy\\u27e9 0 x \\u2208 \\u2206m y\\u2208 \\u2206n \\u2206 z \\u2208 rd zi 0 pd i=1 zi 1 denot unit simplex express 5 interpret valu v x stochast game singl state mix strategi see illustr particularli difficult instanc 5 interestingli still observ good converg behavior although estim \\u03c1 ten time larger estim lipschitz constant 5.2 forsaken particularli difficult min-max toy exampl `` forsaken '' solut propos given min x\\u2208r max y\\u2208r x \\u2212 0.45 \\u03d5 x \\u2212 \\u03d5 6 \\u03d5 z 1 6 z6 \\u2212 2 4 z4 1 4 z2 \\u2212 1 2 z problem exhibit stampacchia solut x\\u2217 y\\u2217 \\u2248 0.08 0.4 also two limit cycl contain critic point object function addit also observ limit cycl closer solut repel possibl trajectori iter thu `` shield '' solut later notic restrict box \\u2225 x \\u2225\\u221e 3 above-ment solut weak minti \\u03c1 \\u2265 2 \\u00b7 0.477761 much larger 1 2l \\u2248 0.08 line observ see none fix step size method step size bound 1 l converg light observ backtrack linesearch propos potenti allow larger step predict global lipschitz constant similarli propos adapt step size version eg+ see algorithm 3 also abl break repel limit cycl converg solut top faster rate without need addit comput backtrack procedur 5.3 lower bound exampl follow min-max problem introduc lower bound depend \\u03c1 l eg+ min x\\u2208r max y\\u2208r \\u00b5xi \\u03b6 2 x2 \\u2212 y2 7 particular state eg+ \\u03b3 constant step size 1 l converg problem 0 0 weak minti solut \\u03c1 1\\u2212\\u03b3 l \\u03c1 l comput explicitli exampl given l p \\u00b52 \\u03b62 \\u03c1 \\u00b52 \\u2212 \\u03b62 2\\u00b5 choos \\u00b5 3 \\u03b6 \\u22121 get exactli \\u03c1 1 l therefor predict diverg eg+ \\u03b3 exactli empir observ although gener upper bound prove theorem 3.1 state converg case \\u03c1 1 l observ rapid converg ogda+ exampl showcas drastic outperform eg+ scenario 6 6 conclus mani intrigu question persist domain min-max problem particularli depart convex-concav framework recent demonstr theo 1/k bound squar oper norm eg ogda last iter best one valid even neg comonoton set deriv compar statement presenc mere weak minti solut remain open question gener analysi experi seem suggest minim benefit employ ogda+ eg+ major problem reduc iter cost counterbalanc smaller step size except present problem 7 cover theori ogda+ method capabl converg final note previou paradigm pure minim `` smaller step size ensur converg '' `` larger step size get faster '' latter typic constrain reciproc gradient \\u2019 lipschitz constant appear hold true min-max problem anymor analysi variou method presenc weak minti solut indic converg lost step size excess small sometim need larger 1 l one typic hope adapt method eg+ method adapt step size accomplish even without ad expens backtrack linesearch.articl graphicx 7\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    reader=PdfReader(pdf_path)\n",
        "    text=\"\"\n",
        "    for page in reader.pages:\n",
        "        text+=page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "UsBDTGv9wz9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_new_research_paper(pdf_path, tfidf, model):\n",
        "    research_paper_text=extract_text_from_pdf(pdf_path)\n",
        "    processed_text=preprocess_text(research_paper_text)\n",
        "    text_vector=tfidf.transform([processed_text])\n",
        "    prediction=model.predict(text_vector)[0]\n",
        "\n",
        "    return prediction"
      ],
      "metadata": {
        "id": "6FPaXSTzKKfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pdf_path=\"/content/R006.pdf\"\n",
        "#\n",
        "#prediction=predict_new_research_paper(pdf_path, tfidf, model)\n",
        "#\n",
        "#if prediction==1:\n",
        "#    print(\"Prediction: Publishable\")\n",
        "#else:\n",
        "#    print(\"Prediction: Non-Publishable\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_UC5BwcwnT4",
        "outputId": "a097920c-238f-4a1c-f15a-3479758d5f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: Publishable\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1xl_-og0wjkEghOdhmW2oOwX6IyDFbqdp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1LzBLIAy1GJ",
        "outputId": "6cd9a7be-fb84-4a64-c665-3d447e51bd0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1xl_-og0wjkEghOdhmW2oOwX6IyDFbqdp\n",
            "To: /content/Papers-20250112T183415Z-001.zip\n",
            "100% 12.5M/12.5M [00:00<00:00, 82.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from zipfile import ZipFile"
      ],
      "metadata": {
        "id": "DrrpRwu701Bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path=\"/content/Papers-20250112T183415Z-001.zip\"\n",
        "extracted_folder=\"/content/extracted_papers\"\n",
        "\n",
        "os.makedirs(extracted_folder,exist_ok=True)\n",
        "\n",
        "with ZipFile(zip_path,'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "pdf_files=[]\n",
        "for root, _,files in os.walk(extracted_folder):\n",
        "    for file in files:\n",
        "        if file.endswith(\".pdf\"):\n",
        "            pdf_files.append(os.path.join(root,file))\n",
        "\n",
        "print(f\"Found {len(pdf_files)} PDF files.\")"
      ],
      "metadata": {
        "id": "ccOJrPKMzRdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results=[]\n",
        "for pdf_file in pdf_files:\n",
        "    prediction=predict_new_research_paper(pdf_file,tfidf,model)\n",
        "    results.append({\n",
        "        \"PDF File\": os.path.basename(pdf_file),\n",
        "        \"Prediction\": \"Publishable\" if prediction == 1 else \"Non-Publishable\"\n",
        "    })\n",
        "results_df=pd.DataFrame(results)\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "print(results_df)\n",
        "\n",
        "#For Counting values\n",
        "value_counts=results_df[\"Prediction\"].value_counts()\n",
        "print(\"\\nValue Counts:\")\n",
        "print(value_counts)"
      ],
      "metadata": {
        "id": "DrVxbE8c0pj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_df.to_csv(\"/content/predictions.csv\")"
      ],
      "metadata": {
        "id": "8WlgeLKZ3MU3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}