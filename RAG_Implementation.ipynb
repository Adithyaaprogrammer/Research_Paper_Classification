{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Using cached numpy-2.2.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Using cached numpy-2.2.1-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.1 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (2.2.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Getting requirements to build wheel did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [15 lines of output]\n",
      "      The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "      rather than 'sklearn' for pip commands.\n",
      "      \n",
      "      Here is how to fix this error in the main use cases:\n",
      "      - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "      - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "        (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "      - if the 'sklearn' package is used by one of your dependencies,\n",
      "        it would be great if you take some time to track which package uses\n",
      "        'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "      - as a last resort, set the environment variable\n",
      "        SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "      \n",
      "      More information is available at\n",
      "      https://github.com/scikit-learn/sklearn-pypi-package\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "× Getting requirements to build wheel did not run successfully.\n",
      "│ exit code: 1\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pathway\n",
      "  Downloading pathway-0.post1-py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading pathway-0.post1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: pathway\n",
      "Successfully installed pathway-0.post1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/44.4 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 20.5/44.4 kB 217.9 kB/s eta 0:00:01\n",
      "     ----------------------------------- -- 41.0/44.4 kB 281.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 218.9 kB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from transformers) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from transformers) (24.2)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.24.0->transformers)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
      "   ---------------------------------------- 0.0/9.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/9.7 MB 991.0 kB/s eta 0:00:10\n",
      "    --------------------------------------- 0.2/9.7 MB 3.1 MB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.5/9.7 MB 4.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.6/9.7 MB 9.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 3.7/9.7 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 8.0/9.7 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.7 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.7/9.7 MB 29.5 MB/s eta 0:00:00\n",
      "Using cached huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Using cached PyYAML-6.0.2-cp311-cp311-win_amd64.whl (161 kB)\n",
      "Using cached regex-2024.11.6-cp311-cp311-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.12.14 charset-normalizer-3.4.1 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.1 idna-3.10 pyyaml-6.0.2 regex-2024.11.6 requests-2.32.3 safetensors-0.5.2 tokenizers-0.21.0 tqdm-4.67.1 transformers-4.48.0 urllib3-2.3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.5.1-cp311-cp311-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from torch) (2024.12.0)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl.metadata (4.1 kB)\n",
      "Using cached torch-2.5.1-cp311-cp311-win_amd64.whl (203.1 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached MarkupSafe-3.0.2-cp311-cp311-win_amd64.whl (15 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 jinja2-3.1.5 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install sklearn\n",
    "!pip install -U pathway\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 115 entries, 0 to 134\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  115 non-null    int64 \n",
      " 1   PDF File    115 non-null    object\n",
      " 2   Prediction  115 non-null    object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 3.6+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('predictions.csv')\n",
    "df = df[df['Prediction'] != 'Non-Publishable']\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from sentence-transformers) (4.48.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.15.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.8 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 537.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from sentence-transformers) (0.27.1)\n",
      "Collecting Pillow (from sentence-transformers)\n",
      "  Using cached pillow-11.1.0-cp311-cp311-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.2.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.2)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.12.14)\n",
      "Downloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "   ---------------------------------------- 0.0/268.8 kB ? eta -:--:--\n",
      "   --------- ------------------------------ 61.4/268.8 kB 3.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 143.4/268.8 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 204.8/268.8 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 268.8/268.8 kB 1.7 MB/s eta 0:00:00\n",
      "Using cached pillow-11.1.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/11.1 MB 8.9 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.5/11.1 MB 6.0 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.8/11.1 MB 7.3 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.9/11.1 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.0/11.1 MB 13.6 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.5/11.1 MB 16.7 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.6/11.1 MB 24.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 46.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 38.5 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.1-cp311-cp311-win_amd64.whl (43.9 MB)\n",
      "   ---------------------------------------- 0.0/43.9 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 5.5/43.9 MB 117.2 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 6.0/43.9 MB 127.7 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 6.0/43.9 MB 127.7 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 6.7/43.9 MB 47.8 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 7.6/43.9 MB 37.3 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 8.3/43.9 MB 31.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 8.3/43.9 MB 31.2 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 9.1/43.9 MB 27.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 9.9/43.9 MB 25.3 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.8/43.9 MB 22.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 10.8/43.9 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 11.8/43.9 MB 19.8 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 12.9/43.9 MB 18.2 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 14.1/43.9 MB 16.8 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 14.6/43.9 MB 15.6 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 15.4/43.9 MB 15.2 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 16.3/43.9 MB 16.4 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 16.6/43.9 MB 15.2 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 16.8/43.9 MB 14.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 17.3/43.9 MB 14.6 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 20.1/43.9 MB 18.2 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 24.5/43.9 MB 31.1 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 32.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 25.2 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.4/43.9 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.5/43.9 MB 10.4 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.5/43.9 MB 9.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.5/43.9 MB 9.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.6/43.9 MB 9.2 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 25.8/43.9 MB 9.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 26.2/43.9 MB 8.6 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 26.2/43.9 MB 8.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.0/43.9 MB 8.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 29.8/43.9 MB 9.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 30.4/43.9 MB 9.2 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 31.7/43.9 MB 8.8 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.3/43.9 MB 8.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.0/43.9 MB 8.1 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.1/43.9 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.7/43.9 MB 7.8 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.4/43.9 MB 7.4 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.9/43.9 MB 7.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 35.1/43.9 MB 7.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.7/43.9 MB 13.4 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 36.1/43.9 MB 16.4 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 36.5/43.9 MB 16.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 36.9/43.9 MB 15.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.3/43.9 MB 16.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 37.8/43.9 MB 14.9 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.3/43.9 MB 13.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.9/43.9 MB 13.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.5/43.9 MB 12.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 39.9/43.9 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.4/43.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.0/43.9 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.6/43.9 MB 10.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.1/43.9 MB 10.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.6/43.9 MB 10.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.1/43.9 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.7/43.9 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.9/43.9 MB 9.3 MB/s eta 0:00:00\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, Pillow, joblib, scikit-learn, sentence-transformers\n",
      "Successfully installed Pillow-11.1.0 joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 sentence-transformers-3.3.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\rchan\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6.76568821e-02  6.34959117e-02  4.87131439e-02  7.93049559e-02\n",
      "   3.74480635e-02  2.65279901e-03  3.93749736e-02 -7.09847594e-03\n",
      "   5.93614466e-02  3.15370448e-02  6.00980595e-02 -5.29052205e-02\n",
      "   4.06067669e-02 -2.59308070e-02  2.98428256e-02  1.12692453e-03\n",
      "   7.35148787e-02 -5.03818952e-02 -1.22386634e-01  2.37028133e-02\n",
      "   2.97265332e-02  4.24768552e-02  2.56337691e-02  1.99518725e-03\n",
      "  -5.69191128e-02 -2.71598194e-02 -3.29035521e-02  6.60248771e-02\n",
      "   1.19007163e-01 -4.58791517e-02 -7.26214498e-02 -3.25840227e-02\n",
      "   5.23413569e-02  4.50553037e-02  8.25298857e-03  3.67024094e-02\n",
      "  -1.39415646e-02  6.53918535e-02 -2.64272671e-02  2.06397686e-04\n",
      "  -1.36643434e-02 -3.62810977e-02 -1.95043795e-02 -2.89738216e-02\n",
      "   3.94270122e-02 -8.84090662e-02  2.62427074e-03  1.36713488e-02\n",
      "   4.83062565e-02 -3.11566107e-02 -1.17329188e-01 -5.11690341e-02\n",
      "  -8.85288268e-02 -2.18962803e-02  1.42986607e-02  4.44167592e-02\n",
      "  -1.34814996e-02  7.43392557e-02  2.66383160e-02 -1.98762398e-02\n",
      "   1.79191250e-02 -1.06052058e-02 -9.04262587e-02  2.13269107e-02\n",
      "   1.41204849e-01 -6.47177268e-03 -1.40381162e-03 -1.53609626e-02\n",
      "  -8.73571411e-02  7.22174197e-02  2.01402809e-02  4.25587781e-02\n",
      "  -3.49014103e-02  3.19512386e-04 -8.02970603e-02 -3.27472650e-02\n",
      "   2.85268631e-02 -5.13657816e-02  1.09389164e-01  8.19327682e-02\n",
      "  -9.84040424e-02 -9.34095532e-02 -1.51292244e-02  4.51248437e-02\n",
      "   4.94171865e-02 -2.51867976e-02  1.57077052e-02 -1.29290715e-01\n",
      "   5.31890662e-03  4.02338477e-03 -2.34571584e-02 -6.72982410e-02\n",
      "   2.92280205e-02 -2.60845162e-02  1.30624939e-02 -3.11663095e-02\n",
      "  -4.82713357e-02 -5.58859631e-02 -3.87505218e-02  1.20010853e-01\n",
      "  -1.03924619e-02  4.89704870e-02  5.53537272e-02  4.49358821e-02\n",
      "  -4.00974229e-03 -1.02959752e-01 -2.92968899e-02 -5.83402514e-02\n",
      "   2.70472206e-02 -2.20168792e-02 -7.22241178e-02 -4.13869917e-02\n",
      "  -1.93298031e-02  2.73329043e-03  2.77012121e-04 -9.67588797e-02\n",
      "  -1.00574754e-01 -1.41923185e-02 -8.07891861e-02  4.53925095e-02\n",
      "   2.45042071e-02  5.97613826e-02 -7.38185421e-02  1.19843995e-02\n",
      "  -6.63403794e-02 -7.69045502e-02  3.85157764e-02 -5.59362183e-33\n",
      "   2.80013923e-02 -5.60784824e-02 -4.86601591e-02  2.15569139e-02\n",
      "   6.01980872e-02 -4.81402576e-02 -3.50247212e-02  1.93314161e-02\n",
      "  -1.75152160e-02 -3.89210619e-02 -3.81070049e-03 -1.70287676e-02\n",
      "   2.82099973e-02  1.28290951e-02  4.71600704e-02  6.21030293e-02\n",
      "  -6.43588752e-02  1.29285589e-01 -1.31231584e-02  5.23069091e-02\n",
      "  -3.73680778e-02  2.89094672e-02 -1.68981124e-02 -2.37330701e-02\n",
      "  -3.33491415e-02 -5.16762435e-02  1.55356834e-02  2.08802950e-02\n",
      "  -1.25372130e-02  4.59579006e-02  3.72719951e-02  2.80566886e-02\n",
      "  -5.90004697e-02 -1.16988299e-02  4.92182635e-02  4.70329225e-02\n",
      "   7.35487938e-02 -3.70529965e-02  3.98458401e-03  1.06411465e-02\n",
      "  -1.61505857e-04 -5.27166501e-02  2.75928043e-02 -3.92920971e-02\n",
      "   8.44717696e-02  4.86860722e-02 -4.85869637e-03  1.79948695e-02\n",
      "  -4.28569466e-02  1.23375226e-02  6.39950763e-03  4.04823236e-02\n",
      "   1.48887197e-02 -1.53941233e-02  7.62948170e-02  2.37043966e-02\n",
      "   4.45237160e-02  5.08195609e-02 -2.31256569e-03 -1.88737046e-02\n",
      "  -1.23335272e-02  4.66001667e-02 -5.63437976e-02  6.29927665e-02\n",
      "  -3.15535180e-02  3.24912444e-02  2.34673601e-02 -6.55438229e-02\n",
      "   2.01709364e-02  2.57082935e-02 -1.23869013e-02 -8.36488325e-03\n",
      "  -6.64377958e-02  9.43073630e-02 -3.57093364e-02 -3.42483260e-02\n",
      "  -6.66355947e-03 -8.01521912e-03 -3.09711546e-02  4.33012173e-02\n",
      "  -8.21401272e-03 -1.50795043e-01  3.07691935e-02  4.00719494e-02\n",
      "  -3.79293226e-02  1.93211960e-03  4.00530845e-02 -8.77074897e-02\n",
      "  -3.68491299e-02  8.57959222e-03 -3.19252126e-02 -1.25257969e-02\n",
      "   7.35540241e-02  1.34738965e-03  2.05918662e-02  2.71098091e-33\n",
      "  -5.18577062e-02  5.78361005e-02 -9.18984860e-02  3.94421443e-02\n",
      "   1.05576515e-01 -1.96912009e-02  6.18402362e-02 -7.63465166e-02\n",
      "   2.40880270e-02  9.40049514e-02 -1.16535418e-01  3.71198617e-02\n",
      "   5.22425212e-02 -3.95853212e-03  5.72214387e-02  5.32854116e-03\n",
      "   1.24016851e-01  1.39022358e-02 -1.10249668e-02  3.56053151e-02\n",
      "  -3.30754369e-02  8.16574246e-02 -1.52003700e-02  6.05585463e-02\n",
      "  -6.01397641e-02  3.26102376e-02 -3.48296650e-02 -1.69881675e-02\n",
      "  -9.74907354e-02 -2.71484405e-02  1.74707524e-03 -7.68982545e-02\n",
      "  -4.31857966e-02 -1.89984906e-02 -2.91661099e-02  5.77488318e-02\n",
      "   2.41821483e-02 -1.16902115e-02 -6.21435270e-02  2.84351762e-02\n",
      "  -2.37522239e-04 -2.51783505e-02  4.39638412e-03  8.12839866e-02\n",
      "   3.64184082e-02 -6.04006499e-02 -3.65516730e-02 -7.93748349e-02\n",
      "  -5.08527132e-03  6.69698790e-02 -1.17784359e-01  3.23743448e-02\n",
      "  -4.71253060e-02 -1.34459333e-02 -9.48445052e-02  8.24958365e-03\n",
      "  -1.06748929e-02 -6.81881458e-02  1.11821061e-03  2.48020198e-02\n",
      "  -6.35889396e-02  2.84492914e-02 -2.61303820e-02  8.58111680e-02\n",
      "   1.14682287e-01 -5.35345599e-02 -5.63588366e-02  4.26008925e-02\n",
      "   1.09454431e-02  2.09578760e-02  1.00131154e-01  3.26050892e-02\n",
      "  -1.84208751e-01 -3.93208638e-02 -6.91455007e-02 -6.38105050e-02\n",
      "  -6.56386092e-02 -6.41252240e-03 -4.79612127e-02 -7.68133253e-02\n",
      "   2.95384582e-02 -2.29948293e-02  4.17036936e-02 -2.50048023e-02\n",
      "  -4.54503438e-03 -4.17136364e-02 -1.32289743e-02 -6.38357475e-02\n",
      "  -2.46471236e-03 -1.37337446e-02  1.68976970e-02 -6.30398095e-02\n",
      "   8.98880959e-02  4.18170653e-02 -1.85687412e-02 -1.80442203e-08\n",
      "  -1.67998094e-02 -3.21577340e-02  6.30383715e-02 -4.13091704e-02\n",
      "   4.44819182e-02  2.02466967e-03  6.29592687e-02 -5.17373811e-03\n",
      "  -1.00444257e-02 -3.05639878e-02  3.52672748e-02  5.58581576e-02\n",
      "  -4.67124507e-02  3.45102660e-02  3.29578109e-02  4.30114605e-02\n",
      "   2.94360574e-02 -3.03164106e-02 -1.71107929e-02  7.37484694e-02\n",
      "  -5.47909699e-02  2.77514905e-02  6.20170543e-03  1.58800315e-02\n",
      "   3.42978910e-02 -5.15750982e-03  2.35079546e-02  7.53135532e-02\n",
      "   1.92842856e-02  3.36197317e-02  5.09103388e-02  1.52497083e-01\n",
      "   1.64207797e-02  2.70528123e-02  3.75162363e-02  2.18552873e-02\n",
      "   5.66333868e-02 -3.95746529e-02  7.12313205e-02 -5.41377328e-02\n",
      "   1.03764469e-03  2.11853106e-02 -3.56309265e-02  1.09016955e-01\n",
      "   2.76535098e-03  3.13997231e-02  1.38419773e-03 -3.45738642e-02\n",
      "  -4.59277555e-02  2.88083088e-02  7.16903480e-03  4.84684743e-02\n",
      "   2.61018202e-02 -9.44072474e-03  2.82169897e-02  3.48724052e-02\n",
      "   3.69098783e-02 -8.58949218e-03 -3.53205465e-02 -2.47856919e-02\n",
      "  -1.91921555e-02  3.80707271e-02  5.99653646e-02 -4.22287174e-02]\n",
      " [ 8.64385962e-02  1.02762640e-01  5.39455563e-03  2.04442022e-03\n",
      "  -9.96338576e-03  2.53855139e-02  4.92875837e-02 -3.06265727e-02\n",
      "   6.87254742e-02  1.01365978e-02  7.75397643e-02 -9.00807083e-02\n",
      "   6.10621599e-03 -5.69898747e-02  1.41714308e-02  2.80491672e-02\n",
      "  -8.68464932e-02  7.64398798e-02 -1.03491299e-01 -6.77437708e-02\n",
      "   6.99946508e-02  8.44251141e-02 -7.24914856e-03  1.04770381e-02\n",
      "   1.34020550e-02  6.77576661e-02 -9.42086205e-02 -3.71689871e-02\n",
      "   5.22617474e-02 -3.10853459e-02 -9.63406861e-02  1.57717038e-02\n",
      "   2.57866848e-02  7.85245001e-02  7.89949447e-02  1.91516057e-02\n",
      "   1.64356641e-02  3.10084829e-03  3.81311364e-02  2.37090625e-02\n",
      "   1.05389506e-02 -4.40645479e-02  4.41738591e-02 -2.58728005e-02\n",
      "   6.15378581e-02 -4.05427665e-02 -8.64140093e-02  3.19722481e-02\n",
      "  -8.90666444e-04 -2.44437028e-02 -9.19721574e-02  2.33939458e-02\n",
      "  -8.30293596e-02  4.41510379e-02 -2.49692630e-02  6.23019971e-02\n",
      "  -1.30356452e-03  7.51395375e-02  2.46384759e-02 -6.47244602e-02\n",
      "  -1.17727757e-01  3.83392163e-02 -9.11767408e-02  6.35446236e-02\n",
      "   7.62739778e-02 -8.80240798e-02  9.54558980e-03 -4.69717458e-02\n",
      "  -8.41740593e-02  3.88823971e-02 -1.14393532e-01  6.28856663e-03\n",
      "  -3.49361934e-02  2.39750445e-02 -3.31317112e-02 -1.57243852e-02\n",
      "  -3.78955565e-02 -8.81250110e-03  7.06118867e-02  3.28066200e-02\n",
      "   2.03669094e-03 -1.12278976e-01  6.79721544e-03  1.22765694e-02\n",
      "   3.35303545e-02 -1.36200720e-02 -2.25489996e-02 -2.25229245e-02\n",
      "  -2.03194916e-02  5.04297279e-02 -7.48652741e-02 -8.22822228e-02\n",
      "   7.65962824e-02  4.93392274e-02 -3.75553221e-02  1.44634591e-02\n",
      "  -5.72457798e-02 -1.79954451e-02  1.09697938e-01  1.19462803e-01\n",
      "   8.09239573e-04  6.17057420e-02  3.26322094e-02 -1.30780071e-01\n",
      "  -1.48636639e-01 -6.16232716e-02  4.33885828e-02  2.67129187e-02\n",
      "   1.39785931e-02 -3.94002497e-02 -2.52711866e-02  3.87740647e-03\n",
      "   3.58664729e-02 -6.15420491e-02  3.76660489e-02  2.67565325e-02\n",
      "  -3.82659733e-02 -3.54793221e-02 -2.39227246e-02  8.67977142e-02\n",
      "  -1.84063129e-02  7.71039277e-02  1.39862252e-03  7.00382814e-02\n",
      "  -4.77877744e-02 -7.89820030e-02  5.10814413e-02 -2.99868333e-33\n",
      "  -3.91645916e-02 -2.56211474e-03  1.65210571e-02  9.48940776e-03\n",
      "  -5.66219166e-02  6.57783374e-02 -4.77002785e-02  1.11662149e-02\n",
      "  -5.73558398e-02 -9.16259363e-03 -2.17521265e-02 -5.59531823e-02\n",
      "  -1.11422949e-02  9.32793245e-02  1.66765265e-02 -1.36723565e-02\n",
      "   4.34388742e-02  1.87245233e-03  7.29948143e-03  5.16331792e-02\n",
      "   4.80608493e-02  1.35341465e-01 -1.71738788e-02 -1.29698329e-02\n",
      "  -7.50109628e-02  2.61107739e-02  2.69802380e-02  7.83041876e-04\n",
      "  -4.87270094e-02  1.17842834e-02 -4.59580496e-02 -4.83213328e-02\n",
      "  -1.95671152e-02  1.93889327e-02  1.98807418e-02  1.67432502e-02\n",
      "   9.87801254e-02 -2.74087712e-02  2.34809034e-02  3.70231457e-03\n",
      "  -6.14514723e-02 -1.21228199e-03 -9.50475689e-03  9.25154053e-03\n",
      "   2.38443874e-02  8.61232057e-02  2.26789862e-02  5.45142510e-04\n",
      "   3.47129516e-02  6.25466509e-03 -6.92775566e-03  3.92400175e-02\n",
      "   1.15675014e-02  3.26280035e-02  6.22155443e-02  2.76114531e-02\n",
      "   1.86883267e-02  3.55805904e-02  4.11795676e-02  1.54782301e-02\n",
      "   4.22691442e-02  3.82248461e-02  1.00313257e-02 -2.83245929e-02\n",
      "   4.47052307e-02 -4.10458632e-02 -4.50551789e-03 -5.44734113e-02\n",
      "   2.62321010e-02  1.79862399e-02 -1.23118743e-01 -4.66952026e-02\n",
      "  -1.35913072e-02  6.46710768e-02  3.57345422e-03 -1.22233620e-02\n",
      "  -1.79382227e-02 -2.55502034e-02  2.37223785e-02  4.08667186e-03\n",
      "  -6.51475936e-02  4.43651602e-02  4.68595773e-02 -3.25174630e-02\n",
      "   4.02272446e-03 -3.97602236e-03  1.11939488e-02 -9.95597839e-02\n",
      "   3.33168544e-02  8.01061392e-02  9.42691863e-02 -6.38293922e-02\n",
      "   3.23151648e-02 -5.13553135e-02 -7.49881379e-03  5.30049734e-34\n",
      "  -4.13194969e-02  9.49646980e-02 -1.06401429e-01  4.96590920e-02\n",
      "  -3.41913216e-02 -3.16745639e-02 -1.71555858e-02  1.70102087e-03\n",
      "   5.79757802e-02 -1.21777621e-03 -1.68536026e-02 -5.16912341e-02\n",
      "   5.52998893e-02 -3.42647843e-02  3.08179352e-02 -3.10480911e-02\n",
      "   9.27532539e-02  3.72663289e-02 -2.37398297e-02  4.45893891e-02\n",
      "   1.46153271e-02  1.16239354e-01 -5.00112846e-02  3.88716571e-02\n",
      "   4.24747914e-03  2.56976765e-02  3.27243656e-02  4.29907367e-02\n",
      "  -1.36144655e-02  2.56122276e-02  1.06262648e-02 -8.46863538e-02\n",
      "  -9.52982381e-02  1.08399875e-01 -7.51599744e-02 -1.37773519e-02\n",
      "   6.37338310e-02 -4.49671224e-03 -3.25321369e-02  6.23613931e-02\n",
      "   3.48053426e-02 -3.54922488e-02 -2.00222433e-02  3.66608389e-02\n",
      "  -2.48837043e-02  1.01818340e-02 -7.01233223e-02 -4.31951284e-02\n",
      "   2.95332558e-02 -2.94936064e-04 -3.45386937e-02  1.46675920e-02\n",
      "  -9.83970240e-02 -4.70488258e-02 -8.85495543e-03 -8.89913812e-02\n",
      "   3.50996107e-02 -1.29601970e-01 -4.98865917e-02 -6.12047315e-02\n",
      "  -5.97797409e-02  9.46318824e-03  4.91217673e-02 -7.75026679e-02\n",
      "   8.09727311e-02 -4.79257591e-02  2.34377990e-03  7.57031515e-02\n",
      "  -2.40175836e-02 -1.52546065e-02  4.86738533e-02 -3.85968313e-02\n",
      "  -7.04831630e-02 -1.20348074e-02 -3.88790779e-02 -7.76016638e-02\n",
      "  -1.07243871e-02  1.04188360e-02 -2.13753767e-02 -9.17386189e-02\n",
      "  -1.11345146e-02 -2.96065882e-02  2.46458109e-02  4.65713628e-03\n",
      "  -1.63449794e-02 -3.95219512e-02  7.73373693e-02 -2.84732785e-02\n",
      "  -3.69938067e-03  8.27665254e-02 -1.10408925e-02  3.13983336e-02\n",
      "   5.35094738e-02  5.75145930e-02 -3.17622237e-02 -1.52911266e-08\n",
      "  -7.99661204e-02 -4.76797186e-02 -8.59788507e-02  5.69616482e-02\n",
      "  -4.08866331e-02  2.23832466e-02 -4.64445865e-03 -3.80130783e-02\n",
      "  -3.10671106e-02 -1.07278014e-02  1.97698809e-02  7.77001306e-03\n",
      "  -6.09476073e-03 -3.86376120e-02  2.80271824e-02  6.78137690e-02\n",
      "  -2.35351678e-02  3.21747586e-02  8.02536029e-03 -2.39107311e-02\n",
      "  -1.21994514e-03  3.14598940e-02 -5.24923950e-02 -8.06813315e-03\n",
      "   3.14771244e-03  5.11496328e-02 -4.44104336e-02  6.36013001e-02\n",
      "   3.85083668e-02  3.30433063e-02 -4.18727705e-03  4.95592766e-02\n",
      "  -5.69605157e-02 -6.49713958e-03 -2.49793120e-02 -1.60867088e-02\n",
      "   6.62289709e-02 -2.06310768e-02  1.08045742e-01  1.68547034e-02\n",
      "   1.43812597e-02 -1.32127320e-02 -1.29387394e-01  6.95216432e-02\n",
      "  -5.55772893e-02 -6.75413162e-02 -5.45819197e-03 -6.13593776e-03\n",
      "   3.90840508e-02 -6.28779382e-02  3.74063514e-02 -1.16570871e-02\n",
      "   1.29150450e-02 -5.52495420e-02  5.16076051e-02 -4.30841651e-03\n",
      "   5.80247901e-02  1.86944753e-02  2.27810368e-02  3.21665145e-02\n",
      "   5.37979044e-02  7.02849105e-02  7.49312118e-02 -8.41774866e-02]]\n"
     ]
    }
   ],
   "source": [
    "# #Testing the sentence transformer\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "# embeddings = model.encode(sentences)\n",
    "# print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pathway as pw\n",
    "# from sentence_transformers import SentenceTransformer, util\n",
    "# import numpy as np\n",
    "# model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model for embedding generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving folder 1RifJJBjm5tA8E20808RjvkIAiWnFbceb CVPR\n",
      "Processing file 1qpRfskGy-m2iq_WT71H9C6OgzwE30ESC R006.pdf\n",
      "Processing file 1iggcvhmWBvHDUhTH6LR3HnMcGsGGxBfY R007.pdf\n",
      "Retrieving folder 1JVzabziJf4d2drCTXFssFr_wZMnjr8oT EMNLP\n",
      "Processing file 1dlzAYEqKrlJqYkuVRkandoyYfPA7bE1V R008.pdf\n",
      "Processing file 1XGrMuvDuK3HHgamNF8kN3lSildl2j7aP R009.pdf\n",
      "Retrieving folder 1sJKv0o5ySrigZewU_wtTxysx9j0kO_nV KDD\n",
      "Processing file 1GxI_lyX0Fps3CGh6qvwBMowgqbzn5p2A R010.pdf\n",
      "Processing file 1AqXTZvXFcTWaOW_8iW3Jsgqyj3CoyJ5e R011.pdf\n",
      "Retrieving folder 1ZgkbpvhoNKUuH0b4uCv30lyWg3-5ijTC NeurIPS\n",
      "Processing file 1G-Te2p5gGJHxDSNfYAOXpr5H1hplqyx1 R012.pdf\n",
      "Processing file 1f7BCC4M8QQjhdabHCzel9EvSXqqY4Pvj R013.pdf\n",
      "Retrieving folder 13eDgt0YghQU2qlogGrTrXJzfD0h0F2Iw TMLR\n",
      "Processing file 1COphhBTWa5VJbw_uqWnv8CB2ubtjHGBl R014.pdf\n",
      "Processing file 1G7L_cjQO_alpQljsvXQ6H_BND9i_rRqP R015.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving folder contents completed\n",
      "Building directory structure\n",
      "Building directory structure completed\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1qpRfskGy-m2iq_WT71H9C6OgzwE30ESC\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\CVPR\\R006.pdf\n",
      "100%|██████████| 134k/134k [00:00<00:00, 676kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1iggcvhmWBvHDUhTH6LR3HnMcGsGGxBfY\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\CVPR\\R007.pdf\n",
      "100%|██████████| 154k/154k [00:00<00:00, 755kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1dlzAYEqKrlJqYkuVRkandoyYfPA7bE1V\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\EMNLP\\R008.pdf\n",
      "100%|██████████| 70.9k/70.9k [00:00<00:00, 447kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1XGrMuvDuK3HHgamNF8kN3lSildl2j7aP\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\EMNLP\\R009.pdf\n",
      "100%|██████████| 59.9k/59.9k [00:00<00:00, 460kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1GxI_lyX0Fps3CGh6qvwBMowgqbzn5p2A\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\KDD\\R010.pdf\n",
      "100%|██████████| 184k/184k [00:00<00:00, 676kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1AqXTZvXFcTWaOW_8iW3Jsgqyj3CoyJ5e\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\KDD\\R011.pdf\n",
      "100%|██████████| 161k/161k [00:00<00:00, 718kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1G-Te2p5gGJHxDSNfYAOXpr5H1hplqyx1\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\NeurIPS\\R012.pdf\n",
      "100%|██████████| 177k/177k [00:00<00:00, 817kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1f7BCC4M8QQjhdabHCzel9EvSXqqY4Pvj\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\NeurIPS\\R013.pdf\n",
      "100%|██████████| 197k/197k [00:00<00:00, 873kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1COphhBTWa5VJbw_uqWnv8CB2ubtjHGBl\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\TMLR\\R014.pdf\n",
      "100%|██████████| 174k/174k [00:00<00:00, 807kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1G7L_cjQO_alpQljsvXQ6H_BND9i_rRqP\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\Publishable\\TMLR\\R015.pdf\n",
      "100%|██████████| 174k/174k [00:00<00:00, 673kB/s]\n",
      "Download completed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\CVPR\\\\R006.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\CVPR\\\\R007.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\EMNLP\\\\R008.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\EMNLP\\\\R009.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\KDD\\\\R010.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\KDD\\\\R011.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\NeurIPS\\\\R012.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\NeurIPS\\\\R013.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\TMLR\\\\R014.pdf',\n",
       " 'c:\\\\Users\\\\rchan\\\\Desktop\\\\Task2\\\\Publishable\\\\TMLR\\\\R015.pdf']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gdown\n",
    "# URL of the Google Drive folder for publishable data\n",
    "url = 'https://drive.google.com/drive/folders/1RKyDkAyW7cf09THED7Ms4LNBdeTDWnlK'\n",
    "\n",
    "# Download the folder\n",
    "gdown.download_folder(url, quiet=False, use_cookies=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/232.6 kB ? eta -:--:--\n",
      "   - -------------------------------------- 10.2/232.6 kB ? eta -:--:--\n",
      "   ------ -------------------------------- 41.0/232.6 kB 393.8 kB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 174.1/232.6 kB 1.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 232.6/232.6 kB 1.4 MB/s eta 0:00:00\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detailed Action Identification in Baseball Gam...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advanced techniques for through and contextual...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Importance of Written Explanations in\\nAgg...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Detecting Medication Usage in Parkinson’s Dise...</td>\n",
       "      <td>KDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
       "      <td>KDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Safe Predictors for Input-Output Specification...</td>\n",
       "      <td>NeurIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Generalization in ReLU Networks via Restricted...</td>\n",
       "      <td>NeurIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Addressing Min-Max Challenges in Nonconvex-Non...</td>\n",
       "      <td>TMLR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Examining the Convergence of Denoising Diffusi...</td>\n",
       "      <td>TMLR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  Detailed Action Identification in Baseball Gam...     CVPR\n",
       "1  Advancements in 3D Food Modeling: A Review of ...     CVPR\n",
       "2  Advanced techniques for through and contextual...    EMNLP\n",
       "3  The Importance of Written Explanations in\\nAgg...    EMNLP\n",
       "4  Detecting Medication Usage in Parkinson’s Dise...      KDD\n",
       "5  Addressing Popularity Bias with Popularity-Con...      KDD\n",
       "6  Safe Predictors for Input-Output Specification...  NeurIPS\n",
       "7  Generalization in ReLU Networks via Restricted...  NeurIPS\n",
       "8  Addressing Min-Max Challenges in Nonconvex-Non...     TMLR\n",
       "9  Examining the Convergence of Denoising Diffusi...     TMLR"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Publishable/CVPR'\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            data.append({'text': text, 'label': 'CVPR'})\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Publishable/EMNLP'\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            data.append({'text': text, 'label': 'EMNLP'})\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Publishable/KDD'\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            data.append({'text': text, 'label': 'KDD'})\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Publishable/NeurIPS'\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            data.append({'text': text, 'label': 'NeurIPS'})\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Publishable/TMLR'\n",
    "# Iterate through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith('.pdf'):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            data.append({'text': text, 'label': 'TMLR'})\n",
    "\n",
    "# Create a dataframe from the data\n",
    "publishable_df = pd.DataFrame(data)\n",
    "publishable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pathway Table Schemas\n",
    "# class ResearchPaper(pw.Schema):\n",
    "#     paper_id: int\n",
    "#     content: str\n",
    "\n",
    "# class ReferencePaper(pw.Schema):\n",
    "#     paper_id: int\n",
    "#     content: str\n",
    "\n",
    "# # Pathway Vectorstore for storing embeddings\n",
    "# class EmbeddingStore(pw.Schema):\n",
    "#     paper_id: int\n",
    "#     embedding: np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Load Reference Papers and Research Papers with Pathway Connectors\n",
    "# def load_reference_data():\n",
    "#     \"\"\"\n",
    "#     Load reference papers into a Pathway table using a static data source (CSV).\n",
    "#     \"\"\"\n",
    "#     return pw.io.csv.read(\"reference_papers.json\", schema=ReferencePaper)\n",
    "\n",
    "# def load_research_papers():\n",
    "#     \"\"\"\n",
    "#     Load research papers dynamically from a CSV source.\n",
    "#     \"\"\"\n",
    "#     return pw.io.csv.read(\"research_papers.csv\", schema=ResearchPaper)\n",
    "\n",
    "# # Step 2: Compute and Store Embeddings in Pathway Vectorstore\n",
    "# def store_reference_embeddings(reference_papers):\n",
    "#     \"\"\"\n",
    "#     Create embeddings for reference papers and store them in Pathway Vectorstore.\n",
    "#     \"\"\"\n",
    "#     # Generate embeddings for reference papers\n",
    "#     reference_embeddings = reference_papers.select(\n",
    "#         reference_papers.conference,\n",
    "#         reference_papers.paper_id,\n",
    "#         embedding=pw.apply(lambda abstract: model.encode(abstract), reference_papers.abstract)\n",
    "#     )\n",
    "\n",
    "#     # Store embeddings in the Pathway Vectorstore\n",
    "#     return reference_embeddings.select(\n",
    "#         reference_papers.paper_id,\n",
    "#         embedding=pw.apply(lambda emb: emb.tolist(), reference_embeddings.embedding)  # Convert to list for storage\n",
    "#     ).store(\"reference_embeddings\", schema=EmbeddingStore)\n",
    "\n",
    "# def store_research_embeddings(research_papers):\n",
    "#     \"\"\"\n",
    "#     Create embeddings for research papers and store them in Pathway Vectorstore.\n",
    "#     \"\"\"\n",
    "#     # Generate embeddings for research papers\n",
    "#     research_embeddings = research_papers.select(\n",
    "#         research_papers.paper_id,\n",
    "#         research_papers.title,\n",
    "#         embedding=pw.apply(lambda abstract: model.encode(abstract), research_papers.abstract)\n",
    "#     )\n",
    "\n",
    "#     # Store embeddings in the Pathway Vectorstore\n",
    "#     return research_embeddings.select(\n",
    "#         research_papers.paper_id,\n",
    "#         embedding=pw.apply(lambda emb: emb.tolist(), research_embeddings.embedding)  # Convert to list for storage\n",
    "#     ).store(\"research_embeddings\", schema=EmbeddingStore)\n",
    "\n",
    "# # Step 3: Retrieve and Match Research Papers to Conferences\n",
    "# def match_conferences_to_papers():\n",
    "#     \"\"\"\n",
    "#     Retrieve stored embeddings for research papers and reference papers, then match them based on similarity.\n",
    "#     \"\"\"\n",
    "#     # Load stored embeddings from Vectorstore\n",
    "#     reference_embeddings = pw.io.vectorstore.read(\"reference_embeddings\", schema=EmbeddingStore)\n",
    "#     research_embeddings = pw.io.vectorstore.read(\"research_embeddings\", schema=EmbeddingStore)\n",
    "\n",
    "#     # Compute similarity scores using cosine similarity\n",
    "#     recommendations = research_embeddings.join(\n",
    "#         reference_embeddings,\n",
    "#         how=\"cross\",\n",
    "#         suffixes=(\"_research\", \"_reference\"),\n",
    "#     ).select(\n",
    "#         research_embeddings.paper_id,\n",
    "#         reference_embeddings.conference,\n",
    "#         score=pw.apply(\n",
    "#             lambda e1, e2: util.pytorch_cos_sim(np.array(e1), np.array(e2)).item(),\n",
    "#             research_embeddings.embedding,\n",
    "#             reference_embeddings.embedding,\n",
    "#         ),\n",
    "#     ).filter(lambda rec: rec.score >= 0.75)  # Filter by threshold\n",
    "\n",
    "#     # Select the highest scoring conference for each paper\n",
    "#     top_recommendations = recommendations.groupby(\n",
    "#         recommendations.paper_id\n",
    "#     ).reduce(\n",
    "#         conference=pw.reducers.argmax(recommendations.score, by=recommendations.conference),\n",
    "#         score=pw.reducers.max(recommendations.score),\n",
    "#     )\n",
    "#     return top_recommendations\n",
    "\n",
    "# # Step 4: Generate Justifications\n",
    "# def generate_justifications(top_recommendations, research_papers, reference_papers):\n",
    "#     \"\"\"\n",
    "#     Generate justifications for recommendations.\n",
    "#     \"\"\"\n",
    "#     return top_recommendations.join(\n",
    "#         research_papers, how=\"inner\", on=\"paper_id\"\n",
    "#     ).join(\n",
    "#         reference_papers, how=\"inner\", on=\"conference\"\n",
    "#     ).select(\n",
    "#         paper_id=research_papers.paper_id,\n",
    "#         conference=top_recommendations.conference,\n",
    "#         justification=pw.apply(\n",
    "#             lambda title, conf, score: f\"This paper titled '{title}' is recommended for the {conf} conference with a similarity score of {score:.2f}.\",\n",
    "#             research_papers.title,\n",
    "#             top_recommendations.conference,\n",
    "#             top_recommendations.score,\n",
    "#         ),\n",
    "#     )\n",
    "\n",
    "# # Step 5: Save Recommendations and Justifications\n",
    "# def save_results(results):\n",
    "#     \"\"\"\n",
    "#     Save the final recommendations and justifications to a dynamic output.\n",
    "#     \"\"\"\n",
    "#     pw.io.jsonl.write(results, \"recommendations.jsonl\")\n",
    "\n",
    "# # Main Pathway Workflow\n",
    "# def main():\n",
    "#     # Load data into Pathway tables\n",
    "#     reference_papers = load_reference_data()\n",
    "#     research_papers = load_research_papers()\n",
    "\n",
    "#     # Compute and store embeddings in Pathway Vectorstore\n",
    "#     store_reference_embeddings(reference_papers)\n",
    "#     store_research_embeddings(research_papers)\n",
    "\n",
    "#     # Match conferences to papers based on cosine similarity\n",
    "#     top_recommendations = match_conferences_to_papers()\n",
    "\n",
    "#     # Generate justifications for the recommendations\n",
    "#     justifications = generate_justifications(top_recommendations, research_papers, reference_papers)\n",
    "\n",
    "#     # Save results to output\n",
    "#     save_results(justifications)\n",
    "\n",
    "# # Run Pathway Workflow\n",
    "# if __name__ == \"__main__\":\n",
    "#     pw.run(main)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Detailed Action Identification in Baseball Gam...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Advancements in 3D Food Modeling: A Review of ...</td>\n",
       "      <td>CVPR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Advanced techniques for through and contextual...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Importance of Written Explanations in\\nAgg...</td>\n",
       "      <td>EMNLP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Detecting Medication Usage in Parkinson’s Dise...</td>\n",
       "      <td>KDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Addressing Popularity Bias with Popularity-Con...</td>\n",
       "      <td>KDD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Safe Predictors for Input-Output Specification...</td>\n",
       "      <td>NeurIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Generalization in ReLU Networks via Restricted...</td>\n",
       "      <td>NeurIPS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Addressing Min-Max Challenges in Nonconvex-Non...</td>\n",
       "      <td>TMLR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Examining the Convergence of Denoising Diffusi...</td>\n",
       "      <td>TMLR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text    label\n",
       "0  Detailed Action Identification in Baseball Gam...     CVPR\n",
       "1  Advancements in 3D Food Modeling: A Review of ...     CVPR\n",
       "2  Advanced techniques for through and contextual...    EMNLP\n",
       "3  The Importance of Written Explanations in\\nAgg...    EMNLP\n",
       "4  Detecting Medication Usage in Parkinson’s Dise...      KDD\n",
       "5  Addressing Popularity Bias with Popularity-Con...      KDD\n",
       "6  Safe Predictors for Input-Output Specification...  NeurIPS\n",
       "7  Generalization in ReLU Networks via Restricted...  NeurIPS\n",
       "8  Addressing Min-Max Challenges in Nonconvex-Non...     TMLR\n",
       "9  Examining the Convergence of Denoising Diffusi...     TMLR"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publishable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\gdown\\parse_url.py:48: UserWarning: You specified a Google Drive link that is not the correct link to download a file. You might want to try `--fuzzy` option or the following url: https://drive.google.com/uc?id=None\n",
      "  warnings.warn(\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P035.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P035.pdf\n",
      "1.22MB [00:00, 3.64MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P093.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P093.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P090.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P090.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P057.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P057.pdf\n",
      "1.70kB [00:00, 423kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P049.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P049.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P111.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P111.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P018.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P018.pdf\n",
      "1.22MB [00:00, 3.39MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P046.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P046.pdf\n",
      "1.70kB [00:00, 307kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P104.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P104.pdf\n",
      "1.22MB [00:00, 3.44MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P122.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P122.pdf\n",
      "1.22MB [00:00, 3.54MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P132.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P132.pdf\n",
      "1.70kB [00:00, 2.71MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P059.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P059.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P089.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P089.pdf\n",
      "1.70kB [00:00, 107kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P067.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P067.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P064.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P064.pdf\n",
      "1.70kB [00:00, 537kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P061.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P061.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P116.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P116.pdf\n",
      "1.70kB [00:00, 581kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P052.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P052.pdf\n",
      "1.70kB [00:00, 168kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P030.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P030.pdf\n",
      "1.70kB [00:00, 502kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P033.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P033.pdf\n",
      "1.70kB [00:00, 138kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P112.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P112.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P044.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P044.pdf\n",
      "1.22MB [00:00, 3.54MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P006.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P006.pdf\n",
      "1.70kB [00:00, 434kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P076.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P076.pdf\n",
      "1.70kB [00:00, 194kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P103.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P103.pdf\n",
      "1.22MB [00:00, 3.57MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P108.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P108.pdf\n",
      "1.70kB [00:00, 236kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P008.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P008.pdf\n",
      "1.70kB [00:00, 682kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P003.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P003.pdf\n",
      "1.70kB [00:00, 440kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P097.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P097.pdf\n",
      "1.22MB [00:00, 3.50MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P124.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P124.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P109.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P109.pdf\n",
      "1.70kB [00:00, 420kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P066.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P066.pdf\n",
      "1.70kB [00:00, 240kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P080.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P080.pdf\n",
      "1.70kB [00:00, 477kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P091.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P091.pdf\n",
      "1.22MB [00:00, 3.55MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P027.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P027.pdf\n",
      "1.22MB [00:00, 3.51MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P133.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P133.pdf\n",
      "1.70kB [00:00, 269kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P114.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P114.pdf\n",
      "1.70kB [00:00, 275kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P128.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P128.pdf\n",
      "1.70kB [00:00, 365kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P028.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P028.pdf\n",
      "1.70kB [00:00, 166kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P058.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P058.pdf\n",
      "1.70kB [00:00, 554kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P021.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P021.pdf\n",
      "1.70kB [00:00, 401kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P029.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P029.pdf\n",
      "1.70kB [00:00, 291kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P062.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P062.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P123.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P123.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P065.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P065.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P079.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P079.pdf\n",
      "1.70kB [00:00, 209kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P120.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P120.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P102.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P102.pdf\n",
      "1.70kB [00:00, 143kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P092.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P092.pdf\n",
      "1.70kB [00:00, 469kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P007.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P007.pdf\n",
      "1.70kB [00:00, 443kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P009.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P009.pdf\n",
      "1.22MB [00:00, 3.50MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P026.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P026.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P115.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P115.pdf\n",
      "1.22MB [00:00, 3.53MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P121.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P121.pdf\n",
      "1.70kB [00:00, 724kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P063.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P063.pdf\n",
      "1.22MB [00:00, 2.52MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P011.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P011.pdf\n",
      "1.22MB [00:00, 3.57MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P099.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P099.pdf\n",
      "1.22MB [00:00, 3.67MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P005.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P005.pdf\n",
      "1.22MB [00:00, 3.53MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P051.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P051.pdf\n",
      "1.22MB [00:00, 3.52MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P085.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P085.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P050.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P050.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P110.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P110.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P012.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P012.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P060.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P060.pdf\n",
      "1.70kB [00:00, 942kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P069.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P069.pdf\n",
      "1.70kB [00:00, 365kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P135.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P135.pdf\n",
      "1.70kB [00:00, 1.46MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P078.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P078.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P082.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P082.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P023.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P023.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P130.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P130.pdf\n",
      "1.70kB [00:00, 789kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P113.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P113.pdf\n",
      "1.22MB [00:00, 3.61MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P010.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P010.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P094.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P094.pdf\n",
      "1.70kB [00:00, 332kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P071.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P071.pdf\n",
      "1.22MB [00:00, 3.48MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P017.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P017.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P074.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P074.pdf\n",
      "1.70kB [00:00, 255kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P025.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P025.pdf\n",
      "1.22MB [00:00, 3.45MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P020.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P020.pdf\n",
      "1.70kB [00:00, 329kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P083.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P083.pdf\n",
      "1.22MB [00:00, 3.50MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P125.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P125.pdf\n",
      "1.70kB [00:00, 336kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P013.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P013.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P107.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P107.pdf\n",
      "1.22MB [00:00, 3.50MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P117.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P117.pdf\n",
      "1.22MB [00:00, 3.49MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P016.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P016.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P126.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P126.pdf\n",
      "1.70kB [00:00, 435kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P087.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P087.pdf\n",
      "1.70kB [00:00, 619kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P055.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P055.pdf\n",
      "1.70kB [00:00, 205kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P014.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P014.pdf\n",
      "1.70kB [00:00, 1.04MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P004.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P004.pdf\n",
      "1.70kB [00:00, 136kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P086.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P086.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P037.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P037.pdf\n",
      "1.70kB [00:00, 277kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P134.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P134.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P075.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P075.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P131.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P131.pdf\n",
      "1.70kB [00:00, 448kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P054.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P054.pdf\n",
      "1.22MB [00:00, 3.57MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P015.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P015.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P084.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P084.pdf\n",
      "1.22MB [00:00, 3.52MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P068.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P068.pdf\n",
      "1.70kB [00:00, 306kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P001.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P001.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P072.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P072.pdf\n",
      "1.70kB [00:00, 934kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P045.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P045.pdf\n",
      "1.70kB [00:00, 1.08MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P034.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P034.pdf\n",
      "1.70kB [00:00, 514kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P073.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P073.pdf\n",
      "1.70kB [00:00, 115kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P118.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P118.pdf\n",
      "1.22MB [00:00, 3.77MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P048.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P048.pdf\n",
      "1.22MB [00:00, 3.63MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P095.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P095.pdf\n",
      "1.22MB [00:00, 3.70MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P019.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P019.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P031.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P031.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P127.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P127.pdf\n",
      "1.22MB [00:00, 3.48MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P088.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P088.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P042.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P042.pdf\n",
      "1.70kB [00:00, 233kB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P038.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P038.pdf\n",
      "1.22MB [00:00, 3.66MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P101.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P101.pdf\n",
      "1.70kB [00:00, 1.08MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P040.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P040.pdf\n",
      "1.70kB [00:00, ?B/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u/P024.pdf\n",
      "To: c:\\Users\\rchan\\Desktop\\Task2\\papers\\P024.pdf\n",
      "1.70kB [00:00, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "# URL of the Google Drive folder for publishable data\n",
    "url = 'https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u'\n",
    "\n",
    "# Get the list of file IDs from the dataframe\n",
    "file_ids = df['PDF File'].tolist()\n",
    "\n",
    "# Function to download a batch of files\n",
    "def download_batch(file_ids, batch_size=50, download_folder='papers'):\n",
    "    if not os.path.exists(download_folder):\n",
    "        os.makedirs(download_folder)\n",
    "    for i in range(0, len(file_ids), batch_size):\n",
    "        batch = file_ids[i:i + batch_size]\n",
    "        for file_id in batch:\n",
    "            file_url = f\"{url}/{file_id}\"\n",
    "            output_path = os.path.join(download_folder, file_id)\n",
    "            gdown.download(file_url, output_path, quiet=False, use_cookies=False)\n",
    "\n",
    "# Download files in batches\n",
    "download_batch(file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m page_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(reader\u001b[38;5;241m.\u001b[39mpages)):\n\u001b[0;32m     11\u001b[0m                 text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reader\u001b[38;5;241m.\u001b[39mpages[page_num]\u001b[38;5;241m.\u001b[39mextract_text()\n\u001b[1;32m---> 12\u001b[0m             \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTMLR\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Create a dataframe from the data\u001b[39;00m\n\u001b[0;32m     15\u001b[0m publishable_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# import PyPDF2\n",
    "# folder_path = 'Publishable/TMLR'\n",
    "# # Iterate through all files in the folder\n",
    "# for filename in os.listdir(folder_path):\n",
    "#     if filename.endswith('.pdf'):\n",
    "#         file_path = os.path.join(folder_path, filename)\n",
    "#         with open(file_path, 'rb') as file:\n",
    "#             reader = PyPDF2.PdfReader(file)\n",
    "#             text = ''\n",
    "#             for page_num in range(len(reader.pages)):\n",
    "#                 text += reader.pages[page_num].extract_text()\n",
    "#             data.append({'text': text, 'label': 'TMLR'})\n",
    "\n",
    "# # Create a dataframe from the data\n",
    "# publishable_df = pd.DataFrame(data)\n",
    "# publishable_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.0/1.5 MB 393.8 kB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.1/1.5 MB 901.1 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.5 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.0/1.5 MB 4.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 5.3 MB/s eta 0:00:00\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Installing collected packages: click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVC_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\rchan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rchan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        CVPR       1.00      1.00      1.00         1\n",
      "       EMNLP       0.50      1.00      0.67         1\n",
      "         KDD       0.00      0.00      0.00         1\n",
      "     NeurIPS       0.50      1.00      0.67         1\n",
      "        TMLR       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.60         5\n",
      "   macro avg       0.40      0.60      0.47         5\n",
      "weighted avg       0.40      0.60      0.47         5\n",
      "\n",
      "\n",
      "Evaluation Summary:\n",
      "      Metric     Value\n",
      "0  Precision  0.400000\n",
      "1     Recall  0.600000\n",
      "2   F1-Score  0.466667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float64'\n",
       "\twith 3997 stored elements and shape (5, 3689)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def preprocess_text(text):\n",
    "    tokens=word_tokenize(text)\n",
    "    tokens=[token.lower() for token in tokens]\n",
    "    tokens=[token for token in tokens if token not in string.punctuation]\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens=[token for token in tokens if token not in stop_words]\n",
    "    stemmer=PorterStemmer()\n",
    "    tokens=[stemmer.stem(token) for token in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "publishable_df[\"processed_text\"]=publishable_df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf=TfidfVectorizer(max_features=5000)\n",
    "X=tfidf.fit_transform(publishable_df[\"processed_text\"])\n",
    "y=publishable_df[\"label\"]\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.5,random_state=42,stratify=y)\n",
    "\n",
    "# LinearSVM Model\n",
    "model = SVC(kernel='linear', C=1.0, class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred=model.predict(X_test)\n",
    "\n",
    "precision,recall,f1, _=precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "classification_report_table=classification_report(y_test, y_pred, target_names=y.unique())\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report_table)\n",
    "\n",
    "# Tabular Summary of Results\n",
    "results=pd.DataFrame({\n",
    "    \"Metric\": [\"Precision\", \"Recall\", \"F1-Score\"],\n",
    "    \"Value\": [precision, recall, f1]\n",
    "})\n",
    "print(\"\\nEvaluation Summary:\")\n",
    "print(results)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "df= pd.read_csv('predictions.csv')\n",
    "df\n",
    "# Filter the dataframe to include only publishable papers\n",
    "publishable_papers = df[df['Prediction'] == 'Publishable']\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "data = []\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = 'Papers'\n",
    "\n",
    "# Iterate through the filtered dataframe\n",
    "for index, row in publishable_papers.iterrows():\n",
    "    file_name = row['PDF File']\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'rb') as file:\n",
    "            reader = PyPDF2.PdfReader(file)\n",
    "            text = ''\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                text += reader.pages[page_num].extract_text()\n",
    "            data.append({'PDF File': file_name, 'text': text})\n",
    "\n",
    "# Create a new dataframe from the data\n",
    "new_df = pd.DataFrame(data)\n",
    "new_df.head()\n",
    "new_df.to_csv('new_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PDF File   Prediction Conference\n",
      "0    P035.pdf  Publishable       CVPR\n",
      "1    P093.pdf  Publishable       CVPR\n",
      "2    P090.pdf  Publishable       CVPR\n",
      "3    P057.pdf  Publishable       CVPR\n",
      "4    P049.pdf  Publishable       CVPR\n",
      "..        ...          ...        ...\n",
      "110  P042.pdf  Publishable       CVPR\n",
      "111  P038.pdf  Publishable    NeurIPS\n",
      "112  P101.pdf  Publishable       CVPR\n",
      "113  P040.pdf  Publishable       CVPR\n",
      "114  P024.pdf  Publishable      EMNLP\n",
      "\n",
      "[115 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rchan\\AppData\\Local\\Temp\\ipykernel_18360\\1817266.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df[\"processed_text\"]=final_df[\"text\"].apply(preprocess_text)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a new dataframe from the data\n",
    "final_df = new_df[[\"text\"]]\n",
    "final_df[\"processed_text\"]=final_df[\"text\"].apply(preprocess_text)\n",
    "# final_df\n",
    "# Use the existing TF-IDF Vectorizer\n",
    "X_new=tfidf.transform(final_df[\"processed_text\"])\n",
    "X_new\n",
    "y_pred=model.predict(X_new)\n",
    "\n",
    "# Create the final submission dataframe\n",
    "final_submission_df = pd.DataFrame({\n",
    "    \"PDF File\": new_df[\"PDF File\"],\n",
    "    \"Prediction\": \"Publishable\",\n",
    "    \"Conference\": y_pred\n",
    "})\n",
    "\n",
    "# Display the final submission dataframe\n",
    "print(final_submission_df)\n",
    "final_submission_df.to_csv(\"final_submission2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conference\n",
      "CVPR       50\n",
      "EMNLP      46\n",
      "NeurIPS    18\n",
      "KDD         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "conference_counts = final_submission_df['Conference'].value_counts()\n",
    "print(conference_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement pathway.connectors (from versions: none)\n",
      "ERROR: No matching distribution found for pathway.connectors\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install -U pathway.connectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "This is not the real Pathway package.\n",
      "Visit https://pathway.com/developers/ to get Pathway.\n",
      "Already tried that? Visit https://pathway.com/troubleshooting/ to get help.\n",
      "Note: your platform is Windows-10-10.0.26100-SP0, your Python is CPython 3.11.9.\n",
      "This is not the real Pathway package.\n",
      "Visit https://pathway.com/developers/ to get Pathway.\n",
      "Already tried that? Visit https://pathway.com/troubleshooting/ to get help.\n",
      "Note: your platform is Windows-10-10.0.26100-SP0, your Python is CPython 3.11.9.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pathway' has no attribute 'io'\nThis is not the real Pathway package.\nVisit https://pathway.com/developers/ to get Pathway.\nAlready tried that? Visit https://pathway.com/troubleshooting/ to get help.\nNote: your platform is Windows-10-10.0.26100-SP0, your Python is CPython 3.11.9.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpublishable_papers.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(csv_path)\n\u001b[1;32m---> 13\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[43mpw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241m.\u001b[39mcsv\u001b[38;5;241m.\u001b[39mread(csv_path, schema\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m})\n\u001b[0;32m     14\u001b[0m processed_papers \u001b[38;5;241m=\u001b[39m table\n\u001b[0;32m     16\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\pathway\\__init__.py:61\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m     59\u001b[0m _warn(warning)\n\u001b[0;32m     60\u001b[0m error \u001b[38;5;241m=\u001b[39m error \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m warning\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(error)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pathway' has no attribute 'io'\nThis is not the real Pathway package.\nVisit https://pathway.com/developers/ to get Pathway.\nAlready tried that? Visit https://pathway.com/troubleshooting/ to get help.\nNote: your platform is Windows-10-10.0.26100-SP0, your Python is CPython 3.11.9."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "import pathway as pw\n",
    "# from pathway.connectors import csv\n",
    "\n",
    "REFERENCE_PAPERS = publishable_df.to_dict(orient='records')\n",
    "\n",
    "csv_path = \"publishable_papers.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "table = pw.io.csv.read(csv_path, schema={\"text\": str, \"label\": str})\n",
    "processed_papers = table\n",
    "\n",
    "embedding_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "reference_embeddings = [\n",
    "    {\n",
    "        \"embedding\": embedding_model.encode(paper[\"text\"], convert_to_tensor=True),\n",
    "        \"conference\": paper[\"label\"],\n",
    "    }\n",
    "    for paper in REFERENCE_PAPERS\n",
    "]\n",
    "\n",
    "vectorstore = pw.Table.from_records(reference_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (2.2.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Downloading attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl.metadata (71 kB)\n",
      "     ---------------------------------------- 0.0/71.4 kB ? eta -:--:--\n",
      "     ----- ---------------------------------- 10.2/71.4 kB ? eta -:--:--\n",
      "     ---------------- --------------------- 30.7/71.4 kB 660.6 kB/s eta 0:00:01\n",
      "     -------------------------------- ----- 61.4/71.4 kB 550.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 71.4/71.4 kB 489.7 kB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
      "Requirement already satisfied: colorama in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
      "   ---------------------------------------- 0.0/480.6 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 61.4/480.6 kB 1.6 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 112.6/480.6 kB 1.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 194.6/480.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 358.4/480.6 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  471.0/480.6 kB 2.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 480.6/480.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "   ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 116.3/116.3 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "   ---------------------------------------- 0.0/179.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 179.3/179.3 kB 11.3 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.11.11-cp311-cp311-win_amd64.whl (442 kB)\n",
      "   ---------------------------------------- 0.0/442.5 kB ? eta -:--:--\n",
      "   ---------------------------------------  440.3/442.5 kB 9.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 442.5/442.5 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "   ---------------------------------------- 0.0/143.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 143.5/143.5 kB 8.9 MB/s eta 0:00:00\n",
      "Downloading pyarrow-18.1.0-cp311-cp311-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/25.1 MB 14.9 MB/s eta 0:00:02\n",
      "   - -------------------------------------- 0.9/25.1 MB 14.5 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 1.5/25.1 MB 13.8 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.6/25.1 MB 16.7 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 4.0/25.1 MB 19.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 6.3/25.1 MB 25.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.0/25.1 MB 30.3 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 13.6/25.1 MB 59.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.6/25.1 MB 81.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.9/25.1 MB 81.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.0/25.1 MB 93.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  25.1/25.1 MB 59.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 43.5 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.4/63.4 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading frozenlist-1.5.0-cp311-cp311-win_amd64.whl (51 kB)\n",
      "   ---------------------------------------- 0.0/51.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 51.6/51.6 kB 2.6 MB/s eta 0:00:00\n",
      "Downloading multidict-6.1.0-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp311-cp311-win_amd64.whl (44 kB)\n",
      "   ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 44.4/44.4 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading yarl-1.18.3-cp311-cp311-win_amd64.whl (91 kB)\n",
      "   ---------------------------------------- 0.0/91.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 91.0/91.0 kB 5.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.12.0\n",
      "    Uninstalling fsspec-2024.12.0:\n",
      "      Successfully uninstalled fsspec-2024.12.0\n",
      "Successfully installed aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 attrs-24.3.0 datasets-3.2.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.9.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-18.1.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 263/263 [00:00<00:00, 4590.63 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of values with 'label' == 0: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import Dataset,load_dataset\n",
    "dataset = load_dataset(\"csv\", data_files=\"final.csv\")\n",
    "# Check how many values in the dataset have 'label' == 0\n",
    "num_label_0 = dataset['train'].filter(lambda x: x['label'] == 0).num_rows\n",
    "print(f\"Number of values with 'label' == 0: {num_label_0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rchan\\Desktop\\Task2\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\")\n",
    "# Use a pipeline as a high-level helper\n",
    "classifier = pipeline(\"text-classification\", model=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\",tokenizer=\"MoritzLaurer/ModernBERT-large-zeroshot-v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PDF File</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>P035.pdf</td>\n",
       "      <td>Game-Theoretic Optimization for Crowdsourced\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P093.pdf</td>\n",
       "      <td>Premature Termination Strategy for Deep Image ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P090.pdf</td>\n",
       "      <td>Equivariant Fine-Tuning of Large Pretrained Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P057.pdf</td>\n",
       "      <td>A Collaborative Painting Experience:\\nHuman-Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P049.pdf</td>\n",
       "      <td>Improving Model Generalization Using a Single ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>P042.pdf</td>\n",
       "      <td>DeepSim: A Semantic Approach to Image Registra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>P038.pdf</td>\n",
       "      <td>Utilizing Graph Neural Networks to Analyze Esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>P101.pdf</td>\n",
       "      <td>A Convolutional LSTM Network Approach for\\nIde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>P040.pdf</td>\n",
       "      <td>A 3D Convolutional Neural Network Approach for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>P024.pdf</td>\n",
       "      <td>Turning the Tables: Exploring Subtle Vulnerabi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>115 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PDF File                                               text\n",
       "0    P035.pdf  Game-Theoretic Optimization for Crowdsourced\\n...\n",
       "1    P093.pdf  Premature Termination Strategy for Deep Image ...\n",
       "2    P090.pdf  Equivariant Fine-Tuning of Large Pretrained Mo...\n",
       "3    P057.pdf  A Collaborative Painting Experience:\\nHuman-Ma...\n",
       "4    P049.pdf  Improving Model Generalization Using a Single ...\n",
       "..        ...                                                ...\n",
       "110  P042.pdf  DeepSim: A Semantic Approach to Image Registra...\n",
       "111  P038.pdf  Utilizing Graph Neural Networks to Analyze Esp...\n",
       "112  P101.pdf  A Convolutional LSTM Network Approach for\\nIde...\n",
       "113  P040.pdf  A 3D Convolutional Neural Network Approach for...\n",
       "114  P024.pdf  Turning the Tables: Exploring Subtle Vulnerabi...\n",
       "\n",
       "[115 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m     inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n\u001b[1;32m----> 6\u001b[0m tokenizer_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241m.\u001b[39mmap(preprocess, batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      7\u001b[0m tokenizer_dataset\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def preprocess(example):\n",
    "    inputs = tokenizer(example[\"text\"], return_tensors=\"pt\", padding=\"max_length\", max_length=512, truncation=True)\n",
    "    inputs[\"label\"] = torch.tensor(example[\"label\"])\n",
    "    return inputs\n",
    "tokenizer_dataset = dataset.map(preprocess, batched=True)\n",
    "tokenizer_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting groq\n",
      "  Downloading groq-0.15.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from groq)\n",
      "  Downloading anyio-4.8.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from groq)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from groq)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from groq)\n",
      "  Using cached pydantic-2.10.5-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting sniffio (from groq)\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from groq) (4.12.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\rchan\\desktop\\task2\\myenv\\lib\\site-packages (from httpx<1,>=0.23.0->groq) (2024.12.14)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq)\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic<3,>=1.9.0->groq)\n",
      "  Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl.metadata (6.7 kB)\n",
      "Downloading groq-0.15.0-py3-none-any.whl (109 kB)\n",
      "   ---------------------------------------- 0.0/109.6 kB ? eta -:--:--\n",
      "   ---------- ---------------------------- 30.7/109.6 kB 660.6 kB/s eta 0:00:01\n",
      "   --------------------- ----------------- 61.4/109.6 kB 812.7 kB/s eta 0:00:01\n",
      "   ----------------------------------- -- 102.4/109.6 kB 837.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- 109.6/109.6 kB 704.4 kB/s eta 0:00:00\n",
      "Downloading anyio-4.8.0-py3-none-any.whl (96 kB)\n",
      "   ---------------------------------------- 0.0/96.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 96.0/96.0 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "   ---------------------------------------- 0.0/73.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 73.5/73.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.6/78.6 kB 2.2 MB/s eta 0:00:00\n",
      "Using cached pydantic-2.10.5-py3-none-any.whl (431 kB)\n",
      "Using cached pydantic_core-2.27.2-cp311-cp311-win_amd64.whl (2.0 MB)\n",
      "Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "   ---------------------------------------- 0.0/58.3 kB ? eta -:--:--\n",
      "   ----------------------------------- ---- 51.2/58.3 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 58.3/58.3 kB 1.0 MB/s eta 0:00:00\n",
      "Installing collected packages: sniffio, pydantic-core, h11, distro, annotated-types, pydantic, httpcore, anyio, httpx, groq\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.8.0 distro-1.9.0 groq-0.15.0 h11-0.14.0 httpcore-1.0.7 httpx-0.28.1 pydantic-2.10.5 pydantic-core-2.27.2 sniffio-1.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GROQ_API_KEY=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\n"
     ]
    }
   ],
   "source": [
    "%env GROQ_API_KEY= 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast language models are crucial for various applications in natural language processing (NLP) and have numerous benefits. Here are some reasons why they are important:\n",
      "\n",
      "1. **Improved User Experience**: Fast language models enable quick response times, which is essential for applications like chatbots, virtual assistants, and language translation software. Users expect instant answers, and slow models can lead to frustration and abandonment.\n",
      "2. **Increased Productivity**: Fast language models can process large amounts of text data quickly, making them ideal for tasks like text summarization, sentiment analysis, and information retrieval. This enables businesses and organizations to analyze and respond to large volumes of text data efficiently.\n",
      "3. **Real-Time Applications**: Fast language models are necessary for real-time applications like speech recognition, voice assistants, and live subtitles. They can process audio or text input quickly, enabling instant responses and interactions.\n",
      "4. **Resource Efficiency**: Fast language models can be more energy-efficient and require less computational resources, making them suitable for deployment on edge devices, mobile devices, or in resource-constrained environments.\n",
      "5. **Competitive Advantage**: Organizations that adopt fast language models can gain a competitive advantage by providing faster and more accurate language-based services, such as language translation, text analysis, and content generation.\n",
      "6. **Improved Accuracy**: Fast language models can be trained on larger datasets and fine-tuned for specific tasks, leading to improved accuracy and better performance on various NLP tasks.\n",
      "7. **Scalability**: Fast language models can handle large volumes of text data and scale to meet the needs of growing applications, making them ideal for large-scale deployments.\n",
      "8. **Cost Savings**: Fast language models can reduce the need for human intervention, automated processing, and manual review, resulting in cost savings and increased efficiency.\n",
      "9. **Enhanced Customer Service**: Fast language models can power chatbots and virtual assistants, enabling businesses to provide 24/7 customer support, answer frequent questions, and route complex issues to human representatives.\n",
      "10. **Advancements in NLP Research**: Fast language models can facilitate research in NLP by enabling researchers to quickly experiment with different models, architectures, and techniques, leading to new breakthroughs and innovations.\n",
      "\n",
      "Some examples of applications that benefit from fast language models include:\n",
      "\n",
      "* Virtual assistants (e.g., Siri, Alexa, Google Assistant)\n",
      "* Language translation software (e.g., Google Translate)\n",
      "* Chatbots and customer support platforms\n",
      "* Sentiment analysis and text classification tools\n",
      "* Speech recognition systems\n",
      "* Text summarization and information retrieval systems\n",
      "* Content generation and writing assistance tools\n",
      "\n",
      "In summary, fast language models are essential for providing efficient, accurate, and scalable language-based services, enabling businesses and organizations to improve user experience, increase productivity, and gain a competitive advantage.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from groq import Groq\n",
    "\n",
    "client = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "\n",
    "chat_completion = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain the importance of fast language models\",\n",
    "        }\n",
    "    ],\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    ")\n",
    "\n",
    "print(chat_completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define classification and rationale generation logic\n",
    "def classify_and_generate_rationale(paper):\n",
    "    # Get embedding for the research paper\n",
    "    paper_embedding = embedding_model.encode(paper[\"text\"], convert_to_tensor=True)\n",
    "\n",
    "    # Retrieve similar reference papers from vectorstore\n",
    "    similarity_scores = [\n",
    "        {\n",
    "            \"conference\": ref[\"label\"],\n",
    "            \"score\": torch.nn.functional.cosine_similarity(paper_embedding, ref[\"embedding\"], dim=0).item(),\n",
    "        }\n",
    "        for ref in reference_embeddings\n",
    "    ]\n",
    "    # Sort by similarity score\n",
    "    best_match = max(similarity_scores, key=lambda x: x[\"score\"])\n",
    "\n",
    "    # Classify the research paper\n",
    "    classification_result = classifier(paper[\"text\"], return_all_scores=True)\n",
    "    predicted_label = max(classification_result[0], key=lambda x: x[\"score\"])[\"label\"]\n",
    "\n",
    "    # Generate a rationale\n",
    "    rationale = f\"The paper was classified into {best_match['conference']} based on its similarity to reference papers and its thematic alignment with topics like {best_match['conference']}.\"\n",
    "\n",
    "    return {\n",
    "        \"paper_id\": paper[\"paper_id\"],\n",
    "        \"predicted_conference\": best_match[\"conference\"],\n",
    "        \"rationale\": rationale,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the classification pipeline\n",
    "results = processed_papers.map(classify_and_generate_rationale)\n",
    "\n",
    "# Write results to CSV\n",
    "output_table = results.to_table()\n",
    "pw.io.csv.write(output_table, \"classified_papers.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
