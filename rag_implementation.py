# -*- coding: utf-8 -*-
"""RAG_Implementation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fyBgCeR678DgMXLfDelk_a26ZZBcVerL
"""

# !pip install pandas
# !pip install numpy
# !pip install sklearn
# !pip install -U pathway
# !pip install transformers
# !pip install torch

import pandas as pd
import numpy as np
df = pd.read_csv('predictions.csv')
df = df[df['Prediction'] != 'Non-Publishable']
df.info()

# !pip install -U sentence-transformers

# #Testing the sentence transformer
# from sentence_transformers import SentenceTransformer
# sentences = ["This is an example sentence", "Each sentence is converted"]
# model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
# embeddings = model.encode(sentences)
# print(embeddings)

# import pathway as pw
# from sentence_transformers import SentenceTransformer, util
# import numpy as np
# model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model for embedding generation

# !pip install gdown

import gdown
# URL of the Google Drive folder for publishable data
url = 'https://drive.google.com/drive/folders/1RKyDkAyW7cf09THED7Ms4LNBdeTDWnlK'

# Download the folder
gdown.download_folder(url, quiet=False, use_cookies=False)


import os
import pandas as pd
import PyPDF2

# Define the folder path
folder_path = 'Publishable/CVPR'

# Initialize an empty list to store the data
data = []

# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.pdf'):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page_num in range(len(reader.pages)):
                text += reader.pages[page_num].extract_text()
            data.append({'text': text, 'label': 'CVPR'})

# Define the folder path
folder_path = 'Publishable/EMNLP'
# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.pdf'):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page_num in range(len(reader.pages)):
                text += reader.pages[page_num].extract_text()
            data.append({'text': text, 'label': 'EMNLP'})

# Define the folder path
folder_path = 'Publishable/KDD'
# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.pdf'):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page_num in range(len(reader.pages)):
                text += reader.pages[page_num].extract_text()
            data.append({'text': text, 'label': 'KDD'})

# Define the folder path
folder_path = 'Publishable/NeurIPS'
# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.pdf'):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page_num in range(len(reader.pages)):
                text += reader.pages[page_num].extract_text()
            data.append({'text': text, 'label': 'NeurIPS'})

# Define the folder path
folder_path = 'Publishable/TMLR'
# Iterate through all files in the folder
for filename in os.listdir(folder_path):
    if filename.endswith('.pdf'):
        file_path = os.path.join(folder_path, filename)
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page_num in range(len(reader.pages)):
                text += reader.pages[page_num].extract_text()
            data.append({'text': text, 'label': 'TMLR'})

# Create a dataframe from the data
publishable_df = pd.DataFrame(data)
publishable_df

# # Pathway Table Schemas
# class ResearchPaper(pw.Schema):
#     paper_id: int
#     content: str

# class ReferencePaper(pw.Schema):
#     paper_id: int
#     content: str

# # Pathway Vectorstore for storing embeddings
# class EmbeddingStore(pw.Schema):
#     paper_id: int
#     embedding: np.ndarray

# # Step 1: Load Reference Papers and Research Papers with Pathway Connectors
# def load_reference_data():
#     """
#     Load reference papers into a Pathway table using a static data source (CSV).
#     """
#     return pw.io.csv.read("reference_papers.json", schema=ReferencePaper)

# def load_research_papers():
#     """
#     Load research papers dynamically from a CSV source.
#     """
#     return pw.io.csv.read("research_papers.csv", schema=ResearchPaper)

# # Step 2: Compute and Store Embeddings in Pathway Vectorstore
# def store_reference_embeddings(reference_papers):
#     """
#     Create embeddings for reference papers and store them in Pathway Vectorstore.
#     """
#     # Generate embeddings for reference papers
#     reference_embeddings = reference_papers.select(
#         reference_papers.conference,
#         reference_papers.paper_id,
#         embedding=pw.apply(lambda abstract: model.encode(abstract), reference_papers.abstract)
#     )

#     # Store embeddings in the Pathway Vectorstore
#     return reference_embeddings.select(
#         reference_papers.paper_id,
#         embedding=pw.apply(lambda emb: emb.tolist(), reference_embeddings.embedding)  # Convert to list for storage
#     ).store("reference_embeddings", schema=EmbeddingStore)

# def store_research_embeddings(research_papers):
#     """
#     Create embeddings for research papers and store them in Pathway Vectorstore.
#     """
#     # Generate embeddings for research papers
#     research_embeddings = research_papers.select(
#         research_papers.paper_id,
#         research_papers.title,
#         embedding=pw.apply(lambda abstract: model.encode(abstract), research_papers.abstract)
#     )

#     # Store embeddings in the Pathway Vectorstore
#     return research_embeddings.select(
#         research_papers.paper_id,
#         embedding=pw.apply(lambda emb: emb.tolist(), research_embeddings.embedding)  # Convert to list for storage
#     ).store("research_embeddings", schema=EmbeddingStore)

# # Step 3: Retrieve and Match Research Papers to Conferences
# def match_conferences_to_papers():
#     """
#     Retrieve stored embeddings for research papers and reference papers, then match them based on similarity.
#     """
#     # Load stored embeddings from Vectorstore
#     reference_embeddings = pw.io.vectorstore.read("reference_embeddings", schema=EmbeddingStore)
#     research_embeddings = pw.io.vectorstore.read("research_embeddings", schema=EmbeddingStore)

#     # Compute similarity scores using cosine similarity
#     recommendations = research_embeddings.join(
#         reference_embeddings,
#         how="cross",
#         suffixes=("_research", "_reference"),
#     ).select(
#         research_embeddings.paper_id,
#         reference_embeddings.conference,
#         score=pw.apply(
#             lambda e1, e2: util.pytorch_cos_sim(np.array(e1), np.array(e2)).item(),
#             research_embeddings.embedding,
#             reference_embeddings.embedding,
#         ),
#     ).filter(lambda rec: rec.score >= 0.75)  # Filter by threshold

#     # Select the highest scoring conference for each paper
#     top_recommendations = recommendations.groupby(
#         recommendations.paper_id
#     ).reduce(
#         conference=pw.reducers.argmax(recommendations.score, by=recommendations.conference),
#         score=pw.reducers.max(recommendations.score),
#     )
#     return top_recommendations

# # Step 4: Generate Justifications
# def generate_justifications(top_recommendations, research_papers, reference_papers):
#     """
#     Generate justifications for recommendations.
#     """
#     return top_recommendations.join(
#         research_papers, how="inner", on="paper_id"
#     ).join(
#         reference_papers, how="inner", on="conference"
#     ).select(
#         paper_id=research_papers.paper_id,
#         conference=top_recommendations.conference,
#         justification=pw.apply(
#             lambda title, conf, score: f"This paper titled '{title}' is recommended for the {conf} conference with a similarity score of {score:.2f}.",
#             research_papers.title,
#             top_recommendations.conference,
#             top_recommendations.score,
#         ),
#     )

# # Step 5: Save Recommendations and Justifications
# def save_results(results):
#     """
#     Save the final recommendations and justifications to a dynamic output.
#     """
#     pw.io.jsonl.write(results, "recommendations.jsonl")

# # Main Pathway Workflow
# def main():
#     # Load data into Pathway tables
#     reference_papers = load_reference_data()
#     research_papers = load_research_papers()

#     # Compute and store embeddings in Pathway Vectorstore
#     store_reference_embeddings(reference_papers)
#     store_research_embeddings(research_papers)

#     # Match conferences to papers based on cosine similarity
#     top_recommendations = match_conferences_to_papers()

#     # Generate justifications for the recommendations
#     justifications = generate_justifications(top_recommendations, research_papers, reference_papers)

#     # Save results to output
#     save_results(justifications)

# # Run Pathway Workflow
# if __name__ == "__main__":
#     pw.run(main)

# publishable_df

# import gdown
# import os

# # URL of the Google Drive folder for publishable data
# url = 'https://drive.google.com/drive/folders/1Y2Y0EsMalo26KcJiPYcAXh6UzgMNjh4u'

# # Get the list of file IDs from the dataframe
# file_ids = df['PDF File'].tolist()

# # Function to download a batch of files
# def download_batch(file_ids, batch_size=50, download_folder='papers'):
#     if not os.path.exists(download_folder):
#         os.makedirs(download_folder)
#     for i in range(0, len(file_ids), batch_size):
#         batch = file_ids[i:i + batch_size]
#         for file_id in batch:
#             file_url = f"{url}/{file_id}"
#             output_path = os.path.join(download_folder, file_id)
#             gdown.download(file_url, output_path, quiet=False, use_cookies=False)

# # Download files in batches
# download_batch(file_ids)

# import PyPDF2
# folder_path = 'Publishable/TMLR'
# # Iterate through all files in the folder
# for filename in os.listdir(folder_path):
#     if filename.endswith('.pdf'):
#         file_path = os.path.join(folder_path, filename)
#         with open(file_path, 'rb') as file:
#             reader = PyPDF2.PdfReader(file)
#             text = ''
#             for page_num in range(len(reader.pages)):
#                 text += reader.pages[page_num].extract_text()
#             data.append({'text': text, 'label': 'TMLR'})

# # Create a dataframe from the data
# publishable_df = pd.DataFrame(data)
# publishable_df

# !pip install nltk

"""#### SVC_Model"""

import pandas as pd
import string
import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, precision_recall_fscore_support

nltk.download("punkt_tab")
nltk.download("stopwords")

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

def preprocess_text(text):
    tokens=word_tokenize(text)
    tokens=[token.lower() for token in tokens]
    tokens=[token for token in tokens if token not in string.punctuation]
    stop_words = set(stopwords.words("english"))
    tokens=[token for token in tokens if token not in stop_words]
    stemmer=PorterStemmer()
    tokens=[stemmer.stem(token) for token in tokens]

    return " ".join(tokens)


publishable_df["processed_text"]=publishable_df["text"].apply(preprocess_text)

# TF-IDF Vectorization
tfidf=TfidfVectorizer(max_features=5000)
X=tfidf.fit_transform(publishable_df["processed_text"])
y=publishable_df["label"]
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.5,random_state=42,stratify=y)

# LinearSVM Model
model = SVC(kernel='linear', C=1.0, class_weight='balanced')
model.fit(X_train, y_train)

y_pred=model.predict(X_test)

precision,recall,f1, _=precision_recall_fscore_support(y_test, y_pred, average="weighted")
classification_report_table=classification_report(y_test, y_pred, target_names=y.unique())

print("Classification Report:")
print(classification_report_table)

# Tabular Summary of Results
results=pd.DataFrame({
    "Metric": ["Precision", "Recall", "F1-Score"],
    "Value": [precision, recall, f1]
})
print("\nEvaluation Summary:")
print(results)
# X_train

import os
import pandas as pd
import PyPDF2
df= pd.read_csv('predictions.csv')
df
# Filter the dataframe to include only publishable papers
publishable_papers = df[df['Prediction'] == 'Publishable']

# Initialize an empty list to store the data
data = []

# Define the folder path
folder_path = 'Papers'

# Iterate through the filtered dataframe
for index, row in publishable_papers.iterrows():
    file_name = row['PDF File']
    file_path = os.path.join(folder_path, file_name)
    if os.path.exists(file_path):
        with open(file_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ''
            for page_num in range(len(reader.pages)):
                text += reader.pages[page_num].extract_text()
            data.append({'PDF File': file_name, 'text': text})

# Create a new dataframe from the data
new_df = pd.DataFrame(data)
# new_df.head()
new_df.to_csv('new_df.csv', index=False)
# Create a new dataframe from the data
final_df = new_df[["text"]]
final_df["processed_text"]=final_df["text"].apply(preprocess_text)
# final_df
# Use the existing TF-IDF Vectorizer
X_new=tfidf.transform(final_df["processed_text"])
X_new
y_pred=model.predict(X_new)

# Create the final submission dataframe
final_submission_df = pd.DataFrame({
    "PDF File": new_df["PDF File"],
    "Prediction": "Publishable",
    "Conference": y_pred
})

# Display the final submission dataframe
print(final_submission_df)
final_submission_df.to_csv("final_submission2.csv", index=False)

conference_counts = final_submission_df['Conference'].value_counts()
print(conference_counts)

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U pathway.connectors

import pandas as pd
import torch
from sentence_transformers import SentenceTransformer
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline
import pathway as pw
# from pathway.connectors import csv

REFERENCE_PAPERS = publishable_df.to_dict(orient='records')

csv_path = "new_df.csv"
rational_df = pd.read_csv(csv_path)

table = pw.io.csv.read(csv_path, schema={"text": str, "label": str})
processed_papers = table

embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

reference_embeddings = [
    {
        "text": paper["text"],
        "embedding": embedding_model.encode(paper["text"], convert_to_tensor=True),
        "conference": paper["label"],
    }
    for paper in REFERENCE_PAPERS
]

vectorstore = pw.Table.from_records(reference_embeddings)

# !pip install datasets

import datasets
from datasets import Dataset,load_dataset
dataset = load_dataset("csv", data_files="prediction.csv")
# Check how many values in the dataset have 'label' == 0
num_label_0 = dataset['train'].filter(lambda x: x['label'] == 0).num_rows
print(f"Number of values with 'label' == 0: {num_label_0}")

# Load model directly
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import pipeline
tokenizer = AutoTokenizer.from_pretrained("MoritzLaurer/ModernBERT-large-zeroshot-v2.0")
model = AutoModelForSequenceClassification.from_pretrained("MoritzLaurer/ModernBERT-large-zeroshot-v2.0")
# Use a pipeline as a high-level helper
classifier = pipeline("text-classification", model="MoritzLaurer/ModernBERT-large-zeroshot-v2.0")

# new_df

import torch
def preprocess(example):
    inputs = tokenizer(example["text"], return_tensors="pt", padding="max_length", max_length=512, truncation=True)
    inputs["label"] = torch.tensor(example["label"])
    return inputs
tokenizer_dataset = dataset.map(preprocess, batched=True)
tokenizer_dataset

# !pip install groq

# Commented out IPython magic to ensure Python compatibility.
# %env GROQ_API_KEY= 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

import os

from groq import Groq

client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

def generate_rationale(best_match):
    chat_completion = client.chat.completions.create(
        messages=[
            {
                "role": "user",
                "content": f"Explain why the conference {best_match['conference']} is the best match based on the similarity analysis from the embedding which has the most match with {best_match['text']}.",
            }
        ],
        model="llama-3.3-70b-versatile",
    )

    return chat_completion.choices[0].message.content

# Define classification and rationale generation logic
def classify_and_generate_rationale(paper):
    # Get embedding for the research paper
    paper_embedding = embedding_model.encode(paper["text"], convert_to_tensor=True)

    # Retrieve similar reference papers from vectorstore
    similarity_scores = [
        {
            "conference": ref["label"],
            "score": torch.nn.functional.cosine_similarity(paper_embedding, ref["embedding"], dim=0).item(),
            "text": ref["text"]
        }
        for ref in reference_embeddings
    ]
    # Sort by similarity score
    best_match = max(similarity_scores, key=lambda x: x["score"])

    # Generate a rationale
    rationale = generate_rationale(best_match)
    return {
        "paper_id": paper["paper_id"],
        "predicted_conference": best_match["conference"],
        "rationale": rationale,
    }

# Apply the classification pipeline
results = processed_papers.map(classify_and_generate_rationale)

# Write results to CSV
output_table = results.to_table()
pw.io.csv.write(output_table, "classified_papers.csv")